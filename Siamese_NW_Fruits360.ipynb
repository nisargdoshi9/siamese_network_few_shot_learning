{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHm+hqQN4oFZplOKDQX4cF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nisargdoshi9/siamese_network_few_shot_learning/blob/main/Siamese_NW_Fruits360.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVHX4PUpq09A",
        "outputId": "f064b303-efd2-4087-91c5-65d817f2524c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install opendatasets"
      ],
      "metadata": {
        "id": "bQJT1E5Aq-Sq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install pandas"
      ],
      "metadata": {
        "id": "s8QhGayzramN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #Configuration environment\n",
        "# import os\n",
        "\n",
        "# os.environ['KAGGLE_USERNAME'] = \"nisargx9\" # username from the json file\n",
        "# os.environ['KAGGLE_KEY'] = \"xxxremovedxxx\" # key from the json file"
      ],
      "metadata": {
        "id": "gT96-pgordNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cd '/content/drive/MyDrive/Projects/Fruit360'"
      ],
      "metadata": {
        "id": "mje3v0KfwhGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !kaggle datasets download -d moltean/fruits"
      ],
      "metadata": {
        "id": "dNJN2TnzvZrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip \"/content/drive/MyDrive/Projects/Fruit360/fruits.zip\" -d \"/content/drive/MyDrive/Projects/Fruit360/fruits\""
      ],
      "metadata": {
        "id": "RGR79Uoqw_Dy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "import cv2\n",
        "\n",
        "import tensorflow as tf\n",
        "import keras\n"
      ],
      "metadata": {
        "id": "AFDA5DAwxRfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dir = \"/content/drive/MyDrive/Projects/Fruit360/fruits/fruits-360_dataset/fruits-360/Training\"\n",
        "split = 0.7\n",
        "num_files_each_class = 10\n",
        "\n",
        "folder_list = os.listdir(dir)\n",
        "print(len(folder_list),\": number of categories\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBWfvZY63Hc9",
        "outputId": "4f42ca6d-1665-4bc6-f7d2-a767f7ef502e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "131 : number of categories\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading dataset into numpy arrays"
      ],
      "metadata": {
        "id": "W4oGcLFJ8rtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = []\n",
        "y = []\n",
        "cat_list = []\n",
        "y_label = 0\n",
        "for folder_name in folder_list:\n",
        "  file_list = os.listdir(dir+'/'+folder_name)\n",
        "  temp = []\n",
        "  for file_name in file_list[:num_files_each_class]:\n",
        "    temp.append(len(x))\n",
        "    im = cv2.imread(dir+'/'+folder_name+'/'+file_name)\n",
        "    im = cv2.resize(im,(100,100))\n",
        "    x.append(im)\n",
        "    y.append(y_label)\n",
        "  y_label+=1\n",
        "  cat_list.append(temp)\n",
        "\n",
        "cat_list = np.asarray(cat_list)\n",
        "x = np.asarray(x)/255\n",
        "y = np.asarray(y)\n",
        "print('X,y, cat_list shape: ',x.shape,y.shape,cat_list.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nhICxRE33V8o",
        "outputId": "54072dd1-d411-4ddd-ceea-c1bb97120ed0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X,y, cat_list shape:  (1310, 100, 100, 3) (1310,) (131, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train test split: Unlike usual training, only training on 70% classes/categories and keep the rest for test"
      ],
      "metadata": {
        "id": "otnZjvWb8wPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = int(len(folder_list)*split)\n",
        "print('Number of folders to train (approx): ',int(num_classes))\n",
        "num_files = int(x.shape[0]*split)\n",
        "print('Number of files to train: ',int(num_files))\n",
        "\n",
        "#Split\n",
        "x_train = x[:num_files]\n",
        "y_train = y[:num_files]\n",
        "cat_train = cat_list[:num_classes]\n",
        "\n",
        "x_val = x[num_files:]\n",
        "y_val = y[num_files:]\n",
        "cat_test = cat_list[num_classes:]\n",
        "\n",
        "print(\"Shape of x_train, y_train, cat_train: \",x_train.shape,y_train.shape,cat_train.shape)\n",
        "print(\"Shape of x_val, y_val, cat_test: \",x_val.shape,y_val.shape,cat_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0TjQG_J71Xb",
        "outputId": "6498c80b-1a7b-424d-c192-a8d8017c1cca"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of folders to train (approx):  91\n",
            "Number of files to train:  916\n",
            "Shape of x_train, y_train, cat_train:  (916, 100, 100, 3) (916,) (91, 10)\n",
            "Shape of x_val, y_val, cat_test:  (394, 100, 100, 3) (394,) (40, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch generation function"
      ],
      "metadata": {
        "id": "cE6Yw_OpS4g9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batch(batch_size=64):\n",
        "  temp_x = x_train\n",
        "  temp_cat_list = cat_train\n",
        "  start = 0\n",
        "  end = num_classes\n",
        "  batch_x = []\n",
        "\n",
        "  batch_y = np.zeros(batch_size)\n",
        "  # Setting other half to 0\n",
        "  batch_y[int(batch_size/2):]=1\n",
        "  np.random.shuffle(batch_y)\n",
        "\n",
        "  class_list = np.random.randint(start,end,batch_size)\n",
        "  batch_x.append(np.zeros((batch_size,100,100,3)))\n",
        "  batch_x.append(np.zeros((batch_size,100,100,3)))\n",
        "\n",
        "  for i in range(batch_size):\n",
        "    batch_x[0][i]=temp_x[np.random.choice(temp_cat_list[class_list[i]])]\n",
        "    # if y_train=0 pick same class, else if y_train=1 pick any other class\n",
        "    if batch_y[i]==0:\n",
        "      batch_x[0][i] = temp_x[np.random.choice(temp_cat_list[class_list[i]])]\n",
        "    else:\n",
        "      temp_list = np.append(temp_cat_list[:class_list[i]].flatten(), temp_cat_list[class_list[i]+1:].flatten())\n",
        "      batch_x[1][i] = temp_x[np.random.choice(temp_list)]\n",
        "  return (batch_x, batch_y)"
      ],
      "metadata": {
        "id": "-T9o5aHc-PHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sequential Model"
      ],
      "metadata": {
        "id": "EJQGV7jLTRs_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Input, Conv2D, Lambda, Concatenate, Dense, Flatten, MaxPooling2D, Subtract\n",
        "from keras.models import Model,Sequential\n",
        "from keras.losses import binary_crossentropy\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.utils import plot_model\n",
        "\n",
        "W_init = keras.initializers.RandomNormal(mean=0.0, stddev = 1e-2)\n",
        "b_init = keras.initializers.RandomNormal(mean=0.0, stddev = 1e-2)\n",
        "\n",
        "input_shape = (100,100,3)\n",
        "left_input = Input(input_shape)\n",
        "right_input = Input(input_shape)\n",
        "\n",
        "model = Sequential([\n",
        "  keras.layers.Conv2D(64, (10,10), activation='relu', input_shape=input_shape, kernel_initializer=W_init, bias_initializer=b_init, kernel_regularizer=l2(2e-4)),\n",
        "  keras.layers.MaxPooling2D(2, 2),\n",
        "  keras.layers.Conv2D(128, (7,7), activation='relu', kernel_initializer=W_init, bias_initializer=b_init, kernel_regularizer=l2(2e-4)),\n",
        "  keras.layers.MaxPooling2D(2,2),\n",
        "  keras.layers.Conv2D(128, (4,4), activation='relu', kernel_initializer=W_init, bias_initializer=b_init, kernel_regularizer=l2(2e-4)),\n",
        "  keras.layers.MaxPooling2D(2,2),\n",
        "  keras.layers.Conv2D(256, (4,4), activation='relu', kernel_initializer=W_init, bias_initializer=b_init, kernel_regularizer=l2(2e-4)),\n",
        "  keras.layers.MaxPooling2D(2,2),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(4096, activation='sigmoid', kernel_initializer=W_init, bias_initializer=b_init)\n",
        "])\n",
        "\n",
        "encoded_l = model(left_input)\n",
        "encoded_r = model(right_input)\n",
        "\n",
        "subtracted = Subtract()([encoded_l, encoded_r])\n",
        "prediction = Dense(1, activation='sigmoid', bias_initializer=b_init)(subtracted)\n",
        "siamese_net = Model([left_input, right_input], prediction)\n",
        "\n",
        "optimizer= Adam(learning_rate=0.0006)\n",
        "siamese_net.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
        "\n",
        "plot_model(siamese_net, show_shapes=True, show_layer_names=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        },
        "id": "IHQmKOpuE_h9",
        "outputId": "49ad7963-fdba-40b7-d95f-f6547cf167a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def nway_one_shot(model, n_way, n_val):\n",
        "    \n",
        "    temp_x = x_val\n",
        "    temp_cat_list = cat_test\n",
        "    batch_x=[]\n",
        "    x_0_choice=[]\n",
        "    n_correct = 0\n",
        "   \n",
        "    class_list = np.random.randint(num_classes+1, len(folder_list)-1, n_val)\n",
        "\n",
        "    for i in class_list:  \n",
        "        j = np.random.choice(cat_list[i])\n",
        "        temp=[]\n",
        "        temp.append(np.zeros((n_way, 100, 100, 3)))\n",
        "        temp.append(np.zeros((n_way, 100, 100, 3)))\n",
        "        for k in range(0, n_way):\n",
        "            temp[0][k] = x[j]\n",
        "            \n",
        "            if k==0:\n",
        "                #print(i, k, j, np.random.choice(cat_list[i]))\n",
        "                temp[1][k] = x[np.random.choice(cat_list[i])]\n",
        "            else:\n",
        "                #print(i, k, j, np.random.choice(np.append(cat_list[:i].flatten(), cat_list[i+1:].flatten())))\n",
        "                temp[1][k] = x[np.random.choice(np.append(cat_list[:i].flatten(), cat_list[i+1:].flatten()))]\n",
        "\n",
        "        result = siamese_net.predict(temp)\n",
        "        result = result.flatten().tolist()\n",
        "        result_index = result.index(min(result))\n",
        "        if result_index == 0:\n",
        "            n_correct = n_correct + 1\n",
        "    print(n_correct, \"correctly classified among\", n_val)\n",
        "    accuracy = (n_correct*100)/n_val\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "aIUC2kcZFBEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 30000\n",
        "n_way = 20\n",
        "n_val = 100\n",
        "batch_size = 64\n",
        "\n",
        "loss_list=[]\n",
        "accuracy_list=[]\n",
        "for epoch in range(1,epochs):\n",
        "    batch_x, batch_y = get_batch(batch_size)\n",
        "    loss = siamese_net.train_on_batch(batch_x, batch_y)\n",
        "    loss_list.append((epoch,loss))\n",
        "    print('Epoch:', epoch, ', Loss:',loss)\n",
        "    if epoch%250 == 0:\n",
        "        print(\"=============================================\")\n",
        "        accuracy = nway_one_shot(model, n_way, n_val)\n",
        "        accuracy_list.append((epoch, accuracy))\n",
        "        print('Accuracy as of', epoch, 'epochs:', accuracy)\n",
        "        print(\"=============================================\")\n",
        "        if(accuracy>99):\n",
        "            print(\"Achieved more than 90% Accuracy\")\n",
        "            #break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgjUVsCBTqCQ",
        "outputId": "c43b0927-c60c-4c4c-836c-a4e502e2c96d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch: 25557 , Loss: 0.0004960311343893409\n",
            "Epoch: 25558 , Loss: 0.0005005330895073712\n",
            "Epoch: 25559 , Loss: 0.0004954634932801127\n",
            "Epoch: 25560 , Loss: 0.0004958545323461294\n",
            "Epoch: 25561 , Loss: 0.0004919614875689149\n",
            "Epoch: 25562 , Loss: 0.0004957318305969238\n",
            "Epoch: 25563 , Loss: 0.0005007173167541623\n",
            "Epoch: 25564 , Loss: 0.0004964363179169595\n",
            "Epoch: 25565 , Loss: 0.0004948038258589804\n",
            "Epoch: 25566 , Loss: 0.0004963933024555445\n",
            "Epoch: 25567 , Loss: 0.0004962299717590213\n",
            "Epoch: 25568 , Loss: 0.000500899797771126\n",
            "Epoch: 25569 , Loss: 0.0004953332245349884\n",
            "Epoch: 25570 , Loss: 0.0004930633585900068\n",
            "Epoch: 25571 , Loss: 0.00049511285033077\n",
            "Epoch: 25572 , Loss: 0.0004929535207338631\n",
            "Epoch: 25573 , Loss: 0.000494724081363529\n",
            "Epoch: 25574 , Loss: 0.0004951164009980857\n",
            "Epoch: 25575 , Loss: 0.0004941792576573789\n",
            "Epoch: 25576 , Loss: 0.0004991415189579129\n",
            "Epoch: 25577 , Loss: 0.0004986018757335842\n",
            "Epoch: 25578 , Loss: 0.0004888872499577701\n",
            "Epoch: 25579 , Loss: 0.0004947808920405805\n",
            "Epoch: 25580 , Loss: 0.0004886363167315722\n",
            "Epoch: 25581 , Loss: 0.0004941041115671396\n",
            "Epoch: 25582 , Loss: 0.0004915565950796008\n",
            "Epoch: 25583 , Loss: 0.0004945565015077591\n",
            "Epoch: 25584 , Loss: 0.0004953421885147691\n",
            "Epoch: 25585 , Loss: 0.0004917585174553096\n",
            "Epoch: 25586 , Loss: 0.0004943800158798695\n",
            "Epoch: 25587 , Loss: 0.000493317493237555\n",
            "Epoch: 25588 , Loss: 0.0004949997528456151\n",
            "Epoch: 25589 , Loss: 0.0004942211671732366\n",
            "Epoch: 25590 , Loss: 0.0004941573133692145\n",
            "Epoch: 25591 , Loss: 0.0004954246105626225\n",
            "Epoch: 25592 , Loss: 0.0004931655712425709\n",
            "Epoch: 25593 , Loss: 0.0004939943319186568\n",
            "Epoch: 25594 , Loss: 0.0004939386853948236\n",
            "Epoch: 25595 , Loss: 0.0004942924133501947\n",
            "Epoch: 25596 , Loss: 0.000497516943141818\n",
            "Epoch: 25597 , Loss: 0.0004940925864502788\n",
            "Epoch: 25598 , Loss: 0.00049372180365026\n",
            "Epoch: 25599 , Loss: 0.0004868708783760667\n",
            "Epoch: 25600 , Loss: 0.0004939960781484842\n",
            "Epoch: 25601 , Loss: 0.0004977747448720038\n",
            "Epoch: 25602 , Loss: 0.0004921656218357384\n",
            "Epoch: 25603 , Loss: 0.0004934512544423342\n",
            "Epoch: 25604 , Loss: 0.0004903092049062252\n",
            "Epoch: 25605 , Loss: 0.0005013276822865009\n",
            "Epoch: 25606 , Loss: 0.0004932474112138152\n",
            "Epoch: 25607 , Loss: 0.0004932669689878821\n",
            "Epoch: 25608 , Loss: 0.0004931443254463375\n",
            "Epoch: 25609 , Loss: 0.0004930919967591763\n",
            "Epoch: 25610 , Loss: 0.000498349720146507\n",
            "Epoch: 25611 , Loss: 0.000492983846925199\n",
            "Epoch: 25612 , Loss: 0.0004929223214276135\n",
            "Epoch: 25613 , Loss: 0.0004928877460770309\n",
            "Epoch: 25614 , Loss: 0.0004966838168911636\n",
            "Epoch: 25615 , Loss: 0.0004904067609459162\n",
            "Epoch: 25616 , Loss: 0.0004883477813564241\n",
            "Epoch: 25617 , Loss: 0.0004912609583698213\n",
            "Epoch: 25618 , Loss: 0.0004925690591335297\n",
            "Epoch: 25619 , Loss: 0.0004925507819280028\n",
            "Epoch: 25620 , Loss: 0.0004951352602802217\n",
            "Epoch: 25621 , Loss: 0.0004991054884158075\n",
            "Epoch: 25622 , Loss: 0.0004976174677722156\n",
            "Epoch: 25623 , Loss: 0.0004967524437233806\n",
            "Epoch: 25624 , Loss: 0.0004943338572047651\n",
            "Epoch: 25625 , Loss: 0.0004984539118595421\n",
            "Epoch: 25626 , Loss: 0.0004907283582724631\n",
            "Epoch: 25627 , Loss: 0.000490846112370491\n",
            "Epoch: 25628 , Loss: 0.0004921396030113101\n",
            "Epoch: 25629 , Loss: 0.0004920907085761428\n",
            "Epoch: 25630 , Loss: 0.0004920467035844922\n",
            "Epoch: 25631 , Loss: 0.0004919999046251178\n",
            "Epoch: 25632 , Loss: 0.0004957400960847735\n",
            "Epoch: 25633 , Loss: 0.000491902232170105\n",
            "Epoch: 25634 , Loss: 0.0004918525810353458\n",
            "Epoch: 25635 , Loss: 0.0005014087655581534\n",
            "Epoch: 25636 , Loss: 0.0004917621263302863\n",
            "Epoch: 25637 , Loss: 0.0004929788410663605\n",
            "Epoch: 25638 , Loss: 0.0004947285633534193\n",
            "Epoch: 25639 , Loss: 0.0004906269023194909\n",
            "Epoch: 25640 , Loss: 0.0004915313329547644\n",
            "Epoch: 25641 , Loss: 0.0004970855079591274\n",
            "Epoch: 25642 , Loss: 0.0004921691142953932\n",
            "Epoch: 25643 , Loss: 0.0004923461237922311\n",
            "Epoch: 25644 , Loss: 0.000491444778162986\n",
            "Epoch: 25645 , Loss: 0.0004983703838661313\n",
            "Epoch: 25646 , Loss: 0.0004939617356285453\n",
            "Epoch: 25647 , Loss: 0.0004913167795166373\n",
            "Epoch: 25648 , Loss: 0.0004912636359222233\n",
            "Epoch: 25649 , Loss: 0.0004885972593910992\n",
            "Epoch: 25650 , Loss: 0.0004893488949164748\n",
            "Epoch: 25651 , Loss: 0.00048815092304721475\n",
            "Epoch: 25652 , Loss: 0.0004915525787509978\n",
            "Epoch: 25653 , Loss: 0.0004940861836075783\n",
            "Epoch: 25654 , Loss: 0.0004909307463094592\n",
            "Epoch: 25655 , Loss: 0.0004947144770994782\n",
            "Epoch: 25656 , Loss: 0.0004914093879051507\n",
            "Epoch: 25657 , Loss: 0.0004924947861582041\n",
            "Epoch: 25658 , Loss: 0.00048745126696303487\n",
            "Epoch: 25659 , Loss: 0.0004944632528349757\n",
            "Epoch: 25660 , Loss: 0.0004870484699495137\n",
            "Epoch: 25661 , Loss: 0.0004949751310050488\n",
            "Epoch: 25662 , Loss: 0.0004888914409093559\n",
            "Epoch: 25663 , Loss: 0.0004905734676867723\n",
            "Epoch: 25664 , Loss: 0.00048695257282815874\n",
            "Epoch: 25665 , Loss: 0.00048743135994300246\n",
            "Epoch: 25666 , Loss: 0.0004921322106383741\n",
            "Epoch: 25667 , Loss: 0.0004950787988491356\n",
            "Epoch: 25668 , Loss: 0.0004902085056528449\n",
            "Epoch: 25669 , Loss: 0.0004874246660619974\n",
            "Epoch: 25670 , Loss: 0.0004900919739156961\n",
            "Epoch: 25671 , Loss: 0.0004900331259705126\n",
            "Epoch: 25672 , Loss: 0.0004900998901575804\n",
            "Epoch: 25673 , Loss: 0.0004899131017737091\n",
            "Epoch: 25674 , Loss: 0.0004932290758006275\n",
            "Epoch: 25675 , Loss: 0.00048743729712441564\n",
            "Epoch: 25676 , Loss: 0.0004949934082105756\n",
            "Epoch: 25677 , Loss: 0.0004896898753941059\n",
            "Epoch: 25678 , Loss: 0.0004887827672064304\n",
            "Epoch: 25679 , Loss: 0.0004909418639726937\n",
            "Epoch: 25680 , Loss: 0.0004895178717561066\n",
            "Epoch: 25681 , Loss: 0.0004891255521215498\n",
            "Epoch: 25682 , Loss: 0.000489391852170229\n",
            "Epoch: 25683 , Loss: 0.000486481876578182\n",
            "Epoch: 25684 , Loss: 0.00048252183478325605\n",
            "Epoch: 25685 , Loss: 0.0004903487279079854\n",
            "Epoch: 25686 , Loss: 0.0004894491285085678\n",
            "Epoch: 25687 , Loss: 0.0004945284454151988\n",
            "Epoch: 25688 , Loss: 0.0004941715742461383\n",
            "Epoch: 25689 , Loss: 0.0004893030854873359\n",
            "Epoch: 25690 , Loss: 0.0004889022675342858\n",
            "Epoch: 25691 , Loss: 0.0004895105957984924\n",
            "Epoch: 25692 , Loss: 0.0004903213120996952\n",
            "Epoch: 25693 , Loss: 0.0004887930699624121\n",
            "Epoch: 25694 , Loss: 0.0004944838583469391\n",
            "Epoch: 25695 , Loss: 0.0004906380781903863\n",
            "Epoch: 25696 , Loss: 0.0004894883604720235\n",
            "Epoch: 25697 , Loss: 0.000489929283503443\n",
            "Epoch: 25698 , Loss: 0.000489603728055954\n",
            "Epoch: 25699 , Loss: 0.0004884937079623342\n",
            "Epoch: 25700 , Loss: 0.000491693033836782\n",
            "Epoch: 25701 , Loss: 0.00048821335076354444\n",
            "Epoch: 25702 , Loss: 0.0004907748661935329\n",
            "Epoch: 25703 , Loss: 0.0004885055823251605\n",
            "Epoch: 25704 , Loss: 0.0004887547111138701\n",
            "Epoch: 25705 , Loss: 0.0004884651280008256\n",
            "Epoch: 25706 , Loss: 0.00048826425336301327\n",
            "Epoch: 25707 , Loss: 0.0004863537906203419\n",
            "Epoch: 25708 , Loss: 0.0004883609944954515\n",
            "Epoch: 25709 , Loss: 0.0004883264191448689\n",
            "Epoch: 25710 , Loss: 0.0004883161745965481\n",
            "Epoch: 25711 , Loss: 0.0004924083477817476\n",
            "Epoch: 25712 , Loss: 0.000492348219268024\n",
            "Epoch: 25713 , Loss: 0.0004858665633946657\n",
            "Epoch: 25714 , Loss: 0.0004924493841826916\n",
            "Epoch: 25715 , Loss: 0.0004912278964184225\n",
            "Epoch: 25716 , Loss: 0.00048274642904289067\n",
            "Epoch: 25717 , Loss: 0.0004935216857120395\n",
            "Epoch: 25718 , Loss: 0.0004909982089884579\n",
            "Epoch: 25719 , Loss: 0.0004877827886957675\n",
            "Epoch: 25720 , Loss: 0.0004877206520177424\n",
            "Epoch: 25721 , Loss: 0.00048771282308734953\n",
            "Epoch: 25722 , Loss: 0.0004873388388659805\n",
            "Epoch: 25723 , Loss: 0.00048752821749076247\n",
            "Epoch: 25724 , Loss: 0.00048812240129336715\n",
            "Epoch: 25725 , Loss: 0.0004873658181168139\n",
            "Epoch: 25726 , Loss: 0.0004843660572078079\n",
            "Epoch: 25727 , Loss: 0.0004912511794827878\n",
            "Epoch: 25728 , Loss: 0.00048743796651251614\n",
            "Epoch: 25729 , Loss: 0.00048730982234701514\n",
            "Epoch: 25730 , Loss: 0.0004889403935521841\n",
            "Epoch: 25731 , Loss: 0.0004908456467092037\n",
            "Epoch: 25732 , Loss: 0.00048695720033720136\n",
            "Epoch: 25733 , Loss: 0.0004850291588809341\n",
            "Epoch: 25734 , Loss: 0.00048755144234746695\n",
            "Epoch: 25735 , Loss: 0.0004850600380450487\n",
            "Epoch: 25736 , Loss: 0.00048746250104159117\n",
            "Epoch: 25737 , Loss: 0.00048672681441530585\n",
            "Epoch: 25738 , Loss: 0.00048645868082530797\n",
            "Epoch: 25739 , Loss: 0.0004866301023866981\n",
            "Epoch: 25740 , Loss: 0.0004935392062179744\n",
            "Epoch: 25741 , Loss: 0.0004904417437501252\n",
            "Epoch: 25742 , Loss: 0.00048689378309063613\n",
            "Epoch: 25743 , Loss: 0.0004925605608150363\n",
            "Epoch: 25744 , Loss: 0.0004902718355879188\n",
            "Epoch: 25745 , Loss: 0.0004866530653089285\n",
            "Epoch: 25746 , Loss: 0.00048388971481472254\n",
            "Epoch: 25747 , Loss: 0.00048324093222618103\n",
            "Epoch: 25748 , Loss: 0.0004861753259319812\n",
            "Epoch: 25749 , Loss: 0.0004876950988546014\n",
            "Epoch: 25750 , Loss: 0.00048309628618881106\n",
            "=============================================\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "66 correctly classified among 100\n",
            "Accuracy as of 25750 epochs: 66.0\n",
            "=============================================\n",
            "Epoch: 25751 , Loss: 0.00048634421546012163\n",
            "Epoch: 25752 , Loss: 0.0004865053342655301\n",
            "Epoch: 25753 , Loss: 0.0004904396482743323\n",
            "Epoch: 25754 , Loss: 0.0004861353081651032\n",
            "Epoch: 25755 , Loss: 0.0004830017569474876\n",
            "Epoch: 25756 , Loss: 0.00048668321687728167\n",
            "Epoch: 25757 , Loss: 0.0004848678654525429\n",
            "Epoch: 25758 , Loss: 0.0004886416136287153\n",
            "Epoch: 25759 , Loss: 0.00048515701200813055\n",
            "Epoch: 25760 , Loss: 0.00048305687960237265\n",
            "Epoch: 25761 , Loss: 0.00048503748257644475\n",
            "Epoch: 25762 , Loss: 0.00048562284791842103\n",
            "Epoch: 25763 , Loss: 0.0004853796272072941\n",
            "Epoch: 25764 , Loss: 0.00048408543807454407\n",
            "Epoch: 25765 , Loss: 0.00048526391037739813\n",
            "Epoch: 25766 , Loss: 0.0004851477569900453\n",
            "Epoch: 25767 , Loss: 0.0004829918616451323\n",
            "Epoch: 25768 , Loss: 0.00048315676394850016\n",
            "Epoch: 25769 , Loss: 0.0004844125942327082\n",
            "Epoch: 25770 , Loss: 0.00048557380796410143\n",
            "Epoch: 25771 , Loss: 0.00048229063395410776\n",
            "Epoch: 25772 , Loss: 0.0004848564276471734\n",
            "Epoch: 25773 , Loss: 0.0004847977834288031\n",
            "Epoch: 25774 , Loss: 0.0004878212930634618\n",
            "Epoch: 25775 , Loss: 0.00048464813153259456\n",
            "Epoch: 25776 , Loss: 0.0004838144523091614\n",
            "Epoch: 25777 , Loss: 0.00048215602873824537\n",
            "Epoch: 25778 , Loss: 0.0004895315505564213\n",
            "Epoch: 25779 , Loss: 0.0004812892875634134\n",
            "Epoch: 25780 , Loss: 0.0004843924252782017\n",
            "Epoch: 25781 , Loss: 0.0004833509447053075\n",
            "Epoch: 25782 , Loss: 0.00048512720968574286\n",
            "Epoch: 25783 , Loss: 0.00048537406837567687\n",
            "Epoch: 25784 , Loss: 0.0004872996942140162\n",
            "Epoch: 25785 , Loss: 0.00048467208398506045\n",
            "Epoch: 25786 , Loss: 0.0004815243592020124\n",
            "Epoch: 25787 , Loss: 0.0004812711849808693\n",
            "Epoch: 25788 , Loss: 0.0004873946600127965\n",
            "Epoch: 25789 , Loss: 0.0004839313041884452\n",
            "Epoch: 25790 , Loss: 0.000487044220790267\n",
            "Epoch: 25791 , Loss: 0.00048382856766693294\n",
            "Epoch: 25792 , Loss: 0.0004837762680836022\n",
            "Epoch: 25793 , Loss: 0.00048135590623132885\n",
            "Epoch: 25794 , Loss: 0.0004837688466068357\n",
            "Epoch: 25795 , Loss: 0.0004836045845877379\n",
            "Epoch: 25796 , Loss: 0.0004831050173379481\n",
            "Epoch: 25797 , Loss: 0.0004866328090429306\n",
            "Epoch: 25798 , Loss: 0.0004815142892766744\n",
            "Epoch: 25799 , Loss: 0.0004883056390099227\n",
            "Epoch: 25800 , Loss: 0.0004823524795938283\n",
            "Epoch: 25801 , Loss: 0.0004873764992225915\n",
            "Epoch: 25802 , Loss: 0.0004808545927517116\n",
            "Epoch: 25803 , Loss: 0.0004826239892281592\n",
            "Epoch: 25804 , Loss: 0.00048324433737434447\n",
            "Epoch: 25805 , Loss: 0.00048700429033488035\n",
            "Epoch: 25806 , Loss: 0.00048257139860652387\n",
            "Epoch: 25807 , Loss: 0.0004829898534808308\n",
            "Epoch: 25808 , Loss: 0.00048470075125806034\n",
            "Epoch: 25809 , Loss: 0.0004811174585483968\n",
            "Epoch: 25810 , Loss: 0.00048182380851358175\n",
            "Epoch: 25811 , Loss: 0.00048038948443718255\n",
            "Epoch: 25812 , Loss: 0.00048209185479208827\n",
            "Epoch: 25813 , Loss: 0.0004840391338802874\n",
            "Epoch: 25814 , Loss: 0.0004824978532269597\n",
            "Epoch: 25815 , Loss: 0.000478334870422259\n",
            "Epoch: 25816 , Loss: 0.00048298854380846024\n",
            "Epoch: 25817 , Loss: 0.00048670166870579123\n",
            "Epoch: 25818 , Loss: 0.00048155197873711586\n",
            "Epoch: 25819 , Loss: 0.0004823193303309381\n",
            "Epoch: 25820 , Loss: 0.0004887941177003086\n",
            "Epoch: 25821 , Loss: 0.00047770963283255696\n",
            "Epoch: 25822 , Loss: 0.00048213297850452363\n",
            "Epoch: 25823 , Loss: 0.00048171437811106443\n",
            "Epoch: 25824 , Loss: 0.00048109859926626086\n",
            "Epoch: 25825 , Loss: 0.0004831491387449205\n",
            "Epoch: 25826 , Loss: 0.00048125110333785415\n",
            "Epoch: 25827 , Loss: 0.0004879879707004875\n",
            "Epoch: 25828 , Loss: 0.0004821432230528444\n",
            "Epoch: 25829 , Loss: 0.00048250705003738403\n",
            "Epoch: 25830 , Loss: 0.00048171577509492636\n",
            "Epoch: 25831 , Loss: 0.00048284183139912784\n",
            "Epoch: 25832 , Loss: 0.00048161917948164046\n",
            "Epoch: 25833 , Loss: 0.00048492412315681577\n",
            "Epoch: 25834 , Loss: 0.00048230294487439096\n",
            "Epoch: 25835 , Loss: 0.0004814889107365161\n",
            "Epoch: 25836 , Loss: 0.00048144665197469294\n",
            "Epoch: 25837 , Loss: 0.0004817080625798553\n",
            "Epoch: 25838 , Loss: 0.0004787255311384797\n",
            "Epoch: 25839 , Loss: 0.0004813140840269625\n",
            "Epoch: 25840 , Loss: 0.00048128937487490475\n",
            "Epoch: 25841 , Loss: 0.00048159086145460606\n",
            "Epoch: 25842 , Loss: 0.0004792773397639394\n",
            "Epoch: 25843 , Loss: 0.00048091242206282914\n",
            "Epoch: 25844 , Loss: 0.0004813346895389259\n",
            "Epoch: 25845 , Loss: 0.00048420875100418925\n",
            "Epoch: 25846 , Loss: 0.00048150753718800843\n",
            "Epoch: 25847 , Loss: 0.00047806347720324993\n",
            "Epoch: 25848 , Loss: 0.0004808230441994965\n",
            "Epoch: 25849 , Loss: 0.0004796762950718403\n",
            "Epoch: 25850 , Loss: 0.00048017181688919663\n",
            "Epoch: 25851 , Loss: 0.00048121181316673756\n",
            "Epoch: 25852 , Loss: 0.00048156280536204576\n",
            "Epoch: 25853 , Loss: 0.00048051844350993633\n",
            "Epoch: 25854 , Loss: 0.00047915949835442007\n",
            "Epoch: 25855 , Loss: 0.00048128474736586213\n",
            "Epoch: 25856 , Loss: 0.0004816059081349522\n",
            "Epoch: 25857 , Loss: 0.0004846355877816677\n",
            "Epoch: 25858 , Loss: 0.00048081905697472394\n",
            "Epoch: 25859 , Loss: 0.0004746182239614427\n",
            "Epoch: 25860 , Loss: 0.0004780302697326988\n",
            "Epoch: 25861 , Loss: 0.00047872107825241983\n",
            "Epoch: 25862 , Loss: 0.0004808458033949137\n",
            "Epoch: 25863 , Loss: 0.00047857104800641537\n",
            "Epoch: 25864 , Loss: 0.00047992984764277935\n",
            "Epoch: 25865 , Loss: 0.0004774806438945234\n",
            "Epoch: 25866 , Loss: 0.0004789697704836726\n",
            "Epoch: 25867 , Loss: 0.00047847104724496603\n",
            "Epoch: 25868 , Loss: 0.00048100759158842266\n",
            "Epoch: 25869 , Loss: 0.0004840231849811971\n",
            "Epoch: 25870 , Loss: 0.00047849660040810704\n",
            "Epoch: 25871 , Loss: 0.0004866121453233063\n",
            "Epoch: 25872 , Loss: 0.00048001870163716376\n",
            "Epoch: 25873 , Loss: 0.0004839146276935935\n",
            "Epoch: 25874 , Loss: 0.0004794651176780462\n",
            "Epoch: 25875 , Loss: 0.000479408772662282\n",
            "Epoch: 25876 , Loss: 0.0004790817911271006\n",
            "Epoch: 25877 , Loss: 0.0004791826650034636\n",
            "Epoch: 25878 , Loss: 0.0004919064231216908\n",
            "Epoch: 25879 , Loss: 0.0004790836828760803\n",
            "Epoch: 25880 , Loss: 0.00047903507947921753\n",
            "Epoch: 25881 , Loss: 0.00047660424024797976\n",
            "Epoch: 25882 , Loss: 0.00047836394514888525\n",
            "Epoch: 25883 , Loss: 0.0004779623413924128\n",
            "Epoch: 25884 , Loss: 0.0004843601491302252\n",
            "Epoch: 25885 , Loss: 0.00047875536256469786\n",
            "Epoch: 25886 , Loss: 0.00048427091678604484\n",
            "Epoch: 25887 , Loss: 0.0004786441568285227\n",
            "Epoch: 25888 , Loss: 0.0004817935114260763\n",
            "Epoch: 25889 , Loss: 0.00047855923185124993\n",
            "Epoch: 25890 , Loss: 0.00047850218834355474\n",
            "Epoch: 25891 , Loss: 0.0004753793473355472\n",
            "Epoch: 25892 , Loss: 0.0004751424421556294\n",
            "Epoch: 25893 , Loss: 0.0004828764358535409\n",
            "Epoch: 25894 , Loss: 0.0004779021255671978\n",
            "Epoch: 25895 , Loss: 0.00047587696462869644\n",
            "Epoch: 25896 , Loss: 0.00048111367505043745\n",
            "Epoch: 25897 , Loss: 0.0004782728501595557\n",
            "Epoch: 25898 , Loss: 0.00047835736768320203\n",
            "Epoch: 25899 , Loss: 0.00047557998914271593\n",
            "Epoch: 25900 , Loss: 0.00048825491103343666\n",
            "Epoch: 25901 , Loss: 0.0004777768044732511\n",
            "Epoch: 25902 , Loss: 0.00048394413897767663\n",
            "Epoch: 25903 , Loss: 0.00047783408081158996\n",
            "Epoch: 25904 , Loss: 0.0004778741276822984\n",
            "Epoch: 25905 , Loss: 0.00047787762014195323\n",
            "Epoch: 25906 , Loss: 0.00047708681086078286\n",
            "Epoch: 25907 , Loss: 0.0004782196774613112\n",
            "Epoch: 25908 , Loss: 0.0004794000706169754\n",
            "Epoch: 25909 , Loss: 0.0004807312798220664\n",
            "Epoch: 25910 , Loss: 0.0004775646375492215\n",
            "Epoch: 25911 , Loss: 0.00047649061889387667\n",
            "Epoch: 25912 , Loss: 0.0004779125447385013\n",
            "Epoch: 25913 , Loss: 0.0004745967744383961\n",
            "Epoch: 25914 , Loss: 0.0004774006665684283\n",
            "Epoch: 25915 , Loss: 0.00047723695752210915\n",
            "Epoch: 25916 , Loss: 0.00048149528447538614\n",
            "Epoch: 25917 , Loss: 0.0004771608510054648\n",
            "Epoch: 25918 , Loss: 0.00047666957834735513\n",
            "Epoch: 25919 , Loss: 0.000477215857245028\n",
            "Epoch: 25920 , Loss: 0.0004816493019461632\n",
            "Epoch: 25921 , Loss: 0.00047691294457763433\n",
            "Epoch: 25922 , Loss: 0.0004808397206943482\n",
            "Epoch: 25923 , Loss: 0.0004744498000945896\n",
            "Epoch: 25924 , Loss: 0.00048044329741969705\n",
            "Epoch: 25925 , Loss: 0.0004775126581080258\n",
            "Epoch: 25926 , Loss: 0.0004766277561429888\n",
            "Epoch: 25927 , Loss: 0.00047806903603486717\n",
            "Epoch: 25928 , Loss: 0.0004765249614138156\n",
            "Epoch: 25929 , Loss: 0.000476061919471249\n",
            "Epoch: 25930 , Loss: 0.0004765917547047138\n",
            "Epoch: 25931 , Loss: 0.0004758031864184886\n",
            "Epoch: 25932 , Loss: 0.00047977594658732414\n",
            "Epoch: 25933 , Loss: 0.0004760432639159262\n",
            "Epoch: 25934 , Loss: 0.00047380157047882676\n",
            "Epoch: 25935 , Loss: 0.0004761342715937644\n",
            "Epoch: 25936 , Loss: 0.0004767747886944562\n",
            "Epoch: 25937 , Loss: 0.00047615543007850647\n",
            "Epoch: 25938 , Loss: 0.0004758402646984905\n",
            "Epoch: 25939 , Loss: 0.00047408125828951597\n",
            "Epoch: 25940 , Loss: 0.00047585024731233716\n",
            "Epoch: 25941 , Loss: 0.00047827442176640034\n",
            "Epoch: 25942 , Loss: 0.0004757202696055174\n",
            "Epoch: 25943 , Loss: 0.00047567838919349015\n",
            "Epoch: 25944 , Loss: 0.00047547128633596003\n",
            "Epoch: 25945 , Loss: 0.0004755404661409557\n",
            "Epoch: 25946 , Loss: 0.0004754764959216118\n",
            "Epoch: 25947 , Loss: 0.0004754314140882343\n",
            "Epoch: 25948 , Loss: 0.000478673871839419\n",
            "Epoch: 25949 , Loss: 0.00047550263116136193\n",
            "Epoch: 25950 , Loss: 0.00047289443318732083\n",
            "Epoch: 25951 , Loss: 0.0004751881642732769\n",
            "Epoch: 25952 , Loss: 0.000479239592095837\n",
            "Epoch: 25953 , Loss: 0.0004745679907500744\n",
            "Epoch: 25954 , Loss: 0.00048014475032687187\n",
            "Epoch: 25955 , Loss: 0.00047181019908748567\n",
            "Epoch: 25956 , Loss: 0.0004740555596072227\n",
            "Epoch: 25957 , Loss: 0.00047218025429174304\n",
            "Epoch: 25958 , Loss: 0.00047485888353548944\n",
            "Epoch: 25959 , Loss: 0.0004748022183775902\n",
            "Epoch: 25960 , Loss: 0.0004747699713334441\n",
            "Epoch: 25961 , Loss: 0.0004746286431327462\n",
            "Epoch: 25962 , Loss: 0.000474661763291806\n",
            "Epoch: 25963 , Loss: 0.00047462075599469244\n",
            "Epoch: 25964 , Loss: 0.0004767845384776592\n",
            "Epoch: 25965 , Loss: 0.00047789490781724453\n",
            "Epoch: 25966 , Loss: 0.00047198199899867177\n",
            "Epoch: 25967 , Loss: 0.0004745528567582369\n",
            "Epoch: 25968 , Loss: 0.0004744500620290637\n",
            "Epoch: 25969 , Loss: 0.0004775429260917008\n",
            "Epoch: 25970 , Loss: 0.0004736567207146436\n",
            "Epoch: 25971 , Loss: 0.00047470873687416315\n",
            "Epoch: 25972 , Loss: 0.00047370552783831954\n",
            "Epoch: 25973 , Loss: 0.0004741199954878539\n",
            "Epoch: 25974 , Loss: 0.0004721961449831724\n",
            "Epoch: 25975 , Loss: 0.0004740691219922155\n",
            "Epoch: 25976 , Loss: 0.00047775599523447454\n",
            "Epoch: 25977 , Loss: 0.00047389513929374516\n",
            "Epoch: 25978 , Loss: 0.00047600982361473143\n",
            "Epoch: 25979 , Loss: 0.0004737488052342087\n",
            "Epoch: 25980 , Loss: 0.0004773131222464144\n",
            "Epoch: 25981 , Loss: 0.0004715753020718694\n",
            "Epoch: 25982 , Loss: 0.00047355066635645926\n",
            "Epoch: 25983 , Loss: 0.0004734891699627042\n",
            "Epoch: 25984 , Loss: 0.0004702809383161366\n",
            "Epoch: 25985 , Loss: 0.0004732091911137104\n",
            "Epoch: 25986 , Loss: 0.0004734733665827662\n",
            "Epoch: 25987 , Loss: 0.0004770045052282512\n",
            "Epoch: 25988 , Loss: 0.0004731692315544933\n",
            "Epoch: 25989 , Loss: 0.0004751570522785187\n",
            "Epoch: 25990 , Loss: 0.00047296605771407485\n",
            "Epoch: 25991 , Loss: 0.0004727075865957886\n",
            "Epoch: 25992 , Loss: 0.00047170455218292773\n",
            "Epoch: 25993 , Loss: 0.00047314303810708225\n",
            "Epoch: 25994 , Loss: 0.0004730846849270165\n",
            "Epoch: 25995 , Loss: 0.0004682701837737113\n",
            "Epoch: 25996 , Loss: 0.0004738467396236956\n",
            "Epoch: 25997 , Loss: 0.00047183677088469267\n",
            "Epoch: 25998 , Loss: 0.0004712723311968148\n",
            "Epoch: 25999 , Loss: 0.0004748052451759577\n",
            "Epoch: 26000 , Loss: 0.00047466601245105267\n",
            "=============================================\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "41 correctly classified among 100\n",
            "Accuracy as of 26000 epochs: 41.0\n",
            "=============================================\n",
            "Epoch: 26001 , Loss: 0.00047600315883755684\n",
            "Epoch: 26002 , Loss: 0.0004732206289190799\n",
            "Epoch: 26003 , Loss: 0.00047573933261446655\n",
            "Epoch: 26004 , Loss: 0.0004722687881439924\n",
            "Epoch: 26005 , Loss: 0.0004722312733065337\n",
            "Epoch: 26006 , Loss: 0.0004719466087408364\n",
            "Epoch: 26007 , Loss: 0.0004764023469761014\n",
            "Epoch: 26008 , Loss: 0.0004721163713838905\n",
            "Epoch: 26009 , Loss: 0.0004705139435827732\n",
            "Epoch: 26010 , Loss: 0.0004721505392808467\n",
            "Epoch: 26011 , Loss: 0.000469500373583287\n",
            "Epoch: 26012 , Loss: 0.0004718525742646307\n",
            "Epoch: 26013 , Loss: 0.00047281343722715974\n",
            "Epoch: 26014 , Loss: 0.0004724774044007063\n",
            "Epoch: 26015 , Loss: 0.00047237222315743566\n",
            "Epoch: 26016 , Loss: 0.0004717900592368096\n",
            "Epoch: 26017 , Loss: 0.0004718662239611149\n",
            "Epoch: 26018 , Loss: 0.0004717137198895216\n",
            "Epoch: 26019 , Loss: 0.0004710716020781547\n",
            "Epoch: 26020 , Loss: 0.00047211957280524075\n",
            "Epoch: 26021 , Loss: 0.00047205755254253745\n",
            "Epoch: 26022 , Loss: 0.00047086848644539714\n",
            "Epoch: 26023 , Loss: 0.0004698415577877313\n",
            "Epoch: 26024 , Loss: 0.00046898581786081195\n",
            "Epoch: 26025 , Loss: 0.00046942359767854214\n",
            "Epoch: 26026 , Loss: 0.00046947566443122923\n",
            "Epoch: 26027 , Loss: 0.00047118752263486385\n",
            "Epoch: 26028 , Loss: 0.0004685356398113072\n",
            "Epoch: 26029 , Loss: 0.00047095646732486784\n",
            "Epoch: 26030 , Loss: 0.0004719216958619654\n",
            "Epoch: 26031 , Loss: 0.0004707750049419701\n",
            "Epoch: 26032 , Loss: 0.00046981649938970804\n",
            "Epoch: 26033 , Loss: 0.0004685151216108352\n",
            "Epoch: 26034 , Loss: 0.00047206159797497094\n",
            "Epoch: 26035 , Loss: 0.0004750431689899415\n",
            "Epoch: 26036 , Loss: 0.0004755957634188235\n",
            "Epoch: 26037 , Loss: 0.0004684134910348803\n",
            "Epoch: 26038 , Loss: 0.00046795926755294204\n",
            "Epoch: 26039 , Loss: 0.00047938781790435314\n",
            "Epoch: 26040 , Loss: 0.00046942898188717663\n",
            "Epoch: 26041 , Loss: 0.0004732348315883428\n",
            "Epoch: 26042 , Loss: 0.0004683069128077477\n",
            "Epoch: 26043 , Loss: 0.00046929437667131424\n",
            "Epoch: 26044 , Loss: 0.0004703262238763273\n",
            "Epoch: 26045 , Loss: 0.0004700751742348075\n",
            "Epoch: 26046 , Loss: 0.00047255901154130697\n",
            "Epoch: 26047 , Loss: 0.00047253890079446137\n",
            "Epoch: 26048 , Loss: 0.00047406688099727035\n",
            "Epoch: 26049 , Loss: 0.0004744575417134911\n",
            "Epoch: 26050 , Loss: 0.0004690256027970463\n",
            "Epoch: 26051 , Loss: 0.00046843779273331165\n",
            "Epoch: 26052 , Loss: 0.00046975130680948496\n",
            "Epoch: 26053 , Loss: 0.000469614373287186\n",
            "Epoch: 26054 , Loss: 0.00047126755816861987\n",
            "Epoch: 26055 , Loss: 0.00046660006046295166\n",
            "Epoch: 26056 , Loss: 0.00047089066356420517\n",
            "Epoch: 26057 , Loss: 0.0004734998510684818\n",
            "Epoch: 26058 , Loss: 0.00046934871352277696\n",
            "Epoch: 26059 , Loss: 0.0004729150386992842\n",
            "Epoch: 26060 , Loss: 0.00047328954678960145\n",
            "Epoch: 26061 , Loss: 0.0004744564648717642\n",
            "Epoch: 26062 , Loss: 0.0004671386268455535\n",
            "Epoch: 26063 , Loss: 0.00046894815750420094\n",
            "Epoch: 26064 , Loss: 0.0004690486821345985\n",
            "Epoch: 26065 , Loss: 0.00047058676136657596\n",
            "Epoch: 26066 , Loss: 0.0004731317749246955\n",
            "Epoch: 26067 , Loss: 0.00047041053767316043\n",
            "Epoch: 26068 , Loss: 0.000468813581392169\n",
            "Epoch: 26069 , Loss: 0.0004687827895395458\n",
            "Epoch: 26070 , Loss: 0.00046873142127878964\n",
            "Epoch: 26071 , Loss: 0.0004686785105150193\n",
            "Epoch: 26072 , Loss: 0.00047210388584062457\n",
            "Epoch: 26073 , Loss: 0.00047049985732883215\n",
            "Epoch: 26074 , Loss: 0.0004683462902903557\n",
            "Epoch: 26075 , Loss: 0.0004704838211182505\n",
            "Epoch: 26076 , Loss: 0.00047259958228096366\n",
            "Epoch: 26077 , Loss: 0.00047129561426118016\n",
            "Epoch: 26078 , Loss: 0.00047091342275962234\n",
            "Epoch: 26079 , Loss: 0.00046621396904811263\n",
            "Epoch: 26080 , Loss: 0.00046977325109764934\n",
            "Epoch: 26081 , Loss: 0.0004669964255299419\n",
            "Epoch: 26082 , Loss: 0.00047009674017317593\n",
            "Epoch: 26083 , Loss: 0.0004612248740158975\n",
            "Epoch: 26084 , Loss: 0.0004679461708292365\n",
            "Epoch: 26085 , Loss: 0.00046813578228466213\n",
            "Epoch: 26086 , Loss: 0.0004774598637595773\n",
            "Epoch: 26087 , Loss: 0.00047232344513759017\n",
            "Epoch: 26088 , Loss: 0.00046537001617252827\n",
            "Epoch: 26089 , Loss: 0.0004713901726063341\n",
            "Epoch: 26090 , Loss: 0.0004694864619523287\n",
            "Epoch: 26091 , Loss: 0.00046773708891123533\n",
            "Epoch: 26092 , Loss: 0.0004676112439483404\n",
            "Epoch: 26093 , Loss: 0.00046869844663888216\n",
            "Epoch: 26094 , Loss: 0.00046897673746570945\n",
            "Epoch: 26095 , Loss: 0.00047123312833718956\n",
            "Epoch: 26096 , Loss: 0.00046743627171963453\n",
            "Epoch: 26097 , Loss: 0.00046741293044760823\n",
            "Epoch: 26098 , Loss: 0.00046967912930995226\n",
            "Epoch: 26099 , Loss: 0.0004616210935637355\n",
            "Epoch: 26100 , Loss: 0.00046911442768760026\n",
            "Epoch: 26101 , Loss: 0.0004703494778368622\n",
            "Epoch: 26102 , Loss: 0.0004686651809606701\n",
            "Epoch: 26103 , Loss: 0.0004673311486840248\n",
            "Epoch: 26104 , Loss: 0.0004666688619181514\n",
            "Epoch: 26105 , Loss: 0.0004704166785813868\n",
            "Epoch: 26106 , Loss: 0.0004705336759798229\n",
            "Epoch: 26107 , Loss: 0.00046684418339282274\n",
            "Epoch: 26108 , Loss: 0.0004646412271540612\n",
            "Epoch: 26109 , Loss: 0.00046677462523803115\n",
            "Epoch: 26110 , Loss: 0.0004695722309406847\n",
            "Epoch: 26111 , Loss: 0.0004645232402253896\n",
            "Epoch: 26112 , Loss: 0.0004693919909186661\n",
            "Epoch: 26113 , Loss: 0.00046697616926394403\n",
            "Epoch: 26114 , Loss: 0.00046809972263872623\n",
            "Epoch: 26115 , Loss: 0.0004641559789888561\n",
            "Epoch: 26116 , Loss: 0.0004630936309695244\n",
            "Epoch: 26117 , Loss: 0.00046967933303676546\n",
            "Epoch: 26118 , Loss: 0.0004648649482987821\n",
            "Epoch: 26119 , Loss: 0.00046605110401287675\n",
            "Epoch: 26120 , Loss: 0.0004705519531853497\n",
            "Epoch: 26121 , Loss: 0.000465946999611333\n",
            "Epoch: 26122 , Loss: 0.0004640893021132797\n",
            "Epoch: 26123 , Loss: 0.00046357919927686453\n",
            "Epoch: 26124 , Loss: 0.00047078076750040054\n",
            "Epoch: 26125 , Loss: 0.0004657043027691543\n",
            "Epoch: 26126 , Loss: 0.0004633941571228206\n",
            "Epoch: 26127 , Loss: 0.00046558951726183295\n",
            "Epoch: 26128 , Loss: 0.00046420295257121325\n",
            "Epoch: 26129 , Loss: 0.00046161096543073654\n",
            "Epoch: 26130 , Loss: 0.0004656839300878346\n",
            "Epoch: 26131 , Loss: 0.00046722200931981206\n",
            "Epoch: 26132 , Loss: 0.0004652093630284071\n",
            "Epoch: 26133 , Loss: 0.0004629973554983735\n",
            "Epoch: 26134 , Loss: 0.0004676681128330529\n",
            "Epoch: 26135 , Loss: 0.000469918770249933\n",
            "Epoch: 26136 , Loss: 0.00046510776155628264\n",
            "Epoch: 26137 , Loss: 0.0004669075715355575\n",
            "Epoch: 26138 , Loss: 0.0004648945468943566\n",
            "Epoch: 26139 , Loss: 0.0004648368339985609\n",
            "Epoch: 26140 , Loss: 0.00046757241943851113\n",
            "Epoch: 26141 , Loss: 0.00046522144111804664\n",
            "Epoch: 26142 , Loss: 0.0004585007263813168\n",
            "Epoch: 26143 , Loss: 0.00046681842650286853\n",
            "Epoch: 26144 , Loss: 0.0004645422159228474\n",
            "Epoch: 26145 , Loss: 0.0004652115167118609\n",
            "Epoch: 26146 , Loss: 0.00046264054253697395\n",
            "Epoch: 26147 , Loss: 0.0004644382279366255\n",
            "Epoch: 26148 , Loss: 0.0004646071756724268\n",
            "Epoch: 26149 , Loss: 0.0004669949703384191\n",
            "Epoch: 26150 , Loss: 0.00046426214976236224\n",
            "Epoch: 26151 , Loss: 0.0004700517747551203\n",
            "Epoch: 26152 , Loss: 0.0004641885752789676\n",
            "Epoch: 26153 , Loss: 0.0004644273140002042\n",
            "Epoch: 26154 , Loss: 0.0004659385303966701\n",
            "Epoch: 26155 , Loss: 0.0004639416001737118\n",
            "Epoch: 26156 , Loss: 0.0004621399275492877\n",
            "Epoch: 26157 , Loss: 0.0004628206370398402\n",
            "Epoch: 26158 , Loss: 0.00046295381616801023\n",
            "Epoch: 26159 , Loss: 0.00046190497232601047\n",
            "Epoch: 26160 , Loss: 0.0004638595855794847\n",
            "Epoch: 26161 , Loss: 0.0004665250307880342\n",
            "Epoch: 26162 , Loss: 0.000466833240352571\n",
            "Epoch: 26163 , Loss: 0.0004636578378267586\n",
            "Epoch: 26164 , Loss: 0.00046916207065805793\n",
            "Epoch: 26165 , Loss: 0.0004627551243174821\n",
            "Epoch: 26166 , Loss: 0.00046348373871296644\n",
            "Epoch: 26167 , Loss: 0.0004627097223419696\n",
            "Epoch: 26168 , Loss: 0.0004655108496081084\n",
            "Epoch: 26169 , Loss: 0.0004639819962903857\n",
            "Epoch: 26170 , Loss: 0.00046322395792230964\n",
            "Epoch: 26171 , Loss: 0.00046430324437096715\n",
            "Epoch: 26172 , Loss: 0.00046771817142143846\n",
            "Epoch: 26173 , Loss: 0.0004630316107068211\n",
            "Epoch: 26174 , Loss: 0.00046138803008943796\n",
            "Epoch: 26175 , Loss: 0.0004628511960618198\n",
            "Epoch: 26176 , Loss: 0.00046660317457281053\n",
            "Epoch: 26177 , Loss: 0.000466366414912045\n",
            "Epoch: 26178 , Loss: 0.0004676204698625952\n",
            "Epoch: 26179 , Loss: 0.00046063197078183293\n",
            "Epoch: 26180 , Loss: 0.0004623568966053426\n",
            "Epoch: 26181 , Loss: 0.00046415242832154036\n",
            "Epoch: 26182 , Loss: 0.0004625575093086809\n",
            "Epoch: 26183 , Loss: 0.0004614044155459851\n",
            "Epoch: 26184 , Loss: 0.0004612159391399473\n",
            "Epoch: 26185 , Loss: 0.0004648489993996918\n",
            "Epoch: 26186 , Loss: 0.0004632195923477411\n",
            "Epoch: 26187 , Loss: 0.0004615979269146919\n",
            "Epoch: 26188 , Loss: 0.0004622568958438933\n",
            "Epoch: 26189 , Loss: 0.00046233905595727265\n",
            "Epoch: 26190 , Loss: 0.0004621670232154429\n",
            "Epoch: 26191 , Loss: 0.00046257596113719046\n",
            "Epoch: 26192 , Loss: 0.00046138049219734967\n",
            "Epoch: 26193 , Loss: 0.0004596870858222246\n",
            "Epoch: 26194 , Loss: 0.000461193616501987\n",
            "Epoch: 26195 , Loss: 0.0004618671373464167\n",
            "Epoch: 26196 , Loss: 0.0004610209434758872\n",
            "Epoch: 26197 , Loss: 0.00046414253301918507\n",
            "Epoch: 26198 , Loss: 0.000460764451418072\n",
            "Epoch: 26199 , Loss: 0.0004595344653353095\n",
            "Epoch: 26200 , Loss: 0.0004607010632753372\n",
            "Epoch: 26201 , Loss: 0.0004592224140651524\n",
            "Epoch: 26202 , Loss: 0.0004593940684571862\n",
            "Epoch: 26203 , Loss: 0.0004635212244465947\n",
            "Epoch: 26204 , Loss: 0.00045883344137109816\n",
            "Epoch: 26205 , Loss: 0.00046115092118270695\n",
            "Epoch: 26206 , Loss: 0.0004611584299709648\n",
            "Epoch: 26207 , Loss: 0.0004642257117666304\n",
            "Epoch: 26208 , Loss: 0.0004625758156180382\n",
            "Epoch: 26209 , Loss: 0.0004599128442350775\n",
            "Epoch: 26210 , Loss: 0.00046092827687971294\n",
            "Epoch: 26211 , Loss: 0.00046586222015321255\n",
            "Epoch: 26212 , Loss: 0.0004639793769456446\n",
            "Epoch: 26213 , Loss: 0.00046357803512364626\n",
            "Epoch: 26214 , Loss: 0.000457388669019565\n",
            "Epoch: 26215 , Loss: 0.000462534895632416\n",
            "Epoch: 26216 , Loss: 0.0004624488647095859\n",
            "Epoch: 26217 , Loss: 0.00046049815136939287\n",
            "Epoch: 26218 , Loss: 0.0004603168345056474\n",
            "Epoch: 26219 , Loss: 0.0004581367829814553\n",
            "Epoch: 26220 , Loss: 0.00046006543561816216\n",
            "Epoch: 26221 , Loss: 0.0004560331581160426\n",
            "Epoch: 26222 , Loss: 0.00046074314741417766\n",
            "Epoch: 26223 , Loss: 0.0004622439155355096\n",
            "Epoch: 26224 , Loss: 0.0004595620557665825\n",
            "Epoch: 26225 , Loss: 0.00045686817611567676\n",
            "Epoch: 26226 , Loss: 0.00045857334043830633\n",
            "Epoch: 26227 , Loss: 0.00046182709047570825\n",
            "Epoch: 26228 , Loss: 0.00045995856635272503\n",
            "Epoch: 26229 , Loss: 0.00046190310968086123\n",
            "Epoch: 26230 , Loss: 0.0004587578005157411\n",
            "Epoch: 26231 , Loss: 0.0004597660154104233\n",
            "Epoch: 26232 , Loss: 0.00045967986807227135\n",
            "Epoch: 26233 , Loss: 0.0004620780819095671\n",
            "Epoch: 26234 , Loss: 0.00046447012573480606\n",
            "Epoch: 26235 , Loss: 0.00045621831668540835\n",
            "Epoch: 26236 , Loss: 0.00046190497232601047\n",
            "Epoch: 26237 , Loss: 0.0004588575102388859\n",
            "Epoch: 26238 , Loss: 0.00046208425192162395\n",
            "Epoch: 26239 , Loss: 0.0004680661950260401\n",
            "Epoch: 26240 , Loss: 0.00045776067418046296\n",
            "Epoch: 26241 , Loss: 0.00045928280451335013\n",
            "Epoch: 26242 , Loss: 0.00045914630754850805\n",
            "Epoch: 26243 , Loss: 0.0004597451479639858\n",
            "Epoch: 26244 , Loss: 0.00046242401003837585\n",
            "Epoch: 26245 , Loss: 0.0004600132233463228\n",
            "Epoch: 26246 , Loss: 0.0004562725662253797\n",
            "Epoch: 26247 , Loss: 0.0004597639199346304\n",
            "Epoch: 26248 , Loss: 0.0004583325644489378\n",
            "Epoch: 26249 , Loss: 0.00045901123667135835\n",
            "Epoch: 26250 , Loss: 0.0004569424781948328\n",
            "=============================================\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "65 correctly classified among 100\n",
            "Accuracy as of 26250 epochs: 65.0\n",
            "=============================================\n",
            "Epoch: 26251 , Loss: 0.00045839283848181367\n",
            "Epoch: 26252 , Loss: 0.00045931688509881496\n",
            "Epoch: 26253 , Loss: 0.0004587473813444376\n",
            "Epoch: 26254 , Loss: 0.00046015926636755466\n",
            "Epoch: 26255 , Loss: 0.0004592124023474753\n",
            "Epoch: 26256 , Loss: 0.0004593301855493337\n",
            "Epoch: 26257 , Loss: 0.0004641858977265656\n",
            "Epoch: 26258 , Loss: 0.00045849414891563356\n",
            "Epoch: 26259 , Loss: 0.0004585467977449298\n",
            "Epoch: 26260 , Loss: 0.0004583931586239487\n",
            "Epoch: 26261 , Loss: 0.00045730394776910543\n",
            "Epoch: 26262 , Loss: 0.00045826021232642233\n",
            "Epoch: 26263 , Loss: 0.0004589321615640074\n",
            "Epoch: 26264 , Loss: 0.0004588571609929204\n",
            "Epoch: 26265 , Loss: 0.000457984977401793\n",
            "Epoch: 26266 , Loss: 0.00045801085070706904\n",
            "Epoch: 26267 , Loss: 0.0004594873171299696\n",
            "Epoch: 26268 , Loss: 0.00045697763562202454\n",
            "Epoch: 26269 , Loss: 0.0004579625965561718\n",
            "Epoch: 26270 , Loss: 0.00045764868264086545\n",
            "Epoch: 26271 , Loss: 0.000457664456916973\n",
            "Epoch: 26272 , Loss: 0.00045820209197700024\n",
            "Epoch: 26273 , Loss: 0.00045910023618489504\n",
            "Epoch: 26274 , Loss: 0.0004574643389787525\n",
            "Epoch: 26275 , Loss: 0.00045712554128840566\n",
            "Epoch: 26276 , Loss: 0.00046080665197223425\n",
            "Epoch: 26277 , Loss: 0.00045713846338912845\n",
            "Epoch: 26278 , Loss: 0.00045724070514552295\n",
            "Epoch: 26279 , Loss: 0.0004577601212076843\n",
            "Epoch: 26280 , Loss: 0.00045700615737587214\n",
            "Epoch: 26281 , Loss: 0.00045695461449213326\n",
            "Epoch: 26282 , Loss: 0.00045716011663898826\n",
            "Epoch: 26283 , Loss: 0.0004556882777251303\n",
            "Epoch: 26284 , Loss: 0.0004569109296426177\n",
            "Epoch: 26285 , Loss: 0.0004594272468239069\n",
            "Epoch: 26286 , Loss: 0.00045552849769592285\n",
            "Epoch: 26287 , Loss: 0.00045568443601951003\n",
            "Epoch: 26288 , Loss: 0.00046042187022976577\n",
            "Epoch: 26289 , Loss: 0.00045659474562853575\n",
            "Epoch: 26290 , Loss: 0.0004565338895190507\n",
            "Epoch: 26291 , Loss: 0.0004556804778985679\n",
            "Epoch: 26292 , Loss: 0.00045643237535841763\n",
            "Epoch: 26293 , Loss: 0.00045515631791204214\n",
            "Epoch: 26294 , Loss: 0.0004548215074464679\n",
            "Epoch: 26295 , Loss: 0.0004652261850424111\n",
            "Epoch: 26296 , Loss: 0.0004533728933893144\n",
            "Epoch: 26297 , Loss: 0.0004544859111774713\n",
            "Epoch: 26298 , Loss: 0.0004594004130922258\n",
            "Epoch: 26299 , Loss: 0.0004576954816002399\n",
            "Epoch: 26300 , Loss: 0.00045597259304486215\n",
            "Epoch: 26301 , Loss: 0.00045593679533340037\n",
            "Epoch: 26302 , Loss: 0.00045589599176310003\n",
            "Epoch: 26303 , Loss: 0.00045894162030890584\n",
            "Epoch: 26304 , Loss: 0.0004557209904305637\n",
            "Epoch: 26305 , Loss: 0.00045828899601474404\n",
            "Epoch: 26306 , Loss: 0.0004568649746943265\n",
            "Epoch: 26307 , Loss: 0.00045577529817819595\n",
            "Epoch: 26308 , Loss: 0.00045565905747935176\n",
            "Epoch: 26309 , Loss: 0.0004588534065987915\n",
            "Epoch: 26310 , Loss: 0.00045488792238757014\n",
            "Epoch: 26311 , Loss: 0.0004548793367575854\n",
            "Epoch: 26312 , Loss: 0.0004543089889921248\n",
            "Epoch: 26313 , Loss: 0.00045413599582388997\n",
            "Epoch: 26314 , Loss: 0.0004540384979918599\n",
            "Epoch: 26315 , Loss: 0.0004579339874908328\n",
            "Epoch: 26316 , Loss: 0.00045385383418761194\n",
            "Epoch: 26317 , Loss: 0.0004551020101644099\n",
            "Epoch: 26318 , Loss: 0.00045558091369457543\n",
            "Epoch: 26319 , Loss: 0.0004542645765468478\n",
            "Epoch: 26320 , Loss: 0.00045542779844254255\n",
            "Epoch: 26321 , Loss: 0.00045478082029148936\n",
            "Epoch: 26322 , Loss: 0.0004528208519332111\n",
            "Epoch: 26323 , Loss: 0.00045459993998520076\n",
            "Epoch: 26324 , Loss: 0.000453427666798234\n",
            "Epoch: 26325 , Loss: 0.00045794277684763074\n",
            "Epoch: 26326 , Loss: 0.0004530239384621382\n",
            "Epoch: 26327 , Loss: 0.00045782013330608606\n",
            "Epoch: 26328 , Loss: 0.000458764930954203\n",
            "Epoch: 26329 , Loss: 0.0004520891816355288\n",
            "Epoch: 26330 , Loss: 0.0004523330135270953\n",
            "Epoch: 26331 , Loss: 0.00045076225069351494\n",
            "Epoch: 26332 , Loss: 0.00045577212586067617\n",
            "Epoch: 26333 , Loss: 0.0004577001673169434\n",
            "Epoch: 26334 , Loss: 0.000449815415777266\n",
            "Epoch: 26335 , Loss: 0.0004519833019003272\n",
            "Epoch: 26336 , Loss: 0.000453782151453197\n",
            "Epoch: 26337 , Loss: 0.00045376006164588034\n",
            "Epoch: 26338 , Loss: 0.0004540722002275288\n",
            "Epoch: 26339 , Loss: 0.00044965979759581387\n",
            "Epoch: 26340 , Loss: 0.00045342944213189185\n",
            "Epoch: 26341 , Loss: 0.00045323881204240024\n",
            "Epoch: 26342 , Loss: 0.00045802086242474616\n",
            "Epoch: 26343 , Loss: 0.00045338194468058646\n",
            "Epoch: 26344 , Loss: 0.00045166461495682597\n",
            "Epoch: 26345 , Loss: 0.00045310467248782516\n",
            "Epoch: 26346 , Loss: 0.0004551566962618381\n",
            "Epoch: 26347 , Loss: 0.00045577401760965586\n",
            "Epoch: 26348 , Loss: 0.0004548360884655267\n",
            "Epoch: 26349 , Loss: 0.0004530440492089838\n",
            "Epoch: 26350 , Loss: 0.0004529935831669718\n",
            "Epoch: 26351 , Loss: 0.0004476284666452557\n",
            "Epoch: 26352 , Loss: 0.00045286936801858246\n",
            "Epoch: 26353 , Loss: 0.00045116731780581176\n",
            "Epoch: 26354 , Loss: 0.0004527836572378874\n",
            "Epoch: 26355 , Loss: 0.00045444921124726534\n",
            "Epoch: 26356 , Loss: 0.0004582556430250406\n",
            "Epoch: 26357 , Loss: 0.0004476491594687104\n",
            "Epoch: 26358 , Loss: 0.0004545448173303157\n",
            "Epoch: 26359 , Loss: 0.0004550596058834344\n",
            "Epoch: 26360 , Loss: 0.000456949055660516\n",
            "Epoch: 26361 , Loss: 0.0004555135383270681\n",
            "Epoch: 26362 , Loss: 0.00045243767090141773\n",
            "Epoch: 26363 , Loss: 0.000452405889518559\n",
            "Epoch: 26364 , Loss: 0.00045639905147254467\n",
            "Epoch: 26365 , Loss: 0.0004523673269432038\n",
            "Epoch: 26366 , Loss: 0.00045827002031728625\n",
            "Epoch: 26367 , Loss: 0.0004523164825513959\n",
            "Epoch: 26368 , Loss: 0.0004530186124611646\n",
            "Epoch: 26369 , Loss: 0.0004506861441768706\n",
            "Epoch: 26370 , Loss: 0.00045568763744086027\n",
            "Epoch: 26371 , Loss: 0.0004516482003964484\n",
            "Epoch: 26372 , Loss: 0.00045474403304979205\n",
            "Epoch: 26373 , Loss: 0.00044966430868953466\n",
            "Epoch: 26374 , Loss: 0.00045144048635847867\n",
            "Epoch: 26375 , Loss: 0.00045195803977549076\n",
            "Epoch: 26376 , Loss: 0.0004516422632150352\n",
            "Epoch: 26377 , Loss: 0.0004528960562311113\n",
            "Epoch: 26378 , Loss: 0.0004499089263845235\n",
            "Epoch: 26379 , Loss: 0.00045490506454370916\n",
            "Epoch: 26380 , Loss: 0.00045405031414702535\n",
            "Epoch: 26381 , Loss: 0.00045150984078645706\n",
            "Epoch: 26382 , Loss: 0.0004514373140409589\n",
            "Epoch: 26383 , Loss: 0.00045130890794098377\n",
            "Epoch: 26384 , Loss: 0.00045182486064732075\n",
            "Epoch: 26385 , Loss: 0.0004494193126447499\n",
            "Epoch: 26386 , Loss: 0.00045393931213766336\n",
            "Epoch: 26387 , Loss: 0.00045109461643733084\n",
            "Epoch: 26388 , Loss: 0.000451032305136323\n",
            "Epoch: 26389 , Loss: 0.0004511073639150709\n",
            "Epoch: 26390 , Loss: 0.0004547681601252407\n",
            "Epoch: 26391 , Loss: 0.00045085238525643945\n",
            "Epoch: 26392 , Loss: 0.0004489846178330481\n",
            "Epoch: 26393 , Loss: 0.0004507367848418653\n",
            "Epoch: 26394 , Loss: 0.00045352528104558587\n",
            "Epoch: 26395 , Loss: 0.00044992342009209096\n",
            "Epoch: 26396 , Loss: 0.00044875883031636477\n",
            "Epoch: 26397 , Loss: 0.0004501408548094332\n",
            "Epoch: 26398 , Loss: 0.00045266104280017316\n",
            "Epoch: 26399 , Loss: 0.0004486465477384627\n",
            "Epoch: 26400 , Loss: 0.0004503260715864599\n",
            "Epoch: 26401 , Loss: 0.00045055747614242136\n",
            "Epoch: 26402 , Loss: 0.00044829805847257376\n",
            "Epoch: 26403 , Loss: 0.0004496264737099409\n",
            "Epoch: 26404 , Loss: 0.000451015483122319\n",
            "Epoch: 26405 , Loss: 0.0004499794158618897\n",
            "Epoch: 26406 , Loss: 0.0004499283095356077\n",
            "Epoch: 26407 , Loss: 0.0004512399318628013\n",
            "Epoch: 26408 , Loss: 0.00044980680104345083\n",
            "Epoch: 26409 , Loss: 0.00044974772026762366\n",
            "Epoch: 26410 , Loss: 0.0004491833387874067\n",
            "Epoch: 26411 , Loss: 0.00044800108298659325\n",
            "Epoch: 26412 , Loss: 0.00045514598605223\n",
            "Epoch: 26413 , Loss: 0.0004518039058893919\n",
            "Epoch: 26414 , Loss: 0.0004483424127101898\n",
            "Epoch: 26415 , Loss: 0.00044801278272643685\n",
            "Epoch: 26416 , Loss: 0.0004555262275971472\n",
            "Epoch: 26417 , Loss: 0.0004495355242397636\n",
            "Epoch: 26418 , Loss: 0.0004480223578866571\n",
            "Epoch: 26419 , Loss: 0.00044915423495694995\n",
            "Epoch: 26420 , Loss: 0.0004500402428675443\n",
            "Epoch: 26421 , Loss: 0.00044903485104441643\n",
            "Epoch: 26422 , Loss: 0.0004469669656828046\n",
            "Epoch: 26423 , Loss: 0.0004522053641267121\n",
            "Epoch: 26424 , Loss: 0.0004497382906265557\n",
            "Epoch: 26425 , Loss: 0.0004478523333091289\n",
            "Epoch: 26426 , Loss: 0.0004541936796158552\n",
            "Epoch: 26427 , Loss: 0.0004470361745916307\n",
            "Epoch: 26428 , Loss: 0.000448674603831023\n",
            "Epoch: 26429 , Loss: 0.000444837030954659\n",
            "Epoch: 26430 , Loss: 0.00044701789738610387\n",
            "Epoch: 26431 , Loss: 0.00044883773080073297\n",
            "Epoch: 26432 , Loss: 0.0004450343258213252\n",
            "Epoch: 26433 , Loss: 0.00044949958100914955\n",
            "Epoch: 26434 , Loss: 0.0004496621841099113\n",
            "Epoch: 26435 , Loss: 0.0004487198602873832\n",
            "Epoch: 26436 , Loss: 0.00045144406612962484\n",
            "Epoch: 26437 , Loss: 0.00044824203359894454\n",
            "Epoch: 26438 , Loss: 0.0004541496164165437\n",
            "Epoch: 26439 , Loss: 0.0004472167347557843\n",
            "Epoch: 26440 , Loss: 0.0004465981328394264\n",
            "Epoch: 26441 , Loss: 0.0004475690657272935\n",
            "Epoch: 26442 , Loss: 0.0004478374612517655\n",
            "Epoch: 26443 , Loss: 0.0004530652950052172\n",
            "Epoch: 26444 , Loss: 0.0004483277443796396\n",
            "Epoch: 26445 , Loss: 0.0004467855324037373\n",
            "Epoch: 26446 , Loss: 0.0004477770416997373\n",
            "Epoch: 26447 , Loss: 0.0004475569003261626\n",
            "Epoch: 26448 , Loss: 0.00044699967838823795\n",
            "Epoch: 26449 , Loss: 0.00044742049067281187\n",
            "Epoch: 26450 , Loss: 0.00044892344158142805\n",
            "Epoch: 26451 , Loss: 0.0004522727685980499\n",
            "Epoch: 26452 , Loss: 0.0004472623986657709\n",
            "Epoch: 26453 , Loss: 0.00044721851008944213\n",
            "Epoch: 26454 , Loss: 0.00044685526518151164\n",
            "Epoch: 26455 , Loss: 0.0004482851945795119\n",
            "Epoch: 26456 , Loss: 0.00045001760008744895\n",
            "Epoch: 26457 , Loss: 0.00044619094114750624\n",
            "Epoch: 26458 , Loss: 0.000445201265392825\n",
            "Epoch: 26459 , Loss: 0.00044515435001812875\n",
            "Epoch: 26460 , Loss: 0.0004449106636457145\n",
            "Epoch: 26461 , Loss: 0.0004432576824910939\n",
            "Epoch: 26462 , Loss: 0.000447113998234272\n",
            "Epoch: 26463 , Loss: 0.00044668972259387374\n",
            "Epoch: 26464 , Loss: 0.0004478427581489086\n",
            "Epoch: 26465 , Loss: 0.0004486604011617601\n",
            "Epoch: 26466 , Loss: 0.0004481528594624251\n",
            "Epoch: 26467 , Loss: 0.0004490473074838519\n",
            "Epoch: 26468 , Loss: 0.0004445939848665148\n",
            "Epoch: 26469 , Loss: 0.0004464772646315396\n",
            "Epoch: 26470 , Loss: 0.00044496601913124323\n",
            "Epoch: 26471 , Loss: 0.00044451444409787655\n",
            "Epoch: 26472 , Loss: 0.00044826747034676373\n",
            "Epoch: 26473 , Loss: 0.0004464735393412411\n",
            "Epoch: 26474 , Loss: 0.0004531991435214877\n",
            "Epoch: 26475 , Loss: 0.0004459840420167893\n",
            "Epoch: 26476 , Loss: 0.00044519142829813063\n",
            "Epoch: 26477 , Loss: 0.0004467501421459019\n",
            "Epoch: 26478 , Loss: 0.00044582400005310774\n",
            "Epoch: 26479 , Loss: 0.0004464582016225904\n",
            "Epoch: 26480 , Loss: 0.0004452394205145538\n",
            "Epoch: 26481 , Loss: 0.00044450571294873953\n",
            "Epoch: 26482 , Loss: 0.00044544198317453265\n",
            "Epoch: 26483 , Loss: 0.00044694592361338437\n",
            "Epoch: 26484 , Loss: 0.000444056378910318\n",
            "Epoch: 26485 , Loss: 0.00044566363794729114\n",
            "Epoch: 26486 , Loss: 0.00044502876698970795\n",
            "Epoch: 26487 , Loss: 0.0004479174967855215\n",
            "Epoch: 26488 , Loss: 0.00044580502435564995\n",
            "Epoch: 26489 , Loss: 0.00044271527440287173\n",
            "Epoch: 26490 , Loss: 0.0004465430974960327\n",
            "Epoch: 26491 , Loss: 0.0004448185209184885\n",
            "Epoch: 26492 , Loss: 0.0004457011236809194\n",
            "Epoch: 26493 , Loss: 0.0004470258136279881\n",
            "Epoch: 26494 , Loss: 0.000443415017798543\n",
            "Epoch: 26495 , Loss: 0.00044457108015194535\n",
            "Epoch: 26496 , Loss: 0.00044427529792301357\n",
            "Epoch: 26497 , Loss: 0.000447529717348516\n",
            "Epoch: 26498 , Loss: 0.00044448330299928784\n",
            "Epoch: 26499 , Loss: 0.00044444610830396414\n",
            "Epoch: 26500 , Loss: 0.0004443772486411035\n",
            "=============================================\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "41 correctly classified among 100\n",
            "Accuracy as of 26500 epochs: 41.0\n",
            "=============================================\n",
            "Epoch: 26501 , Loss: 0.00044240045826882124\n",
            "Epoch: 26502 , Loss: 0.0004424536891747266\n",
            "Epoch: 26503 , Loss: 0.0004421145422384143\n",
            "Epoch: 26504 , Loss: 0.00045223330380395055\n",
            "Epoch: 26505 , Loss: 0.00044448996777646244\n",
            "Epoch: 26506 , Loss: 0.0004420547338668257\n",
            "Epoch: 26507 , Loss: 0.00044218794209882617\n",
            "Epoch: 26508 , Loss: 0.00044365707435645163\n",
            "Epoch: 26509 , Loss: 0.00044372022966854274\n",
            "Epoch: 26510 , Loss: 0.0004442449426278472\n",
            "Epoch: 26511 , Loss: 0.0004422615747898817\n",
            "Epoch: 26512 , Loss: 0.00044159666867926717\n",
            "Epoch: 26513 , Loss: 0.0004406209336593747\n",
            "Epoch: 26514 , Loss: 0.00044703250750899315\n",
            "Epoch: 26515 , Loss: 0.0004445323720574379\n",
            "Epoch: 26516 , Loss: 0.0004487390397116542\n",
            "Epoch: 26517 , Loss: 0.0004457735631149262\n",
            "Epoch: 26518 , Loss: 0.0004466468235477805\n",
            "Epoch: 26519 , Loss: 0.0004465789534151554\n",
            "Epoch: 26520 , Loss: 0.00044155711657367647\n",
            "Epoch: 26521 , Loss: 0.00044137457734905183\n",
            "Epoch: 26522 , Loss: 0.00044639193220064044\n",
            "Epoch: 26523 , Loss: 0.00044849119149148464\n",
            "Epoch: 26524 , Loss: 0.0004426711820997298\n",
            "Epoch: 26525 , Loss: 0.0004518414498306811\n",
            "Epoch: 26526 , Loss: 0.00044121011160314083\n",
            "Epoch: 26527 , Loss: 0.00044105848064646125\n",
            "Epoch: 26528 , Loss: 0.0004453076981008053\n",
            "Epoch: 26529 , Loss: 0.0004444700316525996\n",
            "Epoch: 26530 , Loss: 0.00044400792103260756\n",
            "Epoch: 26531 , Loss: 0.0004438122850842774\n",
            "Epoch: 26532 , Loss: 0.00044280022848397493\n",
            "Epoch: 26533 , Loss: 0.0004428313113749027\n",
            "Epoch: 26534 , Loss: 0.0004431812558323145\n",
            "Epoch: 26535 , Loss: 0.0004427524108905345\n",
            "Epoch: 26536 , Loss: 0.0004427059320732951\n",
            "Epoch: 26537 , Loss: 0.00044259513379074633\n",
            "Epoch: 26538 , Loss: 0.0004409498942550272\n",
            "Epoch: 26539 , Loss: 0.00044162312406115234\n",
            "Epoch: 26540 , Loss: 0.00044191349297761917\n",
            "Epoch: 26541 , Loss: 0.0004407567612361163\n",
            "Epoch: 26542 , Loss: 0.0004419822944328189\n",
            "Epoch: 26543 , Loss: 0.0004447990795597434\n",
            "Epoch: 26544 , Loss: 0.0004403287312015891\n",
            "Epoch: 26545 , Loss: 0.0004420916666276753\n",
            "Epoch: 26546 , Loss: 0.0004447461396921426\n",
            "Epoch: 26547 , Loss: 0.0004419315082486719\n",
            "Epoch: 26548 , Loss: 0.0004421947232913226\n",
            "Epoch: 26549 , Loss: 0.00044167053420096636\n",
            "Epoch: 26550 , Loss: 0.0004400097532197833\n",
            "Epoch: 26551 , Loss: 0.0004409549874253571\n",
            "Epoch: 26552 , Loss: 0.00044289292418397963\n",
            "Epoch: 26553 , Loss: 0.0004396419390104711\n",
            "Epoch: 26554 , Loss: 0.0004379604069981724\n",
            "Epoch: 26555 , Loss: 0.00043858500430360436\n",
            "Epoch: 26556 , Loss: 0.00043969470425508916\n",
            "Epoch: 26557 , Loss: 0.0004441473283804953\n",
            "Epoch: 26558 , Loss: 0.00044147242442704737\n",
            "Epoch: 26559 , Loss: 0.00044600729597732425\n",
            "Epoch: 26560 , Loss: 0.0004409216926433146\n",
            "Epoch: 26561 , Loss: 0.00043742978596128523\n",
            "Epoch: 26562 , Loss: 0.0004399918543640524\n",
            "Epoch: 26563 , Loss: 0.0004392490372993052\n",
            "Epoch: 26564 , Loss: 0.0004455721937119961\n",
            "Epoch: 26565 , Loss: 0.00044086622074246407\n",
            "Epoch: 26566 , Loss: 0.00044048766721971333\n",
            "Epoch: 26567 , Loss: 0.00044069698196835816\n",
            "Epoch: 26568 , Loss: 0.00044277729466557503\n",
            "Epoch: 26569 , Loss: 0.00044209498446434736\n",
            "Epoch: 26570 , Loss: 0.00043831393122673035\n",
            "Epoch: 26571 , Loss: 0.0004406032676342875\n",
            "Epoch: 26572 , Loss: 0.00043700667447410524\n",
            "Epoch: 26573 , Loss: 0.00044025538954883814\n",
            "Epoch: 26574 , Loss: 0.0004403361235745251\n",
            "Epoch: 26575 , Loss: 0.00044015294406563044\n",
            "Epoch: 26576 , Loss: 0.000440126663306728\n",
            "Epoch: 26577 , Loss: 0.00044295372208580375\n",
            "Epoch: 26578 , Loss: 0.00043909132364206016\n",
            "Epoch: 26579 , Loss: 0.00043950980762019753\n",
            "Epoch: 26580 , Loss: 0.0004419256583787501\n",
            "Epoch: 26581 , Loss: 0.0004396015719976276\n",
            "Epoch: 26582 , Loss: 0.0004397728480398655\n",
            "Epoch: 26583 , Loss: 0.0004399807658046484\n",
            "Epoch: 26584 , Loss: 0.00043965253280475736\n",
            "Epoch: 26585 , Loss: 0.0004409528337419033\n",
            "Epoch: 26586 , Loss: 0.00044097716454416513\n",
            "Epoch: 26587 , Loss: 0.0004395432770252228\n",
            "Epoch: 26588 , Loss: 0.000439190655015409\n",
            "Epoch: 26589 , Loss: 0.000439606694271788\n",
            "Epoch: 26590 , Loss: 0.0004358415608294308\n",
            "Epoch: 26591 , Loss: 0.0004417555464897305\n",
            "Epoch: 26592 , Loss: 0.00044090402661822736\n",
            "Epoch: 26593 , Loss: 0.00044359074672684073\n",
            "Epoch: 26594 , Loss: 0.000436825561337173\n",
            "Epoch: 26595 , Loss: 0.00043896352872252464\n",
            "Epoch: 26596 , Loss: 0.000438886956544593\n",
            "Epoch: 26597 , Loss: 0.0004388483357615769\n",
            "Epoch: 26598 , Loss: 0.000437939161201939\n",
            "Epoch: 26599 , Loss: 0.0004368415102362633\n",
            "Epoch: 26600 , Loss: 0.00043785260641016066\n",
            "Epoch: 26601 , Loss: 0.000438589631812647\n",
            "Epoch: 26602 , Loss: 0.0004387286026030779\n",
            "Epoch: 26603 , Loss: 0.0004397379816509783\n",
            "Epoch: 26604 , Loss: 0.00043860412552021444\n",
            "Epoch: 26605 , Loss: 0.00043834876851178706\n",
            "Epoch: 26606 , Loss: 0.00043654043111018836\n",
            "Epoch: 26607 , Loss: 0.0004356161516625434\n",
            "Epoch: 26608 , Loss: 0.0004381528706289828\n",
            "Epoch: 26609 , Loss: 0.0004407164524309337\n",
            "Epoch: 26610 , Loss: 0.0004380501341074705\n",
            "Epoch: 26611 , Loss: 0.0004397078009787947\n",
            "Epoch: 26612 , Loss: 0.0004378761223051697\n",
            "Epoch: 26613 , Loss: 0.000438014481915161\n",
            "Epoch: 26614 , Loss: 0.0004377514705993235\n",
            "Epoch: 26615 , Loss: 0.0004427216772455722\n",
            "Epoch: 26616 , Loss: 0.00044001828064210713\n",
            "Epoch: 26617 , Loss: 0.000437569513451308\n",
            "Epoch: 26618 , Loss: 0.00043750982149504125\n",
            "Epoch: 26619 , Loss: 0.0004353684780653566\n",
            "Epoch: 26620 , Loss: 0.00043580244528129697\n",
            "Epoch: 26621 , Loss: 0.00043541775085031986\n",
            "Epoch: 26622 , Loss: 0.00043779495172202587\n",
            "Epoch: 26623 , Loss: 0.00043720458052121103\n",
            "Epoch: 26624 , Loss: 0.00044136273209005594\n",
            "Epoch: 26625 , Loss: 0.00043716159416362643\n",
            "Epoch: 26626 , Loss: 0.0004457428876776248\n",
            "Epoch: 26627 , Loss: 0.0004450055421330035\n",
            "Epoch: 26628 , Loss: 0.0004348930378910154\n",
            "Epoch: 26629 , Loss: 0.00043789634946733713\n",
            "Epoch: 26630 , Loss: 0.0004344288317952305\n",
            "Epoch: 26631 , Loss: 0.000435045687481761\n",
            "Epoch: 26632 , Loss: 0.0004368891241028905\n",
            "Epoch: 26633 , Loss: 0.0004382086917757988\n",
            "Epoch: 26634 , Loss: 0.0004379503079690039\n",
            "Epoch: 26635 , Loss: 0.0004376514407340437\n",
            "Epoch: 26636 , Loss: 0.0004369080706965178\n",
            "Epoch: 26637 , Loss: 0.00043988856486976147\n",
            "Epoch: 26638 , Loss: 0.00043915133574046195\n",
            "Epoch: 26639 , Loss: 0.0004365066415630281\n",
            "Epoch: 26640 , Loss: 0.0004364502674434334\n",
            "Epoch: 26641 , Loss: 0.0004370109527371824\n",
            "Epoch: 26642 , Loss: 0.00043908925727009773\n",
            "Epoch: 26643 , Loss: 0.0004373583651613444\n",
            "Epoch: 26644 , Loss: 0.00043664471013471484\n",
            "Epoch: 26645 , Loss: 0.0004374865675345063\n",
            "Epoch: 26646 , Loss: 0.00043309322791174054\n",
            "Epoch: 26647 , Loss: 0.0004362410109024495\n",
            "Epoch: 26648 , Loss: 0.000433627690654248\n",
            "Epoch: 26649 , Loss: 0.0004365126369521022\n",
            "Epoch: 26650 , Loss: 0.0004349935334175825\n",
            "Epoch: 26651 , Loss: 0.00044149949098937213\n",
            "Epoch: 26652 , Loss: 0.00043433927930891514\n",
            "Epoch: 26653 , Loss: 0.0004379543534014374\n",
            "Epoch: 26654 , Loss: 0.00043495578574948013\n",
            "Epoch: 26655 , Loss: 0.0004356768913567066\n",
            "Epoch: 26656 , Loss: 0.0004355290438979864\n",
            "Epoch: 26657 , Loss: 0.00043557019671425223\n",
            "Epoch: 26658 , Loss: 0.0004354049451649189\n",
            "Epoch: 26659 , Loss: 0.00043482540058903396\n",
            "Epoch: 26660 , Loss: 0.0004336980637162924\n",
            "Epoch: 26661 , Loss: 0.0004362408071756363\n",
            "Epoch: 26662 , Loss: 0.000434187357313931\n",
            "Epoch: 26663 , Loss: 0.00043506035581231117\n",
            "Epoch: 26664 , Loss: 0.0004349882365204394\n",
            "Epoch: 26665 , Loss: 0.0004349159135017544\n",
            "Epoch: 26666 , Loss: 0.0004348020302131772\n",
            "Epoch: 26667 , Loss: 0.0004326550115365535\n",
            "Epoch: 26668 , Loss: 0.00043590765562839806\n",
            "Epoch: 26669 , Loss: 0.0004344639601185918\n",
            "Epoch: 26670 , Loss: 0.00043736473890021443\n",
            "Epoch: 26671 , Loss: 0.0004344704211689532\n",
            "Epoch: 26672 , Loss: 0.000432950648246333\n",
            "Epoch: 26673 , Loss: 0.0004371461400296539\n",
            "Epoch: 26674 , Loss: 0.0004364105989225209\n",
            "Epoch: 26675 , Loss: 0.0004368798399809748\n",
            "Epoch: 26676 , Loss: 0.00043245949200354517\n",
            "Epoch: 26677 , Loss: 0.00043415979598648846\n",
            "Epoch: 26678 , Loss: 0.00043148582335561514\n",
            "Epoch: 26679 , Loss: 0.0004346371570136398\n",
            "Epoch: 26680 , Loss: 0.0004334959667176008\n",
            "Epoch: 26681 , Loss: 0.0004338514991104603\n",
            "Epoch: 26682 , Loss: 0.00043706432916224003\n",
            "Epoch: 26683 , Loss: 0.00043838017154484987\n",
            "Epoch: 26684 , Loss: 0.00043370446655899286\n",
            "Epoch: 26685 , Loss: 0.00043109917896799743\n",
            "Epoch: 26686 , Loss: 0.0004337838036008179\n",
            "Epoch: 26687 , Loss: 0.00043357606045901775\n",
            "Epoch: 26688 , Loss: 0.0004355052369646728\n",
            "Epoch: 26689 , Loss: 0.0004309150972403586\n",
            "Epoch: 26690 , Loss: 0.00043325001024641097\n",
            "Epoch: 26691 , Loss: 0.0004375425342004746\n",
            "Epoch: 26692 , Loss: 0.00043326476588845253\n",
            "Epoch: 26693 , Loss: 0.0004349471419118345\n",
            "Epoch: 26694 , Loss: 0.00043241208186373115\n",
            "Epoch: 26695 , Loss: 0.0004327580099925399\n",
            "Epoch: 26696 , Loss: 0.00043303309939801693\n",
            "Epoch: 26697 , Loss: 0.0004296337137930095\n",
            "Epoch: 26698 , Loss: 0.0004317747079767287\n",
            "Epoch: 26699 , Loss: 0.0004319657164160162\n",
            "Epoch: 26700 , Loss: 0.00043464984628371894\n",
            "Epoch: 26701 , Loss: 0.00043089943937957287\n",
            "Epoch: 26702 , Loss: 0.00043213285971432924\n",
            "Epoch: 26703 , Loss: 0.00043251365423202515\n",
            "Epoch: 26704 , Loss: 0.0004330732626840472\n",
            "Epoch: 26705 , Loss: 0.0004322310851421207\n",
            "Epoch: 26706 , Loss: 0.0004313951649237424\n",
            "Epoch: 26707 , Loss: 0.0004323665052652359\n",
            "Epoch: 26708 , Loss: 0.00043214450124651194\n",
            "Epoch: 26709 , Loss: 0.00043488058145157993\n",
            "Epoch: 26710 , Loss: 0.0004303009482100606\n",
            "Epoch: 26711 , Loss: 0.0004347943468019366\n",
            "Epoch: 26712 , Loss: 0.00042997588752768934\n",
            "Epoch: 26713 , Loss: 0.0004289999487809837\n",
            "Epoch: 26714 , Loss: 0.00043404710595496\n",
            "Epoch: 26715 , Loss: 0.0004314610850997269\n",
            "Epoch: 26716 , Loss: 0.0004397983429953456\n",
            "Epoch: 26717 , Loss: 0.00043161027133464813\n",
            "Epoch: 26718 , Loss: 0.00043604918755590916\n",
            "Epoch: 26719 , Loss: 0.0004317784623708576\n",
            "Epoch: 26720 , Loss: 0.0004315032856538892\n",
            "Epoch: 26721 , Loss: 0.0004314766847528517\n",
            "Epoch: 26722 , Loss: 0.000431287131505087\n",
            "Epoch: 26723 , Loss: 0.0004331243981141597\n",
            "Epoch: 26724 , Loss: 0.00042899828986264765\n",
            "Epoch: 26725 , Loss: 0.00043133075814694166\n",
            "Epoch: 26726 , Loss: 0.0004313036915846169\n",
            "Epoch: 26727 , Loss: 0.0004380890168249607\n",
            "Epoch: 26728 , Loss: 0.000431167078204453\n",
            "Epoch: 26729 , Loss: 0.0004318535211496055\n",
            "Epoch: 26730 , Loss: 0.0004319563158787787\n",
            "Epoch: 26731 , Loss: 0.0004306334594730288\n",
            "Epoch: 26732 , Loss: 0.00042958991252817214\n",
            "Epoch: 26733 , Loss: 0.00043087537051178515\n",
            "Epoch: 26734 , Loss: 0.00043086608638986945\n",
            "Epoch: 26735 , Loss: 0.00043374678352847695\n",
            "Epoch: 26736 , Loss: 0.00042993982788175344\n",
            "Epoch: 26737 , Loss: 0.00043067068327218294\n",
            "Epoch: 26738 , Loss: 0.0004314240941312164\n",
            "Epoch: 26739 , Loss: 0.00043146044481545687\n",
            "Epoch: 26740 , Loss: 0.00043236571946181357\n",
            "Epoch: 26741 , Loss: 0.0004295464314054698\n",
            "Epoch: 26742 , Loss: 0.00043038747389800847\n",
            "Epoch: 26743 , Loss: 0.00043197532068006694\n",
            "Epoch: 26744 , Loss: 0.000426948769018054\n",
            "Epoch: 26745 , Loss: 0.00043021811870858073\n",
            "Epoch: 26746 , Loss: 0.00042843871051445603\n",
            "Epoch: 26747 , Loss: 0.00043340231059119105\n",
            "Epoch: 26748 , Loss: 0.0004294304526410997\n",
            "Epoch: 26749 , Loss: 0.0004276674007996917\n",
            "Epoch: 26750 , Loss: 0.00042985338950529695\n",
            "=============================================\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "56 correctly classified among 100\n",
            "Accuracy as of 26750 epochs: 56.0\n",
            "=============================================\n",
            "Epoch: 26751 , Loss: 0.0004320687730796635\n",
            "Epoch: 26752 , Loss: 0.00043206155532971025\n",
            "Epoch: 26753 , Loss: 0.000429811654612422\n",
            "Epoch: 26754 , Loss: 0.00043199650826863945\n",
            "Epoch: 26755 , Loss: 0.0004312052624300122\n",
            "Epoch: 26756 , Loss: 0.00042961337021552026\n",
            "Epoch: 26757 , Loss: 0.00042855337960645556\n",
            "Epoch: 26758 , Loss: 0.00042647734517231584\n",
            "Epoch: 26759 , Loss: 0.00043066401849500835\n",
            "Epoch: 26760 , Loss: 0.0004267088952474296\n",
            "Epoch: 26761 , Loss: 0.00042819895315915346\n",
            "Epoch: 26762 , Loss: 0.0004307079943828285\n",
            "Epoch: 26763 , Loss: 0.00042783451499417424\n",
            "Epoch: 26764 , Loss: 0.00043084248318336904\n",
            "Epoch: 26765 , Loss: 0.00042717382893897593\n",
            "Epoch: 26766 , Loss: 0.0004287278279662132\n",
            "Epoch: 26767 , Loss: 0.0004287444753572345\n",
            "Epoch: 26768 , Loss: 0.000428623374318704\n",
            "Epoch: 26769 , Loss: 0.00042707769898697734\n",
            "Epoch: 26770 , Loss: 0.0004258168628439307\n",
            "Epoch: 26771 , Loss: 0.0004289963690098375\n",
            "Epoch: 26772 , Loss: 0.0004309229552745819\n",
            "Epoch: 26773 , Loss: 0.0004282444715499878\n",
            "Epoch: 26774 , Loss: 0.0004282267764210701\n",
            "Epoch: 26775 , Loss: 0.00042811522143892944\n",
            "Epoch: 26776 , Loss: 0.0004260527202859521\n",
            "Epoch: 26777 , Loss: 0.0004289619391784072\n",
            "Epoch: 26778 , Loss: 0.00042580184526741505\n",
            "Epoch: 26779 , Loss: 0.0004259063280187547\n",
            "Epoch: 26780 , Loss: 0.0004289700009394437\n",
            "Epoch: 26781 , Loss: 0.00042514322558417916\n",
            "Epoch: 26782 , Loss: 0.0004260005953256041\n",
            "Epoch: 26783 , Loss: 0.00042896746890619397\n",
            "Epoch: 26784 , Loss: 0.0004327947390265763\n",
            "Epoch: 26785 , Loss: 0.00042568708886392415\n",
            "Epoch: 26786 , Loss: 0.00042566368938423693\n",
            "Epoch: 26787 , Loss: 0.0004281169385649264\n",
            "Epoch: 26788 , Loss: 0.00042614576523192227\n",
            "Epoch: 26789 , Loss: 0.0004273082595318556\n",
            "Epoch: 26790 , Loss: 0.0004257597029209137\n",
            "Epoch: 26791 , Loss: 0.0004264148883521557\n",
            "Epoch: 26792 , Loss: 0.00042462628334760666\n",
            "Epoch: 26793 , Loss: 0.00042234547436237335\n",
            "Epoch: 26794 , Loss: 0.0004296145052649081\n",
            "Epoch: 26795 , Loss: 0.00042601392487995327\n",
            "Epoch: 26796 , Loss: 0.0004255297244526446\n",
            "Epoch: 26797 , Loss: 0.000423058052547276\n",
            "Epoch: 26798 , Loss: 0.0004264942544978112\n",
            "Epoch: 26799 , Loss: 0.0004252410726621747\n",
            "Epoch: 26800 , Loss: 0.0004319066065363586\n",
            "Epoch: 26801 , Loss: 0.00042578985448926687\n",
            "Epoch: 26802 , Loss: 0.0004260471323505044\n",
            "Epoch: 26803 , Loss: 0.0004264336603228003\n",
            "Epoch: 26804 , Loss: 0.00042452782508917153\n",
            "Epoch: 26805 , Loss: 0.00042996954289264977\n",
            "Epoch: 26806 , Loss: 0.0004259398265276104\n",
            "Epoch: 26807 , Loss: 0.0004302299930714071\n",
            "Epoch: 26808 , Loss: 0.0004246387106832117\n",
            "Epoch: 26809 , Loss: 0.00042550789657980204\n",
            "Epoch: 26810 , Loss: 0.00042959745042026043\n",
            "Epoch: 26811 , Loss: 0.00042840465903282166\n",
            "Epoch: 26812 , Loss: 0.00042541802395135164\n",
            "Epoch: 26813 , Loss: 0.0004241609713062644\n",
            "Epoch: 26814 , Loss: 0.0004243937146384269\n",
            "Epoch: 26815 , Loss: 0.0004258354310877621\n",
            "Epoch: 26816 , Loss: 0.00043033339898101985\n",
            "Epoch: 26817 , Loss: 0.0004256262327544391\n",
            "Epoch: 26818 , Loss: 0.0004255588282831013\n",
            "Epoch: 26819 , Loss: 0.00042602221947163343\n",
            "Epoch: 26820 , Loss: 0.00042546604527160525\n",
            "Epoch: 26821 , Loss: 0.0004282340232748538\n",
            "Epoch: 26822 , Loss: 0.00042545300675556064\n",
            "Epoch: 26823 , Loss: 0.0004266690812073648\n",
            "Epoch: 26824 , Loss: 0.00042706436943262815\n",
            "Epoch: 26825 , Loss: 0.00042520236456766725\n",
            "Epoch: 26826 , Loss: 0.0004234854714013636\n",
            "Epoch: 26827 , Loss: 0.00042649160604923964\n",
            "Epoch: 26828 , Loss: 0.00042325735557824373\n",
            "Epoch: 26829 , Loss: 0.0004248671466484666\n",
            "Epoch: 26830 , Loss: 0.0004246914468239993\n",
            "Epoch: 26831 , Loss: 0.0004221521958243102\n",
            "Epoch: 26832 , Loss: 0.0004218344984110445\n",
            "Epoch: 26833 , Loss: 0.0004251464852131903\n",
            "Epoch: 26834 , Loss: 0.0004257883410900831\n",
            "Epoch: 26835 , Loss: 0.0004246663593221456\n",
            "Epoch: 26836 , Loss: 0.00042460192344151437\n",
            "Epoch: 26837 , Loss: 0.00041974350460805\n",
            "Epoch: 26838 , Loss: 0.0004242545983288437\n",
            "Epoch: 26839 , Loss: 0.000426790036726743\n",
            "Epoch: 26840 , Loss: 0.0004237358516547829\n",
            "Epoch: 26841 , Loss: 0.0004238604160491377\n",
            "Epoch: 26842 , Loss: 0.0004254740779288113\n",
            "Epoch: 26843 , Loss: 0.0004226198943797499\n",
            "Epoch: 26844 , Loss: 0.00042342007509432733\n",
            "Epoch: 26845 , Loss: 0.00042305298848077655\n",
            "Epoch: 26846 , Loss: 0.00042064845911227167\n",
            "Epoch: 26847 , Loss: 0.0004237077373545617\n",
            "Epoch: 26848 , Loss: 0.00042390762246213853\n",
            "Epoch: 26849 , Loss: 0.00042389106238260865\n",
            "Epoch: 26850 , Loss: 0.0004235891974531114\n",
            "Epoch: 26851 , Loss: 0.0004245049203746021\n",
            "Epoch: 26852 , Loss: 0.00042605664930306375\n",
            "Epoch: 26853 , Loss: 0.0004324673500377685\n",
            "Epoch: 26854 , Loss: 0.00042390244198031723\n",
            "Epoch: 26855 , Loss: 0.00042319652857258916\n",
            "Epoch: 26856 , Loss: 0.00042780800140462816\n",
            "Epoch: 26857 , Loss: 0.00042357604252174497\n",
            "Epoch: 26858 , Loss: 0.0004245555610395968\n",
            "Epoch: 26859 , Loss: 0.0004231537168379873\n",
            "Epoch: 26860 , Loss: 0.00042176281567662954\n",
            "Epoch: 26861 , Loss: 0.00042060608393512666\n",
            "Epoch: 26862 , Loss: 0.0004231252823956311\n",
            "Epoch: 26863 , Loss: 0.00042310002027079463\n",
            "Epoch: 26864 , Loss: 0.00042382354149594903\n",
            "Epoch: 26865 , Loss: 0.0004216974484734237\n",
            "Epoch: 26866 , Loss: 0.0004229757469147444\n",
            "Epoch: 26867 , Loss: 0.00042184346239082515\n",
            "Epoch: 26868 , Loss: 0.0004229799087624997\n",
            "Epoch: 26869 , Loss: 0.00042464648140594363\n",
            "Epoch: 26870 , Loss: 0.0004244600422680378\n",
            "Epoch: 26871 , Loss: 0.0004227875906508416\n",
            "Epoch: 26872 , Loss: 0.0004210364422760904\n",
            "Epoch: 26873 , Loss: 0.0004226146265864372\n",
            "Epoch: 26874 , Loss: 0.0004225494631100446\n",
            "Epoch: 26875 , Loss: 0.00042218767339363694\n",
            "Epoch: 26876 , Loss: 0.0004243840230628848\n",
            "Epoch: 26877 , Loss: 0.0004217919777147472\n",
            "Epoch: 26878 , Loss: 0.0004224455333314836\n",
            "Epoch: 26879 , Loss: 0.0004222366842441261\n",
            "Epoch: 26880 , Loss: 0.0004249568737577647\n",
            "Epoch: 26881 , Loss: 0.00042197448783554137\n",
            "Epoch: 26882 , Loss: 0.00042256692540831864\n",
            "Epoch: 26883 , Loss: 0.000421923614339903\n",
            "Epoch: 26884 , Loss: 0.0004237396933604032\n",
            "Epoch: 26885 , Loss: 0.00042273831786587834\n",
            "Epoch: 26886 , Loss: 0.00042415986536070704\n",
            "Epoch: 26887 , Loss: 0.00042165344348177314\n",
            "Epoch: 26888 , Loss: 0.0004236880922690034\n",
            "Epoch: 26889 , Loss: 0.0004215223016217351\n",
            "Epoch: 26890 , Loss: 0.00042134031536988914\n",
            "Epoch: 26891 , Loss: 0.00042075294186361134\n",
            "Epoch: 26892 , Loss: 0.0004219470720272511\n",
            "Epoch: 26893 , Loss: 0.00042138586286455393\n",
            "Epoch: 26894 , Loss: 0.0004211593768559396\n",
            "Epoch: 26895 , Loss: 0.0004211595223750919\n",
            "Epoch: 26896 , Loss: 0.00041991774924099445\n",
            "Epoch: 26897 , Loss: 0.00042092552757821977\n",
            "Epoch: 26898 , Loss: 0.00041935447370633483\n",
            "Epoch: 26899 , Loss: 0.00042321361252106726\n",
            "Epoch: 26900 , Loss: 0.00042042089626193047\n",
            "Epoch: 26901 , Loss: 0.00042067712638527155\n",
            "Epoch: 26902 , Loss: 0.00041950971353799105\n",
            "Epoch: 26903 , Loss: 0.0004205474106129259\n",
            "Epoch: 26904 , Loss: 0.0004197194648440927\n",
            "Epoch: 26905 , Loss: 0.0004207062302157283\n",
            "Epoch: 26906 , Loss: 0.00042053224751725793\n",
            "Epoch: 26907 , Loss: 0.0004193343629594892\n",
            "Epoch: 26908 , Loss: 0.0004190318868495524\n",
            "Epoch: 26909 , Loss: 0.0004195787478238344\n",
            "Epoch: 26910 , Loss: 0.0004250588535796851\n",
            "Epoch: 26911 , Loss: 0.00041998393135145307\n",
            "Epoch: 26912 , Loss: 0.00041616777889430523\n",
            "Epoch: 26913 , Loss: 0.00041826284723356366\n",
            "Epoch: 26914 , Loss: 0.00041773662087507546\n",
            "Epoch: 26915 , Loss: 0.0004200264229439199\n",
            "Epoch: 26916 , Loss: 0.0004245143791195005\n",
            "Epoch: 26917 , Loss: 0.00041739296284504235\n",
            "Epoch: 26918 , Loss: 0.0004176661605015397\n",
            "Epoch: 26919 , Loss: 0.00041935715125873685\n",
            "Epoch: 26920 , Loss: 0.00041945930570364\n",
            "Epoch: 26921 , Loss: 0.000413873785873875\n",
            "Epoch: 26922 , Loss: 0.0004195320652797818\n",
            "Epoch: 26923 , Loss: 0.00041188846807926893\n",
            "Epoch: 26924 , Loss: 0.00042145390762016177\n",
            "Epoch: 26925 , Loss: 0.0004180041723884642\n",
            "Epoch: 26926 , Loss: 0.0004188476304989308\n",
            "Epoch: 26927 , Loss: 0.0004170338506810367\n",
            "Epoch: 26928 , Loss: 0.0004196640511509031\n",
            "Epoch: 26929 , Loss: 0.0004217672976665199\n",
            "Epoch: 26930 , Loss: 0.0004183457640465349\n",
            "Epoch: 26931 , Loss: 0.0004185107827652246\n",
            "Epoch: 26932 , Loss: 0.00042130862129852176\n",
            "Epoch: 26933 , Loss: 0.00042082532308995724\n",
            "Epoch: 26934 , Loss: 0.00041892618173733354\n",
            "Epoch: 26935 , Loss: 0.00041621935088187456\n",
            "Epoch: 26936 , Loss: 0.00042222754564136267\n",
            "Epoch: 26937 , Loss: 0.00041760082240216434\n",
            "Epoch: 26938 , Loss: 0.00041677741683088243\n",
            "Epoch: 26939 , Loss: 0.0004181596450507641\n",
            "Epoch: 26940 , Loss: 0.0004173586203251034\n",
            "Epoch: 26941 , Loss: 0.0004187629674561322\n",
            "Epoch: 26942 , Loss: 0.00041681539732962847\n",
            "Epoch: 26943 , Loss: 0.0004179881070740521\n",
            "Epoch: 26944 , Loss: 0.0004198706883471459\n",
            "Epoch: 26945 , Loss: 0.0004164547426626086\n",
            "Epoch: 26946 , Loss: 0.0004178052768111229\n",
            "Epoch: 26947 , Loss: 0.000416342489188537\n",
            "Epoch: 26948 , Loss: 0.0004198512469884008\n",
            "Epoch: 26949 , Loss: 0.0004204433353152126\n",
            "Epoch: 26950 , Loss: 0.00041861640056595206\n",
            "Epoch: 26951 , Loss: 0.00041449832497164607\n",
            "Epoch: 26952 , Loss: 0.00041732704266905785\n",
            "Epoch: 26953 , Loss: 0.00041410664562135935\n",
            "Epoch: 26954 , Loss: 0.0004172754124738276\n",
            "Epoch: 26955 , Loss: 0.00041710518416948617\n",
            "Epoch: 26956 , Loss: 0.0004169768071733415\n",
            "Epoch: 26957 , Loss: 0.0004155270871706307\n",
            "Epoch: 26958 , Loss: 0.0004228365723975003\n",
            "Epoch: 26959 , Loss: 0.00041691886144690216\n",
            "Epoch: 26960 , Loss: 0.0004148637817706913\n",
            "Epoch: 26961 , Loss: 0.0004151000757701695\n",
            "Epoch: 26962 , Loss: 0.0004195897199679166\n",
            "Epoch: 26963 , Loss: 0.00041668210178613663\n",
            "Epoch: 26964 , Loss: 0.00041662933654151857\n",
            "Epoch: 26965 , Loss: 0.00041889504063874483\n",
            "Epoch: 26966 , Loss: 0.00041940741357393563\n",
            "Epoch: 26967 , Loss: 0.000415139424148947\n",
            "Epoch: 26968 , Loss: 0.0004219433758407831\n",
            "Epoch: 26969 , Loss: 0.00041513051837682724\n",
            "Epoch: 26970 , Loss: 0.00042090658098459244\n",
            "Epoch: 26971 , Loss: 0.0004182116244919598\n",
            "Epoch: 26972 , Loss: 0.0004162727855145931\n",
            "Epoch: 26973 , Loss: 0.00041616131784394383\n",
            "Epoch: 26974 , Loss: 0.00041472911834716797\n",
            "Epoch: 26975 , Loss: 0.00041439125197939575\n",
            "Epoch: 26976 , Loss: 0.00041596253868192434\n",
            "Epoch: 26977 , Loss: 0.0004178490489721298\n",
            "Epoch: 26978 , Loss: 0.0004124428960494697\n",
            "Epoch: 26979 , Loss: 0.0004155570059083402\n",
            "Epoch: 26980 , Loss: 0.000417773931985721\n",
            "Epoch: 26981 , Loss: 0.0004156692011747509\n",
            "Epoch: 26982 , Loss: 0.0004141840909142047\n",
            "Epoch: 26983 , Loss: 0.0004152626497671008\n",
            "Epoch: 26984 , Loss: 0.0004139417433179915\n",
            "Epoch: 26985 , Loss: 0.0004154669586569071\n",
            "Epoch: 26986 , Loss: 0.000416114809922874\n",
            "Epoch: 26987 , Loss: 0.0004172417975496501\n",
            "Epoch: 26988 , Loss: 0.0004172646440565586\n",
            "Epoch: 26989 , Loss: 0.0004152829642407596\n",
            "Epoch: 26990 , Loss: 0.0004151517350692302\n",
            "Epoch: 26991 , Loss: 0.0004168002342339605\n",
            "Epoch: 26992 , Loss: 0.00041528636938892305\n",
            "Epoch: 26993 , Loss: 0.0004135571653023362\n",
            "Epoch: 26994 , Loss: 0.00041468587005510926\n",
            "Epoch: 26995 , Loss: 0.0004174820496700704\n",
            "Epoch: 26996 , Loss: 0.0004118239739909768\n",
            "Epoch: 26997 , Loss: 0.00041427440010011196\n",
            "Epoch: 26998 , Loss: 0.00041463339584879577\n",
            "Epoch: 26999 , Loss: 0.0004118874203413725\n",
            "Epoch: 27000 , Loss: 0.00041678149136714637\n",
            "=============================================\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "37 correctly classified among 100\n",
            "Accuracy as of 27000 epochs: 37.0\n",
            "=============================================\n",
            "Epoch: 27001 , Loss: 0.00041690916987136006\n",
            "Epoch: 27002 , Loss: 0.0004160586977377534\n",
            "Epoch: 27003 , Loss: 0.00040966641972772777\n",
            "Epoch: 27004 , Loss: 0.0004162985133007169\n",
            "Epoch: 27005 , Loss: 0.0004158128867857158\n",
            "Epoch: 27006 , Loss: 0.00041252607479691505\n",
            "Epoch: 27007 , Loss: 0.0004139672964811325\n",
            "Epoch: 27008 , Loss: 0.0004139738157391548\n",
            "Epoch: 27009 , Loss: 0.0004145667189732194\n",
            "Epoch: 27010 , Loss: 0.0004135980852879584\n",
            "Epoch: 27011 , Loss: 0.00041265838081017137\n",
            "Epoch: 27012 , Loss: 0.00041844951920211315\n",
            "Epoch: 27013 , Loss: 0.00041360536124557257\n",
            "Epoch: 27014 , Loss: 0.000413479283452034\n",
            "Epoch: 27015 , Loss: 0.0004152769106440246\n",
            "Epoch: 27016 , Loss: 0.0004132273024879396\n",
            "Epoch: 27017 , Loss: 0.0004124281695112586\n",
            "Epoch: 27018 , Loss: 0.00041445024544373155\n",
            "Epoch: 27019 , Loss: 0.00041191864875145257\n",
            "Epoch: 27020 , Loss: 0.00041513366159051657\n",
            "Epoch: 27021 , Loss: 0.0004129289009142667\n",
            "Epoch: 27022 , Loss: 0.00041157560190185905\n",
            "Epoch: 27023 , Loss: 0.0004122468235436827\n",
            "Epoch: 27024 , Loss: 0.0004132205795031041\n",
            "Epoch: 27025 , Loss: 0.00041150598553940654\n",
            "Epoch: 27026 , Loss: 0.00041282863821834326\n",
            "Epoch: 27027 , Loss: 0.00041282299207523465\n",
            "Epoch: 27028 , Loss: 0.0004139050142839551\n",
            "Epoch: 27029 , Loss: 0.000420055934228003\n",
            "Epoch: 27030 , Loss: 0.00041488182614557445\n",
            "Epoch: 27031 , Loss: 0.00041246050386689603\n",
            "Epoch: 27032 , Loss: 0.00041231123032048345\n",
            "Epoch: 27033 , Loss: 0.00041245599277317524\n",
            "Epoch: 27034 , Loss: 0.0004119902150705457\n",
            "Epoch: 27035 , Loss: 0.0004137171490583569\n",
            "Epoch: 27036 , Loss: 0.000412199180573225\n",
            "Epoch: 27037 , Loss: 0.00041371918632648885\n",
            "Epoch: 27038 , Loss: 0.0004142292309552431\n",
            "Epoch: 27039 , Loss: 0.00041227543260902166\n",
            "Epoch: 27040 , Loss: 0.000412362307542935\n",
            "Epoch: 27041 , Loss: 0.00041073979809880257\n",
            "Epoch: 27042 , Loss: 0.00041189734474755824\n",
            "Epoch: 27043 , Loss: 0.00041160674300044775\n",
            "Epoch: 27044 , Loss: 0.00041196829988621175\n",
            "Epoch: 27045 , Loss: 0.00041178910760208964\n",
            "Epoch: 27046 , Loss: 0.000408044783398509\n",
            "Epoch: 27047 , Loss: 0.00040891856770031154\n",
            "Epoch: 27048 , Loss: 0.00041340192547068\n",
            "Epoch: 27049 , Loss: 0.0004100882215425372\n",
            "Epoch: 27050 , Loss: 0.00041138342930935323\n",
            "Epoch: 27051 , Loss: 0.0004132086178287864\n",
            "Epoch: 27052 , Loss: 0.0004112006281502545\n",
            "Epoch: 27053 , Loss: 0.00041041034273803234\n",
            "Epoch: 27054 , Loss: 0.0004156402428634465\n",
            "Epoch: 27055 , Loss: 0.0004171454638708383\n",
            "Epoch: 27056 , Loss: 0.00041084838449023664\n",
            "Epoch: 27057 , Loss: 0.0004109002184122801\n",
            "Epoch: 27058 , Loss: 0.0004118919023312628\n",
            "Epoch: 27059 , Loss: 0.00041024296660907567\n",
            "Epoch: 27060 , Loss: 0.0004107506829313934\n",
            "Epoch: 27061 , Loss: 0.00041182051063515246\n",
            "Epoch: 27062 , Loss: 0.00041064750985242426\n",
            "Epoch: 27063 , Loss: 0.0004103881074115634\n",
            "Epoch: 27064 , Loss: 0.0004085779655724764\n",
            "Epoch: 27065 , Loss: 0.000409297994337976\n",
            "Epoch: 27066 , Loss: 0.0004119168734177947\n",
            "Epoch: 27067 , Loss: 0.00040880427695810795\n",
            "Epoch: 27068 , Loss: 0.0004101501835975796\n",
            "Epoch: 27069 , Loss: 0.00041281883022747934\n",
            "Epoch: 27070 , Loss: 0.0004083881212864071\n",
            "Epoch: 27071 , Loss: 0.0004092439776286483\n",
            "Epoch: 27072 , Loss: 0.00040949470712803304\n",
            "Epoch: 27073 , Loss: 0.00041169210453517735\n",
            "Epoch: 27074 , Loss: 0.0004101005906704813\n",
            "Epoch: 27075 , Loss: 0.00040875704144127667\n",
            "Epoch: 27076 , Loss: 0.0004099856596440077\n",
            "Epoch: 27077 , Loss: 0.00040604869718663394\n",
            "Epoch: 27078 , Loss: 0.0004094561736565083\n",
            "Epoch: 27079 , Loss: 0.0004076000477652997\n",
            "Epoch: 27080 , Loss: 0.00040910689858719707\n",
            "Epoch: 27081 , Loss: 0.00041050714207813144\n",
            "Epoch: 27082 , Loss: 0.0004052056174259633\n",
            "Epoch: 27083 , Loss: 0.0004077127086929977\n",
            "Epoch: 27084 , Loss: 0.0004090496804565191\n",
            "Epoch: 27085 , Loss: 0.000409552245400846\n",
            "Epoch: 27086 , Loss: 0.0004088883870281279\n",
            "Epoch: 27087 , Loss: 0.0004118495562579483\n",
            "Epoch: 27088 , Loss: 0.0004070159629918635\n",
            "Epoch: 27089 , Loss: 0.0004147313302382827\n",
            "Epoch: 27090 , Loss: 0.00040876553975977004\n",
            "Epoch: 27091 , Loss: 0.00040905282367020845\n",
            "Epoch: 27092 , Loss: 0.0004112122405786067\n",
            "Epoch: 27093 , Loss: 0.0004085795662831515\n",
            "Epoch: 27094 , Loss: 0.0004085563123226166\n",
            "Epoch: 27095 , Loss: 0.00041144632268697023\n",
            "Epoch: 27096 , Loss: 0.00040575454477220774\n",
            "Epoch: 27097 , Loss: 0.00040836568223312497\n",
            "Epoch: 27098 , Loss: 0.0004100589721929282\n",
            "Epoch: 27099 , Loss: 0.00041197409154847264\n",
            "Epoch: 27100 , Loss: 0.00040829461067914963\n",
            "Epoch: 27101 , Loss: 0.00040824266034178436\n",
            "Epoch: 27102 , Loss: 0.0004103888350073248\n",
            "Epoch: 27103 , Loss: 0.0004070757422596216\n",
            "Epoch: 27104 , Loss: 0.000410733453463763\n",
            "Epoch: 27105 , Loss: 0.00041086773853749037\n",
            "Epoch: 27106 , Loss: 0.0004079840728081763\n",
            "Epoch: 27107 , Loss: 0.00040868029464036226\n",
            "Epoch: 27108 , Loss: 0.000410630542319268\n",
            "Epoch: 27109 , Loss: 0.0004081372171640396\n",
            "Epoch: 27110 , Loss: 0.00040860226727090776\n",
            "Epoch: 27111 , Loss: 0.0004057501209899783\n",
            "Epoch: 27112 , Loss: 0.000407662068028003\n",
            "Epoch: 27113 , Loss: 0.0004092876042705029\n",
            "Epoch: 27114 , Loss: 0.00040630431612953544\n",
            "Epoch: 27115 , Loss: 0.0004083805251866579\n",
            "Epoch: 27116 , Loss: 0.0004092397284694016\n",
            "Epoch: 27117 , Loss: 0.0004066373221576214\n",
            "Epoch: 27118 , Loss: 0.00040660149534232914\n",
            "Epoch: 27119 , Loss: 0.000408324645832181\n",
            "Epoch: 27120 , Loss: 0.00040753354551270604\n",
            "Epoch: 27121 , Loss: 0.00040762449498288333\n",
            "Epoch: 27122 , Loss: 0.00040533175342716277\n",
            "Epoch: 27123 , Loss: 0.0004099209327250719\n",
            "Epoch: 27124 , Loss: 0.0004063188680447638\n",
            "Epoch: 27125 , Loss: 0.00040704559069126844\n",
            "Epoch: 27126 , Loss: 0.00040960669866763055\n",
            "Epoch: 27127 , Loss: 0.00040519225876778364\n",
            "Epoch: 27128 , Loss: 0.00041124483686871827\n",
            "Epoch: 27129 , Loss: 0.0004054426390212029\n",
            "Epoch: 27130 , Loss: 0.0004080419021192938\n",
            "Epoch: 27131 , Loss: 0.0004049051785841584\n",
            "Epoch: 27132 , Loss: 0.0004072341544087976\n",
            "Epoch: 27133 , Loss: 0.00040555998566560447\n",
            "Epoch: 27134 , Loss: 0.00040614945464767516\n",
            "Epoch: 27135 , Loss: 0.000407323706895113\n",
            "Epoch: 27136 , Loss: 0.0004071862786076963\n",
            "Epoch: 27137 , Loss: 0.0004073874733876437\n",
            "Epoch: 27138 , Loss: 0.0004059619386680424\n",
            "Epoch: 27139 , Loss: 0.00040619910578243434\n",
            "Epoch: 27140 , Loss: 0.0004058319318573922\n",
            "Epoch: 27141 , Loss: 0.0004057364130858332\n",
            "Epoch: 27142 , Loss: 0.0004108038847334683\n",
            "Epoch: 27143 , Loss: 0.00040653711766935885\n",
            "Epoch: 27144 , Loss: 0.00040430782246403396\n",
            "Epoch: 27145 , Loss: 0.0004066734400112182\n",
            "Epoch: 27146 , Loss: 0.0004041091015096754\n",
            "Epoch: 27147 , Loss: 0.000405408936785534\n",
            "Epoch: 27148 , Loss: 0.0004073974268976599\n",
            "Epoch: 27149 , Loss: 0.00040462514152750373\n",
            "Epoch: 27150 , Loss: 0.00040507136145606637\n",
            "Epoch: 27151 , Loss: 0.0004020438645966351\n",
            "Epoch: 27152 , Loss: 0.0004050491552334279\n",
            "Epoch: 27153 , Loss: 0.0004048062255606055\n",
            "Epoch: 27154 , Loss: 0.00040504970820620656\n",
            "Epoch: 27155 , Loss: 0.00040633161552250385\n",
            "Epoch: 27156 , Loss: 0.00040423532482236624\n",
            "Epoch: 27157 , Loss: 0.00040631223237141967\n",
            "Epoch: 27158 , Loss: 0.0004052448784932494\n",
            "Epoch: 27159 , Loss: 0.00040429821819998324\n",
            "Epoch: 27160 , Loss: 0.000404697610065341\n",
            "Epoch: 27161 , Loss: 0.0004070046707056463\n",
            "Epoch: 27162 , Loss: 0.0004079953650943935\n",
            "Epoch: 27163 , Loss: 0.00040576833998784423\n",
            "Epoch: 27164 , Loss: 0.0004027850809507072\n",
            "Epoch: 27165 , Loss: 0.0004040976637043059\n",
            "Epoch: 27166 , Loss: 0.0004056240723002702\n",
            "Epoch: 27167 , Loss: 0.00040574889862909913\n",
            "Epoch: 27168 , Loss: 0.0004054988385178149\n",
            "Epoch: 27169 , Loss: 0.00040387915214523673\n",
            "Epoch: 27170 , Loss: 0.0004023528890684247\n",
            "Epoch: 27171 , Loss: 0.0004039766499772668\n",
            "Epoch: 27172 , Loss: 0.00040364323649555445\n",
            "Epoch: 27173 , Loss: 0.0004027951508760452\n",
            "Epoch: 27174 , Loss: 0.00040651613380759954\n",
            "Epoch: 27175 , Loss: 0.0004036083701066673\n",
            "Epoch: 27176 , Loss: 0.0004048069240525365\n",
            "Epoch: 27177 , Loss: 0.00040210754377767444\n",
            "Epoch: 27178 , Loss: 0.00040250230813398957\n",
            "Epoch: 27179 , Loss: 0.0004034774610772729\n",
            "Epoch: 27180 , Loss: 0.00040194333996623755\n",
            "Epoch: 27181 , Loss: 0.00040331223863177\n",
            "Epoch: 27182 , Loss: 0.0004051364667247981\n",
            "Epoch: 27183 , Loss: 0.00040305545553565025\n",
            "Epoch: 27184 , Loss: 0.00040414679097011685\n",
            "Epoch: 27185 , Loss: 0.0004029365663882345\n",
            "Epoch: 27186 , Loss: 0.0004028201219625771\n",
            "Epoch: 27187 , Loss: 0.00040202660602517426\n",
            "Epoch: 27188 , Loss: 0.00040274130878970027\n",
            "Epoch: 27189 , Loss: 0.000402493606088683\n",
            "Epoch: 27190 , Loss: 0.0004037148901261389\n",
            "Epoch: 27191 , Loss: 0.0003999072650913149\n",
            "Epoch: 27192 , Loss: 0.0004022548964712769\n",
            "Epoch: 27193 , Loss: 0.0004037388716824353\n",
            "Epoch: 27194 , Loss: 0.00040207631536759436\n",
            "Epoch: 27195 , Loss: 0.0004006572999060154\n",
            "Epoch: 27196 , Loss: 0.00040371125214733183\n",
            "Epoch: 27197 , Loss: 0.0003989906399510801\n",
            "Epoch: 27198 , Loss: 0.000401303666876629\n",
            "Epoch: 27199 , Loss: 0.0004049485141877085\n",
            "Epoch: 27200 , Loss: 0.0004033478326164186\n",
            "Epoch: 27201 , Loss: 0.00040417315904051065\n",
            "Epoch: 27202 , Loss: 0.00040413299575448036\n",
            "Epoch: 27203 , Loss: 0.000401177880121395\n",
            "Epoch: 27204 , Loss: 0.00040009483927860856\n",
            "Epoch: 27205 , Loss: 0.0004031698626931757\n",
            "Epoch: 27206 , Loss: 0.00040153376176021993\n",
            "Epoch: 27207 , Loss: 0.0004013122234027833\n",
            "Epoch: 27208 , Loss: 0.00040015808190219104\n",
            "Epoch: 27209 , Loss: 0.0004012238059658557\n",
            "Epoch: 27210 , Loss: 0.0004028781841043383\n",
            "Epoch: 27211 , Loss: 0.0003999406471848488\n",
            "Epoch: 27212 , Loss: 0.0004010613774880767\n",
            "Epoch: 27213 , Loss: 0.00040077409357763827\n",
            "Epoch: 27214 , Loss: 0.00040191394509747624\n",
            "Epoch: 27215 , Loss: 0.00040093844290822744\n",
            "Epoch: 27216 , Loss: 0.0003991984995082021\n",
            "Epoch: 27217 , Loss: 0.0004006719682365656\n",
            "Epoch: 27218 , Loss: 0.0004023768415208906\n",
            "Epoch: 27219 , Loss: 0.00039883755380287766\n",
            "Epoch: 27220 , Loss: 0.00039515175740234554\n",
            "Epoch: 27221 , Loss: 0.0004020805354230106\n",
            "Epoch: 27222 , Loss: 0.0004033052537124604\n",
            "Epoch: 27223 , Loss: 0.00039671437116339803\n",
            "Epoch: 27224 , Loss: 0.00040557721513323486\n",
            "Epoch: 27225 , Loss: 0.0004045722307637334\n",
            "Epoch: 27226 , Loss: 0.00039881616248749197\n",
            "Epoch: 27227 , Loss: 0.0004004738584626466\n",
            "Epoch: 27228 , Loss: 0.0004047242400702089\n",
            "Epoch: 27229 , Loss: 0.00039957690751180053\n",
            "Epoch: 27230 , Loss: 0.00040193830500356853\n",
            "Epoch: 27231 , Loss: 0.0003972803824581206\n",
            "Epoch: 27232 , Loss: 0.0003981047193519771\n",
            "Epoch: 27233 , Loss: 0.0003995777224190533\n",
            "Epoch: 27234 , Loss: 0.0004009930416941643\n",
            "Epoch: 27235 , Loss: 0.0003981544286943972\n",
            "Epoch: 27236 , Loss: 0.0003994573780801147\n",
            "Epoch: 27237 , Loss: 0.00039931543869897723\n",
            "Epoch: 27238 , Loss: 0.00039790113805793226\n",
            "Epoch: 27239 , Loss: 0.00039983930764719844\n",
            "Epoch: 27240 , Loss: 0.00040174394962377846\n",
            "Epoch: 27241 , Loss: 0.0003982235211879015\n",
            "Epoch: 27242 , Loss: 0.00040264881681650877\n",
            "Epoch: 27243 , Loss: 0.0003956154687330127\n",
            "Epoch: 27244 , Loss: 0.0003976469160988927\n",
            "Epoch: 27245 , Loss: 0.0003994530998170376\n",
            "Epoch: 27246 , Loss: 0.0003989734104834497\n",
            "Epoch: 27247 , Loss: 0.0003991622943431139\n",
            "Epoch: 27248 , Loss: 0.0003986212541349232\n",
            "Epoch: 27249 , Loss: 0.00039622222539037466\n",
            "Epoch: 27250 , Loss: 0.0003985594375990331\n",
            "=============================================\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 43ms/step\n",
            "26 correctly classified among 100\n",
            "Accuracy as of 27250 epochs: 26.0\n",
            "=============================================\n",
            "Epoch: 27251 , Loss: 0.0003973822749685496\n",
            "Epoch: 27252 , Loss: 0.0004003028152510524\n",
            "Epoch: 27253 , Loss: 0.0003973362618125975\n",
            "Epoch: 27254 , Loss: 0.00039926727185957134\n",
            "Epoch: 27255 , Loss: 0.00039714190643280745\n",
            "Epoch: 27256 , Loss: 0.00039697380270808935\n",
            "Epoch: 27257 , Loss: 0.0004023222136311233\n",
            "Epoch: 27258 , Loss: 0.0004014045698568225\n",
            "Epoch: 27259 , Loss: 0.00040127092506736517\n",
            "Epoch: 27260 , Loss: 0.0004002476925961673\n",
            "Epoch: 27261 , Loss: 0.0003955416614189744\n",
            "Epoch: 27262 , Loss: 0.00039901724085211754\n",
            "Epoch: 27263 , Loss: 0.00039931046194396913\n",
            "Epoch: 27264 , Loss: 0.0003964679199270904\n",
            "Epoch: 27265 , Loss: 0.000395948882214725\n",
            "Epoch: 27266 , Loss: 0.00039999448927119374\n",
            "Epoch: 27267 , Loss: 0.00039552731323055923\n",
            "Epoch: 27268 , Loss: 0.0003967200464103371\n",
            "Epoch: 27269 , Loss: 0.00039758524508215487\n",
            "Epoch: 27270 , Loss: 0.0003976369625888765\n",
            "Epoch: 27271 , Loss: 0.0003970828838646412\n",
            "Epoch: 27272 , Loss: 0.00039547367487102747\n",
            "Epoch: 27273 , Loss: 0.0003975105646532029\n",
            "Epoch: 27274 , Loss: 0.0003985346993431449\n",
            "Epoch: 27275 , Loss: 0.0003969398676417768\n",
            "Epoch: 27276 , Loss: 0.0003969270037487149\n",
            "Epoch: 27277 , Loss: 0.0003966992371715605\n",
            "Epoch: 27278 , Loss: 0.00039763175300322473\n",
            "Epoch: 27279 , Loss: 0.0003972644917666912\n",
            "Epoch: 27280 , Loss: 0.0003990585100837052\n",
            "Epoch: 27281 , Loss: 0.00039647426456213\n",
            "Epoch: 27282 , Loss: 0.0003966076474171132\n",
            "Epoch: 27283 , Loss: 0.0003963014460168779\n",
            "Epoch: 27284 , Loss: 0.00039626413490623236\n",
            "Epoch: 27285 , Loss: 0.0003974115243181586\n",
            "Epoch: 27286 , Loss: 0.00039792241295799613\n",
            "Epoch: 27287 , Loss: 0.0003999730688519776\n",
            "Epoch: 27288 , Loss: 0.00039613578701391816\n",
            "Epoch: 27289 , Loss: 0.00039740276406519115\n",
            "Epoch: 27290 , Loss: 0.00039326961268670857\n",
            "Epoch: 27291 , Loss: 0.00039505527820438147\n",
            "Epoch: 27292 , Loss: 0.0003964554925914854\n",
            "Epoch: 27293 , Loss: 0.0003964950446970761\n",
            "Epoch: 27294 , Loss: 0.00039867553277872503\n",
            "Epoch: 27295 , Loss: 0.00039980767178349197\n",
            "Epoch: 27296 , Loss: 0.00039203258347697556\n",
            "Epoch: 27297 , Loss: 0.0003952940460294485\n",
            "Epoch: 27298 , Loss: 0.000395779381506145\n",
            "Epoch: 27299 , Loss: 0.0003954339772462845\n",
            "Epoch: 27300 , Loss: 0.00039453394128941\n",
            "Epoch: 27301 , Loss: 0.00039587030187249184\n",
            "Epoch: 27302 , Loss: 0.00039217720041051507\n",
            "Epoch: 27303 , Loss: 0.00039644871139898896\n",
            "Epoch: 27304 , Loss: 0.0003968442033510655\n",
            "Epoch: 27305 , Loss: 0.00039416219806298614\n",
            "Epoch: 27306 , Loss: 0.0003956469881813973\n",
            "Epoch: 27307 , Loss: 0.0003938185691367835\n",
            "Epoch: 27308 , Loss: 0.0003958938759751618\n",
            "Epoch: 27309 , Loss: 0.0003928331716451794\n",
            "Epoch: 27310 , Loss: 0.00039490865310654044\n",
            "Epoch: 27311 , Loss: 0.0003943685151170939\n",
            "Epoch: 27312 , Loss: 0.0003955448919441551\n",
            "Epoch: 27313 , Loss: 0.00039457090315409005\n",
            "Epoch: 27314 , Loss: 0.0003927777870558202\n",
            "Epoch: 27315 , Loss: 0.0003946335054934025\n",
            "Epoch: 27316 , Loss: 0.00039626704528927803\n",
            "Epoch: 27317 , Loss: 0.00039270881097763777\n",
            "Epoch: 27318 , Loss: 0.0003937570727430284\n",
            "Epoch: 27319 , Loss: 0.0003943280316889286\n",
            "Epoch: 27320 , Loss: 0.0003938704903703183\n",
            "Epoch: 27321 , Loss: 0.00039339481736533344\n",
            "Epoch: 27322 , Loss: 0.00039414819912053645\n",
            "Epoch: 27323 , Loss: 0.00039190694224089384\n",
            "Epoch: 27324 , Loss: 0.00039263343205675483\n",
            "Epoch: 27325 , Loss: 0.0003965846262872219\n",
            "Epoch: 27326 , Loss: 0.00039411860052496195\n",
            "Epoch: 27327 , Loss: 0.00039264600491151214\n",
            "Epoch: 27328 , Loss: 0.00039424834540113807\n",
            "Epoch: 27329 , Loss: 0.0003945293137803674\n",
            "Epoch: 27330 , Loss: 0.00039233872666954994\n",
            "Epoch: 27331 , Loss: 0.00039458111859858036\n",
            "Epoch: 27332 , Loss: 0.0004002143396064639\n",
            "Epoch: 27333 , Loss: 0.00039352086605504155\n",
            "Epoch: 27334 , Loss: 0.0003940675815101713\n",
            "Epoch: 27335 , Loss: 0.00039278215263038874\n",
            "Epoch: 27336 , Loss: 0.0003937759902328253\n",
            "Epoch: 27337 , Loss: 0.00039226049557328224\n",
            "Epoch: 27338 , Loss: 0.0003926838980987668\n",
            "Epoch: 27339 , Loss: 0.0003925489727407694\n",
            "Epoch: 27340 , Loss: 0.00039259361801669\n",
            "Epoch: 27341 , Loss: 0.00039171549724414945\n",
            "Epoch: 27342 , Loss: 0.00039190257666632533\n",
            "Epoch: 27343 , Loss: 0.00039502824074588716\n",
            "Epoch: 27344 , Loss: 0.0003927628858946264\n",
            "Epoch: 27345 , Loss: 0.00039109960198402405\n",
            "Epoch: 27346 , Loss: 0.00039383687544614077\n",
            "Epoch: 27347 , Loss: 0.00039198820013552904\n",
            "Epoch: 27348 , Loss: 0.00039197978912852705\n",
            "Epoch: 27349 , Loss: 0.00039122995804063976\n",
            "Epoch: 27350 , Loss: 0.0003922768519259989\n",
            "Epoch: 27351 , Loss: 0.00039209279930219054\n",
            "Epoch: 27352 , Loss: 0.0003902263706550002\n",
            "Epoch: 27353 , Loss: 0.000391340785427019\n",
            "Epoch: 27354 , Loss: 0.00039325247053056955\n",
            "Epoch: 27355 , Loss: 0.0003928779624402523\n",
            "Epoch: 27356 , Loss: 0.0003910931118298322\n",
            "Epoch: 27357 , Loss: 0.0003912345855496824\n",
            "Epoch: 27358 , Loss: 0.0003919533337466419\n",
            "Epoch: 27359 , Loss: 0.0003913063555955887\n",
            "Epoch: 27360 , Loss: 0.000390015949960798\n",
            "Epoch: 27361 , Loss: 0.00039130522054620087\n",
            "Epoch: 27362 , Loss: 0.000390611618058756\n",
            "Epoch: 27363 , Loss: 0.00039091173675842583\n",
            "Epoch: 27364 , Loss: 0.00039164366899058223\n",
            "Epoch: 27365 , Loss: 0.00039044389268383384\n",
            "Epoch: 27366 , Loss: 0.0003906774800270796\n",
            "Epoch: 27367 , Loss: 0.00039066385943442583\n",
            "Epoch: 27368 , Loss: 0.00039354615728370845\n",
            "Epoch: 27369 , Loss: 0.00039091965300031006\n",
            "Epoch: 27370 , Loss: 0.0003899786388501525\n",
            "Epoch: 27371 , Loss: 0.00039050308987498283\n",
            "Epoch: 27372 , Loss: 0.0003908348735421896\n",
            "Epoch: 27373 , Loss: 0.00039123051101341844\n",
            "Epoch: 27374 , Loss: 0.00039266730891540647\n",
            "Epoch: 27375 , Loss: 0.0003920259478036314\n",
            "Epoch: 27376 , Loss: 0.0003913423861376941\n",
            "Epoch: 27377 , Loss: 0.0003905188641510904\n",
            "Epoch: 27378 , Loss: 0.0003898520953953266\n",
            "Epoch: 27379 , Loss: 0.00039048114558681846\n",
            "Epoch: 27380 , Loss: 0.00038995774229988456\n",
            "Epoch: 27381 , Loss: 0.00039013271452859044\n",
            "Epoch: 27382 , Loss: 0.0003888687351718545\n",
            "Epoch: 27383 , Loss: 0.00038952563772909343\n",
            "Epoch: 27384 , Loss: 0.00038913387106731534\n",
            "Epoch: 27385 , Loss: 0.00039022951386868954\n",
            "Epoch: 27386 , Loss: 0.0003903278848156333\n",
            "Epoch: 27387 , Loss: 0.0003916706482414156\n",
            "Epoch: 27388 , Loss: 0.0003896018606610596\n",
            "Epoch: 27389 , Loss: 0.00039026420563459396\n",
            "Epoch: 27390 , Loss: 0.00038816488813608885\n",
            "Epoch: 27391 , Loss: 0.000389284105040133\n",
            "Epoch: 27392 , Loss: 0.0003909363877028227\n",
            "Epoch: 27393 , Loss: 0.000388958549592644\n",
            "Epoch: 27394 , Loss: 0.0003895936533808708\n",
            "Epoch: 27395 , Loss: 0.0003890998777933419\n",
            "Epoch: 27396 , Loss: 0.0003875341499224305\n",
            "Epoch: 27397 , Loss: 0.000389869004720822\n",
            "Epoch: 27398 , Loss: 0.00038951224996708333\n",
            "Epoch: 27399 , Loss: 0.0003876613627653569\n",
            "Epoch: 27400 , Loss: 0.00038963090628385544\n",
            "Epoch: 27401 , Loss: 0.00038783453055657446\n",
            "Epoch: 27402 , Loss: 0.0003875336842611432\n",
            "Epoch: 27403 , Loss: 0.0003888909996021539\n",
            "Epoch: 27404 , Loss: 0.00038825394585728645\n",
            "Epoch: 27405 , Loss: 0.0003871901426464319\n",
            "Epoch: 27406 , Loss: 0.0003881648008245975\n",
            "Epoch: 27407 , Loss: 0.0003883626777678728\n",
            "Epoch: 27408 , Loss: 0.0003879290306940675\n",
            "Epoch: 27409 , Loss: 0.0003877026028931141\n",
            "Epoch: 27410 , Loss: 0.0003870325454045087\n",
            "Epoch: 27411 , Loss: 0.0003879991127178073\n",
            "Epoch: 27412 , Loss: 0.0003902701137121767\n",
            "Epoch: 27413 , Loss: 0.000386840314604342\n",
            "Epoch: 27414 , Loss: 0.0003877153794746846\n",
            "Epoch: 27415 , Loss: 0.00038722180761396885\n",
            "Epoch: 27416 , Loss: 0.00038706668419763446\n",
            "Epoch: 27417 , Loss: 0.00038902583764865994\n",
            "Epoch: 27418 , Loss: 0.0003854053793475032\n",
            "Epoch: 27419 , Loss: 0.00038824963849037886\n",
            "Epoch: 27420 , Loss: 0.0003864112659357488\n",
            "Epoch: 27421 , Loss: 0.0003853648086078465\n",
            "Epoch: 27422 , Loss: 0.00038591035990975797\n",
            "Epoch: 27423 , Loss: 0.0003863146994262934\n",
            "Epoch: 27424 , Loss: 0.0003866316401399672\n",
            "Epoch: 27425 , Loss: 0.00038787710946053267\n",
            "Epoch: 27426 , Loss: 0.0003873559180647135\n",
            "Epoch: 27427 , Loss: 0.00038599848630838096\n",
            "Epoch: 27428 , Loss: 0.0003843781305477023\n",
            "Epoch: 27429 , Loss: 0.0003854918177239597\n",
            "Epoch: 27430 , Loss: 0.0003851058427244425\n",
            "Epoch: 27431 , Loss: 0.00038349241367541254\n",
            "Epoch: 27432 , Loss: 0.0003877504786942154\n",
            "Epoch: 27433 , Loss: 0.00038783636409789324\n",
            "Epoch: 27434 , Loss: 0.0003849806380458176\n",
            "Epoch: 27435 , Loss: 0.0003828763437923044\n",
            "Epoch: 27436 , Loss: 0.00038469306309707463\n",
            "Epoch: 27437 , Loss: 0.0003890569496434182\n",
            "Epoch: 27438 , Loss: 0.0003888363135047257\n",
            "Epoch: 27439 , Loss: 0.00038460310315713286\n",
            "Epoch: 27440 , Loss: 0.00038584580761380494\n",
            "Epoch: 27441 , Loss: 0.0003842761507257819\n",
            "Epoch: 27442 , Loss: 0.0003888323262799531\n",
            "Epoch: 27443 , Loss: 0.00038959510857239366\n",
            "Epoch: 27444 , Loss: 0.00039151142118498683\n",
            "Epoch: 27445 , Loss: 0.00038469600258395076\n",
            "Epoch: 27446 , Loss: 0.0003836052492260933\n",
            "Epoch: 27447 , Loss: 0.00038623635191470385\n",
            "Epoch: 27448 , Loss: 0.0003866657498292625\n",
            "Epoch: 27449 , Loss: 0.00038568885065615177\n",
            "Epoch: 27450 , Loss: 0.00038486503763124347\n",
            "Epoch: 27451 , Loss: 0.000387072388548404\n",
            "Epoch: 27452 , Loss: 0.0003832918300759047\n",
            "Epoch: 27453 , Loss: 0.00038564929855056107\n",
            "Epoch: 27454 , Loss: 0.00038252584636211395\n",
            "Epoch: 27455 , Loss: 0.0003838842094410211\n",
            "Epoch: 27456 , Loss: 0.0003851551446132362\n",
            "Epoch: 27457 , Loss: 0.00038515051710419357\n",
            "Epoch: 27458 , Loss: 0.00038591193151660264\n",
            "Epoch: 27459 , Loss: 0.0003852342488244176\n",
            "Epoch: 27460 , Loss: 0.00038329680683091283\n",
            "Epoch: 27461 , Loss: 0.00038501564995385706\n",
            "Epoch: 27462 , Loss: 0.0003838978591375053\n",
            "Epoch: 27463 , Loss: 0.00038326936191879213\n",
            "Epoch: 27464 , Loss: 0.00038494932232424617\n",
            "Epoch: 27465 , Loss: 0.0003852964728139341\n",
            "Epoch: 27466 , Loss: 0.00038517708890140057\n",
            "Epoch: 27467 , Loss: 0.0003841854049824178\n",
            "Epoch: 27468 , Loss: 0.00038321397732943296\n",
            "Epoch: 27469 , Loss: 0.00038450444117188454\n",
            "Epoch: 27470 , Loss: 0.0003846186154987663\n",
            "Epoch: 27471 , Loss: 0.00038482589297927916\n",
            "Epoch: 27472 , Loss: 0.0003851642250083387\n",
            "Epoch: 27473 , Loss: 0.00038340111495926976\n",
            "Epoch: 27474 , Loss: 0.0003827040200121701\n",
            "Epoch: 27475 , Loss: 0.00038527362630702555\n",
            "Epoch: 27476 , Loss: 0.00038342204061336815\n",
            "Epoch: 27477 , Loss: 0.00038531882455572486\n",
            "Epoch: 27478 , Loss: 0.0003846331383101642\n",
            "Epoch: 27479 , Loss: 0.00038264531758613884\n",
            "Epoch: 27480 , Loss: 0.0003856549446936697\n",
            "Epoch: 27481 , Loss: 0.00038236670661717653\n",
            "Epoch: 27482 , Loss: 0.00038337314617820084\n",
            "Epoch: 27483 , Loss: 0.0003832009097095579\n",
            "Epoch: 27484 , Loss: 0.0003827041364274919\n",
            "Epoch: 27485 , Loss: 0.0003841834841296077\n",
            "Epoch: 27486 , Loss: 0.0003833778027910739\n",
            "Epoch: 27487 , Loss: 0.00038296947604976594\n",
            "Epoch: 27488 , Loss: 0.0003832260554190725\n",
            "Epoch: 27489 , Loss: 0.00038288478390313685\n",
            "Epoch: 27490 , Loss: 0.0003836998075712472\n",
            "Epoch: 27491 , Loss: 0.00038302066968753934\n",
            "Epoch: 27492 , Loss: 0.0003816483949776739\n",
            "Epoch: 27493 , Loss: 0.00038094131741672754\n",
            "Epoch: 27494 , Loss: 0.0003824079758487642\n",
            "Epoch: 27495 , Loss: 0.0003824496525339782\n",
            "Epoch: 27496 , Loss: 0.00038414273876696825\n",
            "Epoch: 27497 , Loss: 0.0003822472062893212\n",
            "Epoch: 27498 , Loss: 0.00038318918086588383\n",
            "Epoch: 27499 , Loss: 0.0003818799159489572\n",
            "Epoch: 27500 , Loss: 0.00038155290530994534\n",
            "=============================================\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "5 correctly classified among 100\n",
            "Accuracy as of 27500 epochs: 5.0\n",
            "=============================================\n",
            "Epoch: 27501 , Loss: 0.00038166565354913473\n",
            "Epoch: 27502 , Loss: 0.00038321781903505325\n",
            "Epoch: 27503 , Loss: 0.00038165689329616725\n",
            "Epoch: 27504 , Loss: 0.00038500907248817384\n",
            "Epoch: 27505 , Loss: 0.00038142979610711336\n",
            "Epoch: 27506 , Loss: 0.0003818224649876356\n",
            "Epoch: 27507 , Loss: 0.00038129702443256974\n",
            "Epoch: 27508 , Loss: 0.00038269441574811935\n",
            "Epoch: 27509 , Loss: 0.0003820718266069889\n",
            "Epoch: 27510 , Loss: 0.00038244095048867166\n",
            "Epoch: 27511 , Loss: 0.00038153474451974034\n",
            "Epoch: 27512 , Loss: 0.0003810355847235769\n",
            "Epoch: 27513 , Loss: 0.00038164365105330944\n",
            "Epoch: 27514 , Loss: 0.0003811193164438009\n",
            "Epoch: 27515 , Loss: 0.0003806198656093329\n",
            "Epoch: 27516 , Loss: 0.0003817233373411\n",
            "Epoch: 27517 , Loss: 0.00038046168629080057\n",
            "Epoch: 27518 , Loss: 0.00038123023114167154\n",
            "Epoch: 27519 , Loss: 0.00038002009387128055\n",
            "Epoch: 27520 , Loss: 0.00038229997153393924\n",
            "Epoch: 27521 , Loss: 0.00038128317100927234\n",
            "Epoch: 27522 , Loss: 0.0003791319322772324\n",
            "Epoch: 27523 , Loss: 0.00038074233452789485\n",
            "Epoch: 27524 , Loss: 0.00038054934702813625\n",
            "Epoch: 27525 , Loss: 0.00038007283001206815\n",
            "Epoch: 27526 , Loss: 0.00038150555337779224\n",
            "Epoch: 27527 , Loss: 0.0003789071342907846\n",
            "Epoch: 27528 , Loss: 0.0003797702374868095\n",
            "Epoch: 27529 , Loss: 0.0003799086553044617\n",
            "Epoch: 27530 , Loss: 0.0003805342421401292\n",
            "Epoch: 27531 , Loss: 0.00038045496330596507\n",
            "Epoch: 27532 , Loss: 0.00037919043097645044\n",
            "Epoch: 27533 , Loss: 0.00038127374136820436\n",
            "Epoch: 27534 , Loss: 0.0003800374979618937\n",
            "Epoch: 27535 , Loss: 0.00037819938734173775\n",
            "Epoch: 27536 , Loss: 0.00037908536614850163\n",
            "Epoch: 27537 , Loss: 0.00037928071105852723\n",
            "Epoch: 27538 , Loss: 0.00037929206155240536\n",
            "Epoch: 27539 , Loss: 0.0003788923495449126\n",
            "Epoch: 27540 , Loss: 0.0003785272128880024\n",
            "Epoch: 27541 , Loss: 0.000378986238501966\n",
            "Epoch: 27542 , Loss: 0.00037915472057648003\n",
            "Epoch: 27543 , Loss: 0.0003793994546867907\n",
            "Epoch: 27544 , Loss: 0.0003792098432313651\n",
            "Epoch: 27545 , Loss: 0.00037846859777346253\n",
            "Epoch: 27546 , Loss: 0.0003792348434217274\n",
            "Epoch: 27547 , Loss: 0.00037800686550326645\n",
            "Epoch: 27548 , Loss: 0.00037887293728999794\n",
            "Epoch: 27549 , Loss: 0.00037895713467150927\n",
            "Epoch: 27550 , Loss: 0.00037884333869442344\n",
            "Epoch: 27551 , Loss: 0.0003783556167036295\n",
            "Epoch: 27552 , Loss: 0.000379086472094059\n",
            "Epoch: 27553 , Loss: 0.0003793452924583107\n",
            "Epoch: 27554 , Loss: 0.0003777898382395506\n",
            "Epoch: 27555 , Loss: 0.0003778631507884711\n",
            "Epoch: 27556 , Loss: 0.0003798040561378002\n",
            "Epoch: 27557 , Loss: 0.00037766731111332774\n",
            "Epoch: 27558 , Loss: 0.0003779849212151021\n",
            "Epoch: 27559 , Loss: 0.0003781259001698345\n",
            "Epoch: 27560 , Loss: 0.00037744673318229616\n",
            "Epoch: 27561 , Loss: 0.0003781970008276403\n",
            "Epoch: 27562 , Loss: 0.0003782444109674543\n",
            "Epoch: 27563 , Loss: 0.00037686669384129345\n",
            "Epoch: 27564 , Loss: 0.00037717423401772976\n",
            "Epoch: 27565 , Loss: 0.00037830430665053427\n",
            "Epoch: 27566 , Loss: 0.0003764229768421501\n",
            "Epoch: 27567 , Loss: 0.00037712047924287617\n",
            "Epoch: 27568 , Loss: 0.0003779965918511152\n",
            "Epoch: 27569 , Loss: 0.00037795197567902505\n",
            "Epoch: 27570 , Loss: 0.0003780198749154806\n",
            "Epoch: 27571 , Loss: 0.0003766006848309189\n",
            "Epoch: 27572 , Loss: 0.0003767471353057772\n",
            "Epoch: 27573 , Loss: 0.0003760010004043579\n",
            "Epoch: 27574 , Loss: 0.0003785011940635741\n",
            "Epoch: 27575 , Loss: 0.00037227015127427876\n",
            "Epoch: 27576 , Loss: 0.0003787202585954219\n",
            "Epoch: 27577 , Loss: 0.00037738480023108423\n",
            "Epoch: 27578 , Loss: 0.0003780889674089849\n",
            "Epoch: 27579 , Loss: 0.00037691224133595824\n",
            "Epoch: 27580 , Loss: 0.0003796909877564758\n",
            "Epoch: 27581 , Loss: 0.00037699833046644926\n",
            "Epoch: 27582 , Loss: 0.000376378302462399\n",
            "Epoch: 27583 , Loss: 0.00037860064185224473\n",
            "Epoch: 27584 , Loss: 0.00037686328869313\n",
            "Epoch: 27585 , Loss: 0.0003758895618375391\n",
            "Epoch: 27586 , Loss: 0.0003763613058254123\n",
            "Epoch: 27587 , Loss: 0.0003765010042116046\n",
            "Epoch: 27588 , Loss: 0.0003758634557016194\n",
            "Epoch: 27589 , Loss: 0.0003773918142542243\n",
            "Epoch: 27590 , Loss: 0.0003769195463974029\n",
            "Epoch: 27591 , Loss: 0.0003758811217267066\n",
            "Epoch: 27592 , Loss: 0.00037588662235066295\n",
            "Epoch: 27593 , Loss: 0.0003757341764867306\n",
            "Epoch: 27594 , Loss: 0.00037583307130262256\n",
            "Epoch: 27595 , Loss: 0.0003749300667550415\n",
            "Epoch: 27596 , Loss: 0.00037529761902987957\n",
            "Epoch: 27597 , Loss: 0.00037643505493178964\n",
            "Epoch: 27598 , Loss: 0.00037495081778615713\n",
            "Epoch: 27599 , Loss: 0.00037409583455882967\n",
            "Epoch: 27600 , Loss: 0.0003751884796656668\n",
            "Epoch: 27601 , Loss: 0.00037619026261381805\n",
            "Epoch: 27602 , Loss: 0.0003748128074221313\n",
            "Epoch: 27603 , Loss: 0.00037469257949851453\n",
            "Epoch: 27604 , Loss: 0.000373626098735258\n",
            "Epoch: 27605 , Loss: 0.0003740539541468024\n",
            "Epoch: 27606 , Loss: 0.00037452345713973045\n",
            "Epoch: 27607 , Loss: 0.0003753057972062379\n",
            "Epoch: 27608 , Loss: 0.00037340220296755433\n",
            "Epoch: 27609 , Loss: 0.00037584631354548037\n",
            "Epoch: 27610 , Loss: 0.00037601287476718426\n",
            "Epoch: 27611 , Loss: 0.00037492922274395823\n",
            "Epoch: 27612 , Loss: 0.00037576467730104923\n",
            "Epoch: 27613 , Loss: 0.0003758954117074609\n",
            "Epoch: 27614 , Loss: 0.0003741054388228804\n",
            "Epoch: 27615 , Loss: 0.00037350389175117016\n",
            "Epoch: 27616 , Loss: 0.0003742756962310523\n",
            "Epoch: 27617 , Loss: 0.00037523871287703514\n",
            "Epoch: 27618 , Loss: 0.0003727547009475529\n",
            "Epoch: 27619 , Loss: 0.00037543827784247696\n",
            "Epoch: 27620 , Loss: 0.0003730531607288867\n",
            "Epoch: 27621 , Loss: 0.0003754004428628832\n",
            "Epoch: 27622 , Loss: 0.0003740767133422196\n",
            "Epoch: 27623 , Loss: 0.00037551327841356397\n",
            "Epoch: 27624 , Loss: 0.00037442584289237857\n",
            "Epoch: 27625 , Loss: 0.00037459490704350173\n",
            "Epoch: 27626 , Loss: 0.00037345493910834193\n",
            "Epoch: 27627 , Loss: 0.00037274282658472657\n",
            "Epoch: 27628 , Loss: 0.000374043796909973\n",
            "Epoch: 27629 , Loss: 0.00037210958544164896\n",
            "Epoch: 27630 , Loss: 0.0003737084043677896\n",
            "Epoch: 27631 , Loss: 0.0003738633240573108\n",
            "Epoch: 27632 , Loss: 0.00037210789741948247\n",
            "Epoch: 27633 , Loss: 0.00037343756412155926\n",
            "Epoch: 27634 , Loss: 0.0003739976091310382\n",
            "Epoch: 27635 , Loss: 0.00037276133662089705\n",
            "Epoch: 27636 , Loss: 0.0003700441448017955\n",
            "Epoch: 27637 , Loss: 0.00037125725066289306\n",
            "Epoch: 27638 , Loss: 0.0003759556857403368\n",
            "Epoch: 27639 , Loss: 0.00037058041198179126\n",
            "Epoch: 27640 , Loss: 0.0003723998961504549\n",
            "Epoch: 27641 , Loss: 0.0003735563368536532\n",
            "Epoch: 27642 , Loss: 0.0003751344047486782\n",
            "Epoch: 27643 , Loss: 0.0003749180759768933\n",
            "Epoch: 27644 , Loss: 0.00037785025779157877\n",
            "Epoch: 27645 , Loss: 0.000371695205103606\n",
            "Epoch: 27646 , Loss: 0.00037245533894747496\n",
            "Epoch: 27647 , Loss: 0.0003716283245012164\n",
            "Epoch: 27648 , Loss: 0.00037230574525892735\n",
            "Epoch: 27649 , Loss: 0.0003710336168296635\n",
            "Epoch: 27650 , Loss: 0.000371079018805176\n",
            "Epoch: 27651 , Loss: 0.0003725805727299303\n",
            "Epoch: 27652 , Loss: 0.00037055538268759847\n",
            "Epoch: 27653 , Loss: 0.0003749564348254353\n",
            "Epoch: 27654 , Loss: 0.00037253485061228275\n",
            "Epoch: 27655 , Loss: 0.0003736212383955717\n",
            "Epoch: 27656 , Loss: 0.0003728177980519831\n",
            "Epoch: 27657 , Loss: 0.0003715583588927984\n",
            "Epoch: 27658 , Loss: 0.00037048073136247694\n",
            "Epoch: 27659 , Loss: 0.00037088466342538595\n",
            "Epoch: 27660 , Loss: 0.0003706971474457532\n",
            "Epoch: 27661 , Loss: 0.00037132255965843797\n",
            "Epoch: 27662 , Loss: 0.0003719449741765857\n",
            "Epoch: 27663 , Loss: 0.00037121790228411555\n",
            "Epoch: 27664 , Loss: 0.0003720735548995435\n",
            "Epoch: 27665 , Loss: 0.0003722272231243551\n",
            "Epoch: 27666 , Loss: 0.0003727528383024037\n",
            "Epoch: 27667 , Loss: 0.0003711296885740012\n",
            "Epoch: 27668 , Loss: 0.000371760776033625\n",
            "Epoch: 27669 , Loss: 0.0003708247095346451\n",
            "Epoch: 27670 , Loss: 0.0003708677540998906\n",
            "Epoch: 27671 , Loss: 0.00037092791171744466\n",
            "Epoch: 27672 , Loss: 0.00037063236231915653\n",
            "Epoch: 27673 , Loss: 0.0003704895207192749\n",
            "Epoch: 27674 , Loss: 0.00037029077066108584\n",
            "Epoch: 27675 , Loss: 0.00037072086706757545\n",
            "Epoch: 27676 , Loss: 0.0003694150364026427\n",
            "Epoch: 27677 , Loss: 0.0003708147269207984\n",
            "Epoch: 27678 , Loss: 0.0003688948636408895\n",
            "Epoch: 27679 , Loss: 0.00036860426189377904\n",
            "Epoch: 27680 , Loss: 0.00037188248825259507\n",
            "Epoch: 27681 , Loss: 0.0003701855894178152\n",
            "Epoch: 27682 , Loss: 0.0003694724291563034\n",
            "Epoch: 27683 , Loss: 0.0003712891775649041\n",
            "Epoch: 27684 , Loss: 0.0003684718394652009\n",
            "Epoch: 27685 , Loss: 0.00037254387279972434\n",
            "Epoch: 27686 , Loss: 0.0003712029429152608\n",
            "Epoch: 27687 , Loss: 0.000369196233805269\n",
            "Epoch: 27688 , Loss: 0.00036991050001233816\n",
            "Epoch: 27689 , Loss: 0.00036975296097807586\n",
            "Epoch: 27690 , Loss: 0.00036959792487323284\n",
            "Epoch: 27691 , Loss: 0.00036894582444801927\n",
            "Epoch: 27692 , Loss: 0.00036875109071843326\n",
            "Epoch: 27693 , Loss: 0.00036994286347180605\n",
            "Epoch: 27694 , Loss: 0.00036928808549419045\n",
            "Epoch: 27695 , Loss: 0.00036815262865275145\n",
            "Epoch: 27696 , Loss: 0.0003677415079437196\n",
            "Epoch: 27697 , Loss: 0.000369841669453308\n",
            "Epoch: 27698 , Loss: 0.0003693804028443992\n",
            "Epoch: 27699 , Loss: 0.0003692750178743154\n",
            "Epoch: 27700 , Loss: 0.00036931666545569897\n",
            "Epoch: 27701 , Loss: 0.0003686053096316755\n",
            "Epoch: 27702 , Loss: 0.00036899701808579266\n",
            "Epoch: 27703 , Loss: 0.00036703614750877023\n",
            "Epoch: 27704 , Loss: 0.0003683607210405171\n",
            "Epoch: 27705 , Loss: 0.00036935796379111707\n",
            "Epoch: 27706 , Loss: 0.00036855333019047976\n",
            "Epoch: 27707 , Loss: 0.000366690888768062\n",
            "Epoch: 27708 , Loss: 0.0003682633978314698\n",
            "Epoch: 27709 , Loss: 0.00037019269075244665\n",
            "Epoch: 27710 , Loss: 0.00036854573409073055\n",
            "Epoch: 27711 , Loss: 0.0003686042327899486\n",
            "Epoch: 27712 , Loss: 0.0003670182195492089\n",
            "Epoch: 27713 , Loss: 0.0003684615367092192\n",
            "Epoch: 27714 , Loss: 0.0003680111840367317\n",
            "Epoch: 27715 , Loss: 0.0003667689161375165\n",
            "Epoch: 27716 , Loss: 0.00036923011066392064\n",
            "Epoch: 27717 , Loss: 0.00036807445576414466\n",
            "Epoch: 27718 , Loss: 0.00036773720057681203\n",
            "Epoch: 27719 , Loss: 0.0003686171257868409\n",
            "Epoch: 27720 , Loss: 0.00036786263808608055\n",
            "Epoch: 27721 , Loss: 0.000367536093108356\n",
            "Epoch: 27722 , Loss: 0.00036755570909008384\n",
            "Epoch: 27723 , Loss: 0.00036783202085644007\n",
            "Epoch: 27724 , Loss: 0.0003689104050863534\n",
            "Epoch: 27725 , Loss: 0.0003670054138638079\n",
            "Epoch: 27726 , Loss: 0.0003671020967885852\n",
            "Epoch: 27727 , Loss: 0.0003666196425911039\n",
            "Epoch: 27728 , Loss: 0.00036709365667775273\n",
            "Epoch: 27729 , Loss: 0.0003658020868897438\n",
            "Epoch: 27730 , Loss: 0.0003666345728561282\n",
            "Epoch: 27731 , Loss: 0.0003665297699626535\n",
            "Epoch: 27732 , Loss: 0.0003664051473606378\n",
            "Epoch: 27733 , Loss: 0.00036717462353408337\n",
            "Epoch: 27734 , Loss: 0.0003670246805995703\n",
            "Epoch: 27735 , Loss: 0.00036615412682294846\n",
            "Epoch: 27736 , Loss: 0.0003684947732836008\n",
            "Epoch: 27737 , Loss: 0.00036537006963044405\n",
            "Epoch: 27738 , Loss: 0.00036539253778755665\n",
            "Epoch: 27739 , Loss: 0.00036598893348127604\n",
            "Epoch: 27740 , Loss: 0.0003657319466583431\n",
            "Epoch: 27741 , Loss: 0.0003657568304333836\n",
            "Epoch: 27742 , Loss: 0.00036623398773372173\n",
            "Epoch: 27743 , Loss: 0.00036382648977451026\n",
            "Epoch: 27744 , Loss: 0.00036654769792221487\n",
            "Epoch: 27745 , Loss: 0.00036589644150808454\n",
            "Epoch: 27746 , Loss: 0.0003650355210993439\n",
            "Epoch: 27747 , Loss: 0.0003666084667202085\n",
            "Epoch: 27748 , Loss: 0.0003681378439068794\n",
            "Epoch: 27749 , Loss: 0.00036726961843669415\n",
            "Epoch: 27750 , Loss: 0.0003625312529038638\n",
            "=============================================\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "8 correctly classified among 100\n",
            "Accuracy as of 27750 epochs: 8.0\n",
            "=============================================\n",
            "Epoch: 27751 , Loss: 0.00036391697358340025\n",
            "Epoch: 27752 , Loss: 0.000363410945283249\n",
            "Epoch: 27753 , Loss: 0.0003658409113995731\n",
            "Epoch: 27754 , Loss: 0.0003633581509348005\n",
            "Epoch: 27755 , Loss: 0.0003628179256338626\n",
            "Epoch: 27756 , Loss: 0.0003651013830676675\n",
            "Epoch: 27757 , Loss: 0.00036454229848459363\n",
            "Epoch: 27758 , Loss: 0.0003685253032017499\n",
            "Epoch: 27759 , Loss: 0.00037009947118349373\n",
            "Epoch: 27760 , Loss: 0.00036397416261024773\n",
            "Epoch: 27761 , Loss: 0.00036276798346079886\n",
            "Epoch: 27762 , Loss: 0.0003655181499198079\n",
            "Epoch: 27763 , Loss: 0.0003624767414294183\n",
            "Epoch: 27764 , Loss: 0.0003644439857453108\n",
            "Epoch: 27765 , Loss: 0.0003665041003841907\n",
            "Epoch: 27766 , Loss: 0.0003696728963404894\n",
            "Epoch: 27767 , Loss: 0.0003657406778074801\n",
            "Epoch: 27768 , Loss: 0.00036545784678310156\n",
            "Epoch: 27769 , Loss: 0.0003637639165390283\n",
            "Epoch: 27770 , Loss: 0.00036467009340412915\n",
            "Epoch: 27771 , Loss: 0.00036466537858359516\n",
            "Epoch: 27772 , Loss: 0.00036419290699996054\n",
            "Epoch: 27773 , Loss: 0.0003644851385615766\n",
            "Epoch: 27774 , Loss: 0.00036357107455842197\n",
            "Epoch: 27775 , Loss: 0.00036434343201108277\n",
            "Epoch: 27776 , Loss: 0.00036480766721069813\n",
            "Epoch: 27777 , Loss: 0.0003625571262091398\n",
            "Epoch: 27778 , Loss: 0.00036290643038228154\n",
            "Epoch: 27779 , Loss: 0.00036845001159235835\n",
            "Epoch: 27780 , Loss: 0.00036246952367946506\n",
            "Epoch: 27781 , Loss: 0.00036423758137971163\n",
            "Epoch: 27782 , Loss: 0.00036259950138628483\n",
            "Epoch: 27783 , Loss: 0.0003649121499620378\n",
            "Epoch: 27784 , Loss: 0.0003633232554420829\n",
            "Epoch: 27785 , Loss: 0.00036435332731343806\n",
            "Epoch: 27786 , Loss: 0.0003630186547525227\n",
            "Epoch: 27787 , Loss: 0.0003664050018414855\n",
            "Epoch: 27788 , Loss: 0.0003640830982476473\n",
            "Epoch: 27789 , Loss: 0.0003627223486546427\n",
            "Epoch: 27790 , Loss: 0.0003637238987721503\n",
            "Epoch: 27791 , Loss: 0.00036412422196008265\n",
            "Epoch: 27792 , Loss: 0.00036310346331447363\n",
            "Epoch: 27793 , Loss: 0.0003624641103670001\n",
            "Epoch: 27794 , Loss: 0.0003630390274338424\n",
            "Epoch: 27795 , Loss: 0.00036256827297620475\n",
            "Epoch: 27796 , Loss: 0.0003632519510574639\n",
            "Epoch: 27797 , Loss: 0.00036241859197616577\n",
            "Epoch: 27798 , Loss: 0.00036301740328781307\n",
            "Epoch: 27799 , Loss: 0.00036288704723119736\n",
            "Epoch: 27800 , Loss: 0.0003621721698436886\n",
            "Epoch: 27801 , Loss: 0.0003620715579017997\n",
            "Epoch: 27802 , Loss: 0.0003618035407271236\n",
            "Epoch: 27803 , Loss: 0.0003622892254497856\n",
            "Epoch: 27804 , Loss: 0.00036199696478433907\n",
            "Epoch: 27805 , Loss: 0.00036176014691591263\n",
            "Epoch: 27806 , Loss: 0.000362919905455783\n",
            "Epoch: 27807 , Loss: 0.0003617992624640465\n",
            "Epoch: 27808 , Loss: 0.00036195048596709967\n",
            "Epoch: 27809 , Loss: 0.0003616181784309447\n",
            "Epoch: 27810 , Loss: 0.0003619999042712152\n",
            "Epoch: 27811 , Loss: 0.0003604012308642268\n",
            "Epoch: 27812 , Loss: 0.0003610881103668362\n",
            "Epoch: 27813 , Loss: 0.0003610415733419359\n",
            "Epoch: 27814 , Loss: 0.00036137306597083807\n",
            "Epoch: 27815 , Loss: 0.00036218902096152306\n",
            "Epoch: 27816 , Loss: 0.00036097486736252904\n",
            "Epoch: 27817 , Loss: 0.00036119334981776774\n",
            "Epoch: 27818 , Loss: 0.00036118007847107947\n",
            "Epoch: 27819 , Loss: 0.0003602743672672659\n",
            "Epoch: 27820 , Loss: 0.00036058202385902405\n",
            "Epoch: 27821 , Loss: 0.000361212354619056\n",
            "Epoch: 27822 , Loss: 0.0003610221901908517\n",
            "Epoch: 27823 , Loss: 0.0003601335920393467\n",
            "Epoch: 27824 , Loss: 0.00036082236329093575\n",
            "Epoch: 27825 , Loss: 0.0003614522283896804\n",
            "Epoch: 27826 , Loss: 0.00036014645593240857\n",
            "Epoch: 27827 , Loss: 0.0003605082747526467\n",
            "Epoch: 27828 , Loss: 0.00035855837631970644\n",
            "Epoch: 27829 , Loss: 0.0003612814180087298\n",
            "Epoch: 27830 , Loss: 0.00035975221544504166\n",
            "Epoch: 27831 , Loss: 0.00036004220601171255\n",
            "Epoch: 27832 , Loss: 0.0003597880422603339\n",
            "Epoch: 27833 , Loss: 0.0003585721424315125\n",
            "Epoch: 27834 , Loss: 0.0003614142769947648\n",
            "Epoch: 27835 , Loss: 0.00035936132189817727\n",
            "Epoch: 27836 , Loss: 0.00035796547308564186\n",
            "Epoch: 27837 , Loss: 0.000361106387572363\n",
            "Epoch: 27838 , Loss: 0.00036398458178155124\n",
            "Epoch: 27839 , Loss: 0.00035884001408703625\n",
            "Epoch: 27840 , Loss: 0.00036008073948323727\n",
            "Epoch: 27841 , Loss: 0.000362407008651644\n",
            "Epoch: 27842 , Loss: 0.0003593746805563569\n",
            "Epoch: 27843 , Loss: 0.00035950008896179497\n",
            "Epoch: 27844 , Loss: 0.00035967043368145823\n",
            "Epoch: 27845 , Loss: 0.00035925779957324266\n",
            "Epoch: 27846 , Loss: 0.0003592928114812821\n",
            "Epoch: 27847 , Loss: 0.0003593013680074364\n",
            "Epoch: 27848 , Loss: 0.00035871233558282256\n",
            "Epoch: 27849 , Loss: 0.000359948055120185\n",
            "Epoch: 27850 , Loss: 0.00035889027640223503\n",
            "Epoch: 27851 , Loss: 0.00035886664409190416\n",
            "Epoch: 27852 , Loss: 0.0003585547674447298\n",
            "Epoch: 27853 , Loss: 0.00035851087886840105\n",
            "Epoch: 27854 , Loss: 0.00035828081308864057\n",
            "Epoch: 27855 , Loss: 0.00035781250335276127\n",
            "Epoch: 27856 , Loss: 0.00035802999627776444\n",
            "Epoch: 27857 , Loss: 0.000357992626959458\n",
            "Epoch: 27858 , Loss: 0.00035896204644814134\n",
            "Epoch: 27859 , Loss: 0.0003579574404284358\n",
            "Epoch: 27860 , Loss: 0.00035995597136206925\n",
            "Epoch: 27861 , Loss: 0.00035829131957143545\n",
            "Epoch: 27862 , Loss: 0.00035955000203102827\n",
            "Epoch: 27863 , Loss: 0.0003575808950699866\n",
            "Epoch: 27864 , Loss: 0.0003581955097615719\n",
            "Epoch: 27865 , Loss: 0.0003579820040613413\n",
            "Epoch: 27866 , Loss: 0.00035732489777728915\n",
            "Epoch: 27867 , Loss: 0.00035797961754724383\n",
            "Epoch: 27868 , Loss: 0.0003576946328394115\n",
            "Epoch: 27869 , Loss: 0.0003583872749004513\n",
            "Epoch: 27870 , Loss: 0.00035871064756065607\n",
            "Epoch: 27871 , Loss: 0.00035669555654749274\n",
            "Epoch: 27872 , Loss: 0.00035894697066396475\n",
            "Epoch: 27873 , Loss: 0.0003594437730498612\n",
            "Epoch: 27874 , Loss: 0.0003585566591937095\n",
            "Epoch: 27875 , Loss: 0.00035735711571760476\n",
            "Epoch: 27876 , Loss: 0.0003575629380065948\n",
            "Epoch: 27877 , Loss: 0.0003569439286366105\n",
            "Epoch: 27878 , Loss: 0.0003569530963432044\n",
            "Epoch: 27879 , Loss: 0.0003573925350792706\n",
            "Epoch: 27880 , Loss: 0.0003570093831513077\n",
            "Epoch: 27881 , Loss: 0.00035689829383045435\n",
            "Epoch: 27882 , Loss: 0.0003566426457837224\n",
            "Epoch: 27883 , Loss: 0.00035693898098543286\n",
            "Epoch: 27884 , Loss: 0.0003565268125385046\n",
            "Epoch: 27885 , Loss: 0.00035658368142321706\n",
            "Epoch: 27886 , Loss: 0.0003561099001672119\n",
            "Epoch: 27887 , Loss: 0.0003547518281266093\n",
            "Epoch: 27888 , Loss: 0.00035913678584620357\n",
            "Epoch: 27889 , Loss: 0.00035502624814398587\n",
            "Epoch: 27890 , Loss: 0.00035641982685774565\n",
            "Epoch: 27891 , Loss: 0.00035740763996727765\n",
            "Epoch: 27892 , Loss: 0.00035510273301042616\n",
            "Epoch: 27893 , Loss: 0.0003600863565225154\n",
            "Epoch: 27894 , Loss: 0.0003554678405635059\n",
            "Epoch: 27895 , Loss: 0.0003543490020092577\n",
            "Epoch: 27896 , Loss: 0.0003552074485924095\n",
            "Epoch: 27897 , Loss: 0.0003554842551238835\n",
            "Epoch: 27898 , Loss: 0.0003559767792467028\n",
            "Epoch: 27899 , Loss: 0.0003559190081432462\n",
            "Epoch: 27900 , Loss: 0.00035571405896916986\n",
            "Epoch: 27901 , Loss: 0.00035616240347735584\n",
            "Epoch: 27902 , Loss: 0.00035525846760720015\n",
            "Epoch: 27903 , Loss: 0.00035570922773331404\n",
            "Epoch: 27904 , Loss: 0.0003543585480656475\n",
            "Epoch: 27905 , Loss: 0.0003561789926607162\n",
            "Epoch: 27906 , Loss: 0.0003558027383405715\n",
            "Epoch: 27907 , Loss: 0.0003534754505380988\n",
            "Epoch: 27908 , Loss: 0.00035524481791071594\n",
            "Epoch: 27909 , Loss: 0.000357589015038684\n",
            "Epoch: 27910 , Loss: 0.0003560493641998619\n",
            "Epoch: 27911 , Loss: 0.00035610914346762\n",
            "Epoch: 27912 , Loss: 0.00035517901415005326\n",
            "Epoch: 27913 , Loss: 0.0003534638090059161\n",
            "Epoch: 27914 , Loss: 0.0003547764790710062\n",
            "Epoch: 27915 , Loss: 0.0003544451028574258\n",
            "Epoch: 27916 , Loss: 0.0003545322688296437\n",
            "Epoch: 27917 , Loss: 0.0003530869435053319\n",
            "Epoch: 27918 , Loss: 0.00035662585287354887\n",
            "Epoch: 27919 , Loss: 0.00035862321965396404\n",
            "Epoch: 27920 , Loss: 0.0003554276772774756\n",
            "Epoch: 27921 , Loss: 0.00035518629010766745\n",
            "Epoch: 27922 , Loss: 0.0003544869541656226\n",
            "Epoch: 27923 , Loss: 0.00035352594568394125\n",
            "Epoch: 27924 , Loss: 0.0003538154414854944\n",
            "Epoch: 27925 , Loss: 0.0003532160189934075\n",
            "Epoch: 27926 , Loss: 0.0003553034330252558\n",
            "Epoch: 27927 , Loss: 0.00035404093796387315\n",
            "Epoch: 27928 , Loss: 0.0003503273183014244\n",
            "Epoch: 27929 , Loss: 0.0003530559770297259\n",
            "Epoch: 27930 , Loss: 0.0003537590382620692\n",
            "Epoch: 27931 , Loss: 0.00035402929643169045\n",
            "Epoch: 27932 , Loss: 0.00035175291122868657\n",
            "Epoch: 27933 , Loss: 0.0003527137159835547\n",
            "Epoch: 27934 , Loss: 0.00035248036147095263\n",
            "Epoch: 27935 , Loss: 0.00035520983510650694\n",
            "Epoch: 27936 , Loss: 0.00035457912599667907\n",
            "Epoch: 27937 , Loss: 0.0003480088780634105\n",
            "Epoch: 27938 , Loss: 0.00034875283017754555\n",
            "Epoch: 27939 , Loss: 0.00034877454163506627\n",
            "Epoch: 27940 , Loss: 0.00035444306558929384\n",
            "Epoch: 27941 , Loss: 0.0003529840905684978\n",
            "Epoch: 27942 , Loss: 0.0003686297277454287\n",
            "Epoch: 27943 , Loss: 0.00035615282831713557\n",
            "Epoch: 27944 , Loss: 0.0003527161607053131\n",
            "Epoch: 27945 , Loss: 0.00035516073694452643\n",
            "Epoch: 27946 , Loss: 0.00035307061625644565\n",
            "Epoch: 27947 , Loss: 0.00035451550502330065\n",
            "Epoch: 27948 , Loss: 0.0003519499150570482\n",
            "Epoch: 27949 , Loss: 0.0003533713170327246\n",
            "Epoch: 27950 , Loss: 0.0003517665318213403\n",
            "Epoch: 27951 , Loss: 0.00035252017551101744\n",
            "Epoch: 27952 , Loss: 0.0003532745176926255\n",
            "Epoch: 27953 , Loss: 0.0003519696765579283\n",
            "Epoch: 27954 , Loss: 0.0003527453518472612\n",
            "Epoch: 27955 , Loss: 0.0003523024497553706\n",
            "Epoch: 27956 , Loss: 0.0003521814360283315\n",
            "Epoch: 27957 , Loss: 0.0003525995125528425\n",
            "Epoch: 27958 , Loss: 0.00035208783810958266\n",
            "Epoch: 27959 , Loss: 0.0003518546000123024\n",
            "Epoch: 27960 , Loss: 0.00035166548332199454\n",
            "Epoch: 27961 , Loss: 0.0003514098934829235\n",
            "Epoch: 27962 , Loss: 0.00035178876714780927\n",
            "Epoch: 27963 , Loss: 0.0003527736116666347\n",
            "Epoch: 27964 , Loss: 0.00035205838503316045\n",
            "Epoch: 27965 , Loss: 0.0003514135896693915\n",
            "Epoch: 27966 , Loss: 0.00035180291160941124\n",
            "Epoch: 27967 , Loss: 0.0003516276483424008\n",
            "Epoch: 27968 , Loss: 0.0003516677825246006\n",
            "Epoch: 27969 , Loss: 0.00035251709050498903\n",
            "Epoch: 27970 , Loss: 0.0003508995287120342\n",
            "Epoch: 27971 , Loss: 0.00035104757989756763\n",
            "Epoch: 27972 , Loss: 0.00035085936542600393\n",
            "Epoch: 27973 , Loss: 0.00035058020148426294\n",
            "Epoch: 27974 , Loss: 0.00035089641460217535\n",
            "Epoch: 27975 , Loss: 0.0003502107865642756\n",
            "Epoch: 27976 , Loss: 0.000350629590684548\n",
            "Epoch: 27977 , Loss: 0.00035061780363321304\n",
            "Epoch: 27978 , Loss: 0.00035065505653619766\n",
            "Epoch: 27979 , Loss: 0.0003509631205815822\n",
            "Epoch: 27980 , Loss: 0.00035019268398173153\n",
            "Epoch: 27981 , Loss: 0.0003504018532112241\n",
            "Epoch: 27982 , Loss: 0.00034875297569669783\n",
            "Epoch: 27983 , Loss: 0.0003475185949355364\n",
            "Epoch: 27984 , Loss: 0.00034915358992293477\n",
            "Epoch: 27985 , Loss: 0.0003499791491776705\n",
            "Epoch: 27986 , Loss: 0.0003516209835652262\n",
            "Epoch: 27987 , Loss: 0.00034970961860381067\n",
            "Epoch: 27988 , Loss: 0.00035004012170247734\n",
            "Epoch: 27989 , Loss: 0.0003502751642372459\n",
            "Epoch: 27990 , Loss: 0.00035181158455088735\n",
            "Epoch: 27991 , Loss: 0.0003517832374200225\n",
            "Epoch: 27992 , Loss: 0.0003493200056254864\n",
            "Epoch: 27993 , Loss: 0.0003497005091048777\n",
            "Epoch: 27994 , Loss: 0.0003495049895718694\n",
            "Epoch: 27995 , Loss: 0.00034811964724212885\n",
            "Epoch: 27996 , Loss: 0.00034857168793678284\n",
            "Epoch: 27997 , Loss: 0.0003510897804517299\n",
            "Epoch: 27998 , Loss: 0.00034900964237749577\n",
            "Epoch: 27999 , Loss: 0.0003508772642817348\n",
            "Epoch: 28000 , Loss: 0.00034990604035556316\n",
            "=============================================\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "13 correctly classified among 100\n",
            "Accuracy as of 28000 epochs: 13.0\n",
            "=============================================\n",
            "Epoch: 28001 , Loss: 0.0003491883398965001\n",
            "Epoch: 28002 , Loss: 0.00034817808773368597\n",
            "Epoch: 28003 , Loss: 0.0003490039089228958\n",
            "Epoch: 28004 , Loss: 0.00034988042898476124\n",
            "Epoch: 28005 , Loss: 0.0003486925270408392\n",
            "Epoch: 28006 , Loss: 0.00034819438587874174\n",
            "Epoch: 28007 , Loss: 0.0003484305052552372\n",
            "Epoch: 28008 , Loss: 0.00034860247978940606\n",
            "Epoch: 28009 , Loss: 0.00034928746754303575\n",
            "Epoch: 28010 , Loss: 0.0003498565056361258\n",
            "Epoch: 28011 , Loss: 0.0003492423566058278\n",
            "Epoch: 28012 , Loss: 0.00034875795245170593\n",
            "Epoch: 28013 , Loss: 0.0003487427020445466\n",
            "Epoch: 28014 , Loss: 0.00034811143996194005\n",
            "Epoch: 28015 , Loss: 0.00034811795921996236\n",
            "Epoch: 28016 , Loss: 0.00034793614759109914\n",
            "Epoch: 28017 , Loss: 0.000346939021255821\n",
            "Epoch: 28018 , Loss: 0.0003481035528238863\n",
            "Epoch: 28019 , Loss: 0.0003485803899820894\n",
            "Epoch: 28020 , Loss: 0.00034702595439739525\n",
            "Epoch: 28021 , Loss: 0.0003480615559965372\n",
            "Epoch: 28022 , Loss: 0.00034750651684589684\n",
            "Epoch: 28023 , Loss: 0.0003481519815977663\n",
            "Epoch: 28024 , Loss: 0.00034724012948572636\n",
            "Epoch: 28025 , Loss: 0.00034836039412766695\n",
            "Epoch: 28026 , Loss: 0.00034880865132436156\n",
            "Epoch: 28027 , Loss: 0.00034776204847730696\n",
            "Epoch: 28028 , Loss: 0.0003468471113592386\n",
            "Epoch: 28029 , Loss: 0.0003472121898084879\n",
            "Epoch: 28030 , Loss: 0.0003477586433291435\n",
            "Epoch: 28031 , Loss: 0.00034678136580623686\n",
            "Epoch: 28032 , Loss: 0.00034669210435822606\n",
            "Epoch: 28033 , Loss: 0.0003475337289273739\n",
            "Epoch: 28034 , Loss: 0.00034717906964942813\n",
            "Epoch: 28035 , Loss: 0.00034661608515307307\n",
            "Epoch: 28036 , Loss: 0.00034726792364381254\n",
            "Epoch: 28037 , Loss: 0.0003466082562226802\n",
            "Epoch: 28038 , Loss: 0.000346998538589105\n",
            "Epoch: 28039 , Loss: 0.00034558260813355446\n",
            "Epoch: 28040 , Loss: 0.000345932028722018\n",
            "Epoch: 28041 , Loss: 0.00034540414344519377\n",
            "Epoch: 28042 , Loss: 0.00034485553624108434\n",
            "Epoch: 28043 , Loss: 0.0003474573022685945\n",
            "Epoch: 28044 , Loss: 0.00034596919431351125\n",
            "Epoch: 28045 , Loss: 0.00034358585253357887\n",
            "Epoch: 28046 , Loss: 0.00034558598417788744\n",
            "Epoch: 28047 , Loss: 0.00034956634044647217\n",
            "Epoch: 28048 , Loss: 0.0003481943567749113\n",
            "Epoch: 28049 , Loss: 0.00034739269176498055\n",
            "Epoch: 28050 , Loss: 0.0003488146758172661\n",
            "Epoch: 28051 , Loss: 0.00034311722265556455\n",
            "Epoch: 28052 , Loss: 0.0003437620180193335\n",
            "Epoch: 28053 , Loss: 0.00034537832834757864\n",
            "Epoch: 28054 , Loss: 0.00034343727747909725\n",
            "Epoch: 28055 , Loss: 0.00034395113470964134\n",
            "Epoch: 28056 , Loss: 0.00034504395443946123\n",
            "Epoch: 28057 , Loss: 0.0003431153018027544\n",
            "Epoch: 28058 , Loss: 0.00034652615431696177\n",
            "Epoch: 28059 , Loss: 0.00034573947777971625\n",
            "Epoch: 28060 , Loss: 0.0003468554059509188\n",
            "Epoch: 28061 , Loss: 0.0003462832828518003\n",
            "Epoch: 28062 , Loss: 0.0003456590056885034\n",
            "Epoch: 28063 , Loss: 0.000344673142535612\n",
            "Epoch: 28064 , Loss: 0.0003440976142883301\n",
            "Epoch: 28065 , Loss: 0.00034328404581174254\n",
            "Epoch: 28066 , Loss: 0.00034475509892217815\n",
            "Epoch: 28067 , Loss: 0.00034218482323922217\n",
            "Epoch: 28068 , Loss: 0.00034403213066980243\n",
            "Epoch: 28069 , Loss: 0.0003453887184150517\n",
            "Epoch: 28070 , Loss: 0.0003459508006926626\n",
            "Epoch: 28071 , Loss: 0.0003453716926742345\n",
            "Epoch: 28072 , Loss: 0.000345163163729012\n",
            "Epoch: 28073 , Loss: 0.00034446577774360776\n",
            "Epoch: 28074 , Loss: 0.0003432267112657428\n",
            "Epoch: 28075 , Loss: 0.00034303852589800954\n",
            "Epoch: 28076 , Loss: 0.0003422285954002291\n",
            "Epoch: 28077 , Loss: 0.0003439157153479755\n",
            "Epoch: 28078 , Loss: 0.0003430883807595819\n",
            "Epoch: 28079 , Loss: 0.0003437838167883456\n",
            "Epoch: 28080 , Loss: 0.0003457958810031414\n",
            "Epoch: 28081 , Loss: 0.0003426307230256498\n",
            "Epoch: 28082 , Loss: 0.0003444142348598689\n",
            "Epoch: 28083 , Loss: 0.00034329810296185315\n",
            "Epoch: 28084 , Loss: 0.0003434050886426121\n",
            "Epoch: 28085 , Loss: 0.00034257484367117286\n",
            "Epoch: 28086 , Loss: 0.0003431929799262434\n",
            "Epoch: 28087 , Loss: 0.00034446403151378036\n",
            "Epoch: 28088 , Loss: 0.0003431947552599013\n",
            "Epoch: 28089 , Loss: 0.00034239282831549644\n",
            "Epoch: 28090 , Loss: 0.00034303084248676896\n",
            "Epoch: 28091 , Loss: 0.00034352706279605627\n",
            "Epoch: 28092 , Loss: 0.0003434033424127847\n",
            "Epoch: 28093 , Loss: 0.0003426289767958224\n",
            "Epoch: 28094 , Loss: 0.0003426203038543463\n",
            "Epoch: 28095 , Loss: 0.0003416833933442831\n",
            "Epoch: 28096 , Loss: 0.000343533989507705\n",
            "Epoch: 28097 , Loss: 0.00034189579309895635\n",
            "Epoch: 28098 , Loss: 0.000341448379913345\n",
            "Epoch: 28099 , Loss: 0.00034259053063578904\n",
            "Epoch: 28100 , Loss: 0.00034097072784788907\n",
            "Epoch: 28101 , Loss: 0.00034707406302914023\n",
            "Epoch: 28102 , Loss: 0.0003424227761570364\n",
            "Epoch: 28103 , Loss: 0.000341907812980935\n",
            "Epoch: 28104 , Loss: 0.0003411932848393917\n",
            "Epoch: 28105 , Loss: 0.00034161185612902045\n",
            "Epoch: 28106 , Loss: 0.0003421782748773694\n",
            "Epoch: 28107 , Loss: 0.00034014490665867925\n",
            "Epoch: 28108 , Loss: 0.0003417234984226525\n",
            "Epoch: 28109 , Loss: 0.00034104095539078116\n",
            "Epoch: 28110 , Loss: 0.0003374838561285287\n",
            "Epoch: 28111 , Loss: 0.00034263619454577565\n",
            "Epoch: 28112 , Loss: 0.0003421609289944172\n",
            "Epoch: 28113 , Loss: 0.0003415117389522493\n",
            "Epoch: 28114 , Loss: 0.0003427668707445264\n",
            "Epoch: 28115 , Loss: 0.0003434114041738212\n",
            "Epoch: 28116 , Loss: 0.00034009001683443785\n",
            "Epoch: 28117 , Loss: 0.00034100282937288284\n",
            "Epoch: 28118 , Loss: 0.00034108295221813023\n",
            "Epoch: 28119 , Loss: 0.00034218491055071354\n",
            "Epoch: 28120 , Loss: 0.0003413035301491618\n",
            "Epoch: 28121 , Loss: 0.00034076874726451933\n",
            "Epoch: 28122 , Loss: 0.0003405235183890909\n",
            "Epoch: 28123 , Loss: 0.00034097384195774794\n",
            "Epoch: 28124 , Loss: 0.00034117125323973596\n",
            "Epoch: 28125 , Loss: 0.00034183301613666117\n",
            "Epoch: 28126 , Loss: 0.0003399818670004606\n",
            "Epoch: 28127 , Loss: 0.0003413169179111719\n",
            "Epoch: 28128 , Loss: 0.0003423390444368124\n",
            "Epoch: 28129 , Loss: 0.0003421208821237087\n",
            "Epoch: 28130 , Loss: 0.00034001661697402596\n",
            "Epoch: 28131 , Loss: 0.00033902665018104017\n",
            "Epoch: 28132 , Loss: 0.00033925718162208796\n",
            "Epoch: 28133 , Loss: 0.0003396953106857836\n",
            "Epoch: 28134 , Loss: 0.0003404505841899663\n",
            "Epoch: 28135 , Loss: 0.0003388450713828206\n",
            "Epoch: 28136 , Loss: 0.00033858168171718717\n",
            "Epoch: 28137 , Loss: 0.0003394771192688495\n",
            "Epoch: 28138 , Loss: 0.00034027660149149597\n",
            "Epoch: 28139 , Loss: 0.00033896652166731656\n",
            "Epoch: 28140 , Loss: 0.00033961699227802455\n",
            "Epoch: 28141 , Loss: 0.0003366224991623312\n",
            "Epoch: 28142 , Loss: 0.00033885124139487743\n",
            "Epoch: 28143 , Loss: 0.0003401294816285372\n",
            "Epoch: 28144 , Loss: 0.00034180417424067855\n",
            "Epoch: 28145 , Loss: 0.00034163956297561526\n",
            "Epoch: 28146 , Loss: 0.0003380672715138644\n",
            "Epoch: 28147 , Loss: 0.00033872874337248504\n",
            "Epoch: 28148 , Loss: 0.00033924629678949714\n",
            "Epoch: 28149 , Loss: 0.00033934172824956477\n",
            "Epoch: 28150 , Loss: 0.000339241698384285\n",
            "Epoch: 28151 , Loss: 0.0003383394214324653\n",
            "Epoch: 28152 , Loss: 0.00033926754258573055\n",
            "Epoch: 28153 , Loss: 0.0003391440259292722\n",
            "Epoch: 28154 , Loss: 0.00033975995029322803\n",
            "Epoch: 28155 , Loss: 0.00033841177355498075\n",
            "Epoch: 28156 , Loss: 0.0003391401842236519\n",
            "Epoch: 28157 , Loss: 0.0003397069813217968\n",
            "Epoch: 28158 , Loss: 0.00033942016307264566\n",
            "Epoch: 28159 , Loss: 0.0003365954617038369\n",
            "Epoch: 28160 , Loss: 0.00033828814048320055\n",
            "Epoch: 28161 , Loss: 0.00033870048355311155\n",
            "Epoch: 28162 , Loss: 0.0003378379042260349\n",
            "Epoch: 28163 , Loss: 0.0003380562411621213\n",
            "Epoch: 28164 , Loss: 0.0003384239098522812\n",
            "Epoch: 28165 , Loss: 0.00033699863706715405\n",
            "Epoch: 28166 , Loss: 0.0003372355131432414\n",
            "Epoch: 28167 , Loss: 0.0003391218197066337\n",
            "Epoch: 28168 , Loss: 0.000338127079885453\n",
            "Epoch: 28169 , Loss: 0.0003373118524905294\n",
            "Epoch: 28170 , Loss: 0.00033726400579325855\n",
            "Epoch: 28171 , Loss: 0.00033782701939344406\n",
            "Epoch: 28172 , Loss: 0.00033709092531353235\n",
            "Epoch: 28173 , Loss: 0.00033779931254684925\n",
            "Epoch: 28174 , Loss: 0.000337695877533406\n",
            "Epoch: 28175 , Loss: 0.0003373383660800755\n",
            "Epoch: 28176 , Loss: 0.00033687494578771293\n",
            "Epoch: 28177 , Loss: 0.0003385637828614563\n",
            "Epoch: 28178 , Loss: 0.00033944984897971153\n",
            "Epoch: 28179 , Loss: 0.00033972907112911344\n",
            "Epoch: 28180 , Loss: 0.0003356887318659574\n",
            "Epoch: 28181 , Loss: 0.0003378319088369608\n",
            "Epoch: 28182 , Loss: 0.000336428260197863\n",
            "Epoch: 28183 , Loss: 0.0003364181029610336\n",
            "Epoch: 28184 , Loss: 0.00033621900365687907\n",
            "Epoch: 28185 , Loss: 0.0003363644646015018\n",
            "Epoch: 28186 , Loss: 0.000335787539370358\n",
            "Epoch: 28187 , Loss: 0.0003382485592737794\n",
            "Epoch: 28188 , Loss: 0.00033514489769004285\n",
            "Epoch: 28189 , Loss: 0.00033684985828585923\n",
            "Epoch: 28190 , Loss: 0.00033702136715874076\n",
            "Epoch: 28191 , Loss: 0.00033464349689893425\n",
            "Epoch: 28192 , Loss: 0.00033374299528077245\n",
            "Epoch: 28193 , Loss: 0.00033863051794469357\n",
            "Epoch: 28194 , Loss: 0.00032904057297855616\n",
            "Epoch: 28195 , Loss: 0.0003343668067827821\n",
            "Epoch: 28196 , Loss: 0.0003359091351740062\n",
            "Epoch: 28197 , Loss: 0.0003331765183247626\n",
            "Epoch: 28198 , Loss: 0.00033406782313250005\n",
            "Epoch: 28199 , Loss: 0.0003371313796378672\n",
            "Epoch: 28200 , Loss: 0.0003396947868168354\n",
            "Epoch: 28201 , Loss: 0.00033965223701670766\n",
            "Epoch: 28202 , Loss: 0.0003377583925612271\n",
            "Epoch: 28203 , Loss: 0.0003331685729790479\n",
            "Epoch: 28204 , Loss: 0.00033970619551837444\n",
            "Epoch: 28205 , Loss: 0.00033466637250967324\n",
            "Epoch: 28206 , Loss: 0.0003369721816852689\n",
            "Epoch: 28207 , Loss: 0.0003370921185705811\n",
            "Epoch: 28208 , Loss: 0.0003352063649799675\n",
            "Epoch: 28209 , Loss: 0.00033495656680315733\n",
            "Epoch: 28210 , Loss: 0.00033488543704152107\n",
            "Epoch: 28211 , Loss: 0.00033447827445343137\n",
            "Epoch: 28212 , Loss: 0.0003354583459440619\n",
            "Epoch: 28213 , Loss: 0.0003343780990689993\n",
            "Epoch: 28214 , Loss: 0.0003359148104209453\n",
            "Epoch: 28215 , Loss: 0.00033448904287070036\n",
            "Epoch: 28216 , Loss: 0.0003345039440318942\n",
            "Epoch: 28217 , Loss: 0.0003339226823300123\n",
            "Epoch: 28218 , Loss: 0.00033749424619600177\n",
            "Epoch: 28219 , Loss: 0.0003337858943268657\n",
            "Epoch: 28220 , Loss: 0.00033266597893089056\n",
            "Epoch: 28221 , Loss: 0.00033542350865900517\n",
            "Epoch: 28222 , Loss: 0.0003355911758262664\n",
            "Epoch: 28223 , Loss: 0.000334633223246783\n",
            "Epoch: 28224 , Loss: 0.00033438665559515357\n",
            "Epoch: 28225 , Loss: 0.00033433971111662686\n",
            "Epoch: 28226 , Loss: 0.00033343135146424174\n",
            "Epoch: 28227 , Loss: 0.0003335105429869145\n",
            "Epoch: 28228 , Loss: 0.0003345443692523986\n",
            "Epoch: 28229 , Loss: 0.0003345921286381781\n",
            "Epoch: 28230 , Loss: 0.00033339104265905917\n",
            "Epoch: 28231 , Loss: 0.00033440260449424386\n",
            "Epoch: 28232 , Loss: 0.0003326087608002126\n",
            "Epoch: 28233 , Loss: 0.00033171410905197263\n",
            "Epoch: 28234 , Loss: 0.0003340950352139771\n",
            "Epoch: 28235 , Loss: 0.00033298786729574203\n",
            "Epoch: 28236 , Loss: 0.00033545386395417154\n",
            "Epoch: 28237 , Loss: 0.0003329063765704632\n",
            "Epoch: 28238 , Loss: 0.00033228410757146776\n",
            "Epoch: 28239 , Loss: 0.0003346508601680398\n",
            "Epoch: 28240 , Loss: 0.0003340868279337883\n",
            "Epoch: 28241 , Loss: 0.0003340753319207579\n",
            "Epoch: 28242 , Loss: 0.0003309549647383392\n",
            "Epoch: 28243 , Loss: 0.0003330152540002018\n",
            "Epoch: 28244 , Loss: 0.00033249473199248314\n",
            "Epoch: 28245 , Loss: 0.0003329307655803859\n",
            "Epoch: 28246 , Loss: 0.0003331216867081821\n",
            "Epoch: 28247 , Loss: 0.00033245148370042443\n",
            "Epoch: 28248 , Loss: 0.00033265622914768755\n",
            "Epoch: 28249 , Loss: 0.0003325568395666778\n",
            "Epoch: 28250 , Loss: 0.00033274615998379886\n",
            "=============================================\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 45ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "6 correctly classified among 100\n",
            "Accuracy as of 28250 epochs: 6.0\n",
            "=============================================\n",
            "Epoch: 28251 , Loss: 0.00033211763366125524\n",
            "Epoch: 28252 , Loss: 0.0003327862941659987\n",
            "Epoch: 28253 , Loss: 0.0003311638720333576\n",
            "Epoch: 28254 , Loss: 0.00033139187144115567\n",
            "Epoch: 28255 , Loss: 0.00033229912514798343\n",
            "Epoch: 28256 , Loss: 0.00033170494134537876\n",
            "Epoch: 28257 , Loss: 0.00033202776103280485\n",
            "Epoch: 28258 , Loss: 0.0003317098307888955\n",
            "Epoch: 28259 , Loss: 0.00033128244103863835\n",
            "Epoch: 28260 , Loss: 0.00033326519769616425\n",
            "Epoch: 28261 , Loss: 0.00033491573412902653\n",
            "Epoch: 28262 , Loss: 0.00033123293542303145\n",
            "Epoch: 28263 , Loss: 0.000329427479300648\n",
            "Epoch: 28264 , Loss: 0.000330346985720098\n",
            "Epoch: 28265 , Loss: 0.00032844702946022153\n",
            "Epoch: 28266 , Loss: 0.00033204181818291545\n",
            "Epoch: 28267 , Loss: 0.0003330387407913804\n",
            "Epoch: 28268 , Loss: 0.0003311566251795739\n",
            "Epoch: 28269 , Loss: 0.0003299445379525423\n",
            "Epoch: 28270 , Loss: 0.0003294668858870864\n",
            "Epoch: 28271 , Loss: 0.0003309480962343514\n",
            "Epoch: 28272 , Loss: 0.0003315029898658395\n",
            "Epoch: 28273 , Loss: 0.00033576914574950933\n",
            "Epoch: 28274 , Loss: 0.0003323561104480177\n",
            "Epoch: 28275 , Loss: 0.00033079698914662004\n",
            "Epoch: 28276 , Loss: 0.0003321858821436763\n",
            "Epoch: 28277 , Loss: 0.00033127848291769624\n",
            "Epoch: 28278 , Loss: 0.00033041980350390077\n",
            "Epoch: 28279 , Loss: 0.0003315652720630169\n",
            "Epoch: 28280 , Loss: 0.00033096858533099294\n",
            "Epoch: 28281 , Loss: 0.0003284845734015107\n",
            "Epoch: 28282 , Loss: 0.00032839496270753443\n",
            "Epoch: 28283 , Loss: 0.000332028343109414\n",
            "Epoch: 28284 , Loss: 0.0003294821653980762\n",
            "Epoch: 28285 , Loss: 0.00033136774436570704\n",
            "Epoch: 28286 , Loss: 0.00032991485204547644\n",
            "Epoch: 28287 , Loss: 0.00032946013379842043\n",
            "Epoch: 28288 , Loss: 0.00033080880530178547\n",
            "Epoch: 28289 , Loss: 0.0003298769879620522\n",
            "Epoch: 28290 , Loss: 0.000329107278957963\n",
            "Epoch: 28291 , Loss: 0.00032826498500071466\n",
            "Epoch: 28292 , Loss: 0.00032851705327630043\n",
            "Epoch: 28293 , Loss: 0.00032965431455522776\n",
            "Epoch: 28294 , Loss: 0.00033050726051442325\n",
            "Epoch: 28295 , Loss: 0.0003294306807219982\n",
            "Epoch: 28296 , Loss: 0.0003301162214484066\n",
            "Epoch: 28297 , Loss: 0.00032911583548411727\n",
            "Epoch: 28298 , Loss: 0.0003279646916780621\n",
            "Epoch: 28299 , Loss: 0.00032889959402382374\n",
            "Epoch: 28300 , Loss: 0.0003285198472440243\n",
            "Epoch: 28301 , Loss: 0.00032911100424826145\n",
            "Epoch: 28302 , Loss: 0.00032785741495899856\n",
            "Epoch: 28303 , Loss: 0.0003280123637523502\n",
            "Epoch: 28304 , Loss: 0.0003288056468591094\n",
            "Epoch: 28305 , Loss: 0.00032848844421096146\n",
            "Epoch: 28306 , Loss: 0.0003286817518528551\n",
            "Epoch: 28307 , Loss: 0.00032717801514081657\n",
            "Epoch: 28308 , Loss: 0.0003287188010290265\n",
            "Epoch: 28309 , Loss: 0.00032773424754850566\n",
            "Epoch: 28310 , Loss: 0.0003281152748968452\n",
            "Epoch: 28311 , Loss: 0.00032818480394780636\n",
            "Epoch: 28312 , Loss: 0.0003297646762803197\n",
            "Epoch: 28313 , Loss: 0.0003289350715931505\n",
            "Epoch: 28314 , Loss: 0.0003285569546278566\n",
            "Epoch: 28315 , Loss: 0.0003296109789516777\n",
            "Epoch: 28316 , Loss: 0.0003279473748989403\n",
            "Epoch: 28317 , Loss: 0.0003271924506407231\n",
            "Epoch: 28318 , Loss: 0.0003283185069449246\n",
            "Epoch: 28319 , Loss: 0.0003272891044616699\n",
            "Epoch: 28320 , Loss: 0.00032714338158257306\n",
            "Epoch: 28321 , Loss: 0.00032883629319258034\n",
            "Epoch: 28322 , Loss: 0.00033016636734828353\n",
            "Epoch: 28323 , Loss: 0.0003259375225752592\n",
            "Epoch: 28324 , Loss: 0.0003270124434493482\n",
            "Epoch: 28325 , Loss: 0.00032712722895666957\n",
            "Epoch: 28326 , Loss: 0.00032624206505715847\n",
            "Epoch: 28327 , Loss: 0.00032873390591703355\n",
            "Epoch: 28328 , Loss: 0.00033229708787985146\n",
            "Epoch: 28329 , Loss: 0.00032751489197835326\n",
            "Epoch: 28330 , Loss: 0.00032708904473111033\n",
            "Epoch: 28331 , Loss: 0.0003269942244514823\n",
            "Epoch: 28332 , Loss: 0.00032597972312942147\n",
            "Epoch: 28333 , Loss: 0.00032678557909093797\n",
            "Epoch: 28334 , Loss: 0.0003261706733610481\n",
            "Epoch: 28335 , Loss: 0.0003271130262874067\n",
            "Epoch: 28336 , Loss: 0.00032626191386952996\n",
            "Epoch: 28337 , Loss: 0.00032619957346469164\n",
            "Epoch: 28338 , Loss: 0.0003262243408244103\n",
            "Epoch: 28339 , Loss: 0.0003261701203882694\n",
            "Epoch: 28340 , Loss: 0.0003260390949435532\n",
            "Epoch: 28341 , Loss: 0.00032600940903648734\n",
            "Epoch: 28342 , Loss: 0.00032583947177045047\n",
            "Epoch: 28343 , Loss: 0.000326231267536059\n",
            "Epoch: 28344 , Loss: 0.00032680860022082925\n",
            "Epoch: 28345 , Loss: 0.0003250639420002699\n",
            "Epoch: 28346 , Loss: 0.00032621121499687433\n",
            "Epoch: 28347 , Loss: 0.0003248959837947041\n",
            "Epoch: 28348 , Loss: 0.00032504709088243544\n",
            "Epoch: 28349 , Loss: 0.0003282676334492862\n",
            "Epoch: 28350 , Loss: 0.0003244056133553386\n",
            "Epoch: 28351 , Loss: 0.0003235297917854041\n",
            "Epoch: 28352 , Loss: 0.00032706180354580283\n",
            "Epoch: 28353 , Loss: 0.00032528169685974717\n",
            "Epoch: 28354 , Loss: 0.00032577512320131063\n",
            "Epoch: 28355 , Loss: 0.0003257164207752794\n",
            "Epoch: 28356 , Loss: 0.00032128652674145997\n",
            "Epoch: 28357 , Loss: 0.00031947955721989274\n",
            "Epoch: 28358 , Loss: 0.000317968922900036\n",
            "Epoch: 28359 , Loss: 0.00032627853215672076\n",
            "Epoch: 28360 , Loss: 0.00033012276981025934\n",
            "Epoch: 28361 , Loss: 0.00034553505247458816\n",
            "Epoch: 28362 , Loss: 0.00032608170295134187\n",
            "Epoch: 28363 , Loss: 0.00032210216159000993\n",
            "Epoch: 28364 , Loss: 0.00032497092615813017\n",
            "Epoch: 28365 , Loss: 0.00032443055533804\n",
            "Epoch: 28366 , Loss: 0.00032568300957791507\n",
            "Epoch: 28367 , Loss: 0.000324178981827572\n",
            "Epoch: 28368 , Loss: 0.00032958819065243006\n",
            "Epoch: 28369 , Loss: 0.0003211782895959914\n",
            "Epoch: 28370 , Loss: 0.00032898859353736043\n",
            "Epoch: 28371 , Loss: 0.00032735191052779555\n",
            "Epoch: 28372 , Loss: 0.00032463716343045235\n",
            "Epoch: 28373 , Loss: 0.0003240936202928424\n",
            "Epoch: 28374 , Loss: 0.0003240097430534661\n",
            "Epoch: 28375 , Loss: 0.0003235860785935074\n",
            "Epoch: 28376 , Loss: 0.00032639800338074565\n",
            "Epoch: 28377 , Loss: 0.00032434501918032765\n",
            "Epoch: 28378 , Loss: 0.0003234850009903312\n",
            "Epoch: 28379 , Loss: 0.00032360103796236217\n",
            "Epoch: 28380 , Loss: 0.00032633994123898447\n",
            "Epoch: 28381 , Loss: 0.0003240391961298883\n",
            "Epoch: 28382 , Loss: 0.00032243519672192633\n",
            "Epoch: 28383 , Loss: 0.0003227013221476227\n",
            "Epoch: 28384 , Loss: 0.00032438410562463105\n",
            "Epoch: 28385 , Loss: 0.0003242694365326315\n",
            "Epoch: 28386 , Loss: 0.0003231782466173172\n",
            "Epoch: 28387 , Loss: 0.00032387999817728996\n",
            "Epoch: 28388 , Loss: 0.00032546656439080834\n",
            "Epoch: 28389 , Loss: 0.00032284323242492974\n",
            "Epoch: 28390 , Loss: 0.00032282271422445774\n",
            "Epoch: 28391 , Loss: 0.0003219752397853881\n",
            "Epoch: 28392 , Loss: 0.00032134403591044247\n",
            "Epoch: 28393 , Loss: 0.0003242050006520003\n",
            "Epoch: 28394 , Loss: 0.00032286412897519767\n",
            "Epoch: 28395 , Loss: 0.00032240699511021376\n",
            "Epoch: 28396 , Loss: 0.00032311020186170936\n",
            "Epoch: 28397 , Loss: 0.0003231750742997974\n",
            "Epoch: 28398 , Loss: 0.0003229968133382499\n",
            "Epoch: 28399 , Loss: 0.0003222413652110845\n",
            "Epoch: 28400 , Loss: 0.00032233347883448005\n",
            "Epoch: 28401 , Loss: 0.00032321002800017595\n",
            "Epoch: 28402 , Loss: 0.0003230592410545796\n",
            "Epoch: 28403 , Loss: 0.0003214280877728015\n",
            "Epoch: 28404 , Loss: 0.0003230172151234001\n",
            "Epoch: 28405 , Loss: 0.00032200044370256364\n",
            "Epoch: 28406 , Loss: 0.0003225562977604568\n",
            "Epoch: 28407 , Loss: 0.0003221656079404056\n",
            "Epoch: 28408 , Loss: 0.00032050031586550176\n",
            "Epoch: 28409 , Loss: 0.0003219161299057305\n",
            "Epoch: 28410 , Loss: 0.0003215311444364488\n",
            "Epoch: 28411 , Loss: 0.00032165838638320565\n",
            "Epoch: 28412 , Loss: 0.0003214751777704805\n",
            "Epoch: 28413 , Loss: 0.00032097435905598104\n",
            "Epoch: 28414 , Loss: 0.0003213576565030962\n",
            "Epoch: 28415 , Loss: 0.0003214026801288128\n",
            "Epoch: 28416 , Loss: 0.0003224681131541729\n",
            "Epoch: 28417 , Loss: 0.00032102048862725496\n",
            "Epoch: 28418 , Loss: 0.00032077328069135547\n",
            "Epoch: 28419 , Loss: 0.000320978433592245\n",
            "Epoch: 28420 , Loss: 0.00032058177748695016\n",
            "Epoch: 28421 , Loss: 0.00032268743962049484\n",
            "Epoch: 28422 , Loss: 0.0003196208563167602\n",
            "Epoch: 28423 , Loss: 0.000320195424137637\n",
            "Epoch: 28424 , Loss: 0.000320660590659827\n",
            "Epoch: 28425 , Loss: 0.00032028465648181736\n",
            "Epoch: 28426 , Loss: 0.0003197055484633893\n",
            "Epoch: 28427 , Loss: 0.0003183114458806813\n",
            "Epoch: 28428 , Loss: 0.0003188867121934891\n",
            "Epoch: 28429 , Loss: 0.00032015025499276817\n",
            "Epoch: 28430 , Loss: 0.00031727884197607636\n",
            "Epoch: 28431 , Loss: 0.00032060593366622925\n",
            "Epoch: 28432 , Loss: 0.00032028311397880316\n",
            "Epoch: 28433 , Loss: 0.0003178996848873794\n",
            "Epoch: 28434 , Loss: 0.0003115436411462724\n",
            "Epoch: 28435 , Loss: 0.0003215994802303612\n",
            "Epoch: 28436 , Loss: 0.00031700325780548155\n",
            "Epoch: 28437 , Loss: 0.0003232462040614337\n",
            "Epoch: 28438 , Loss: 0.0003165861708112061\n",
            "Epoch: 28439 , Loss: 0.0003234111063648015\n",
            "Epoch: 28440 , Loss: 0.0003227614797651768\n",
            "Epoch: 28441 , Loss: 0.00032015988836064935\n",
            "Epoch: 28442 , Loss: 0.00031780058634467423\n",
            "Epoch: 28443 , Loss: 0.0003206570982001722\n",
            "Epoch: 28444 , Loss: 0.00032028171699494123\n",
            "Epoch: 28445 , Loss: 0.000319668062729761\n",
            "Epoch: 28446 , Loss: 0.00031964853405952454\n",
            "Epoch: 28447 , Loss: 0.00031960313208401203\n",
            "Epoch: 28448 , Loss: 0.0003189270501025021\n",
            "Epoch: 28449 , Loss: 0.00031964320805855095\n",
            "Epoch: 28450 , Loss: 0.00031976652098819613\n",
            "Epoch: 28451 , Loss: 0.0003190939605701715\n",
            "Epoch: 28452 , Loss: 0.00031943805515766144\n",
            "Epoch: 28453 , Loss: 0.00031835000845603645\n",
            "Epoch: 28454 , Loss: 0.0003205258690286428\n",
            "Epoch: 28455 , Loss: 0.0003181104548275471\n",
            "Epoch: 28456 , Loss: 0.00031940717599354684\n",
            "Epoch: 28457 , Loss: 0.00032041023951023817\n",
            "Epoch: 28458 , Loss: 0.0003180842031724751\n",
            "Epoch: 28459 , Loss: 0.000318989681545645\n",
            "Epoch: 28460 , Loss: 0.0003186918329447508\n",
            "Epoch: 28461 , Loss: 0.0003183713706675917\n",
            "Epoch: 28462 , Loss: 0.0003195347380824387\n",
            "Epoch: 28463 , Loss: 0.0003181904321536422\n",
            "Epoch: 28464 , Loss: 0.0003179614432156086\n",
            "Epoch: 28465 , Loss: 0.0003187301626894623\n",
            "Epoch: 28466 , Loss: 0.00031902629416435957\n",
            "Epoch: 28467 , Loss: 0.0003183001826982945\n",
            "Epoch: 28468 , Loss: 0.00031644533737562597\n",
            "Epoch: 28469 , Loss: 0.0003179243649356067\n",
            "Epoch: 28470 , Loss: 0.0003192582225892693\n",
            "Epoch: 28471 , Loss: 0.00031825833139009774\n",
            "Epoch: 28472 , Loss: 0.0003191525174770504\n",
            "Epoch: 28473 , Loss: 0.00031758780824020505\n",
            "Epoch: 28474 , Loss: 0.00031720654806122184\n",
            "Epoch: 28475 , Loss: 0.0003175224410369992\n",
            "Epoch: 28476 , Loss: 0.0003174258454237133\n",
            "Epoch: 28477 , Loss: 0.0003171892021782696\n",
            "Epoch: 28478 , Loss: 0.0003194518794771284\n",
            "Epoch: 28479 , Loss: 0.0003172647557221353\n",
            "Epoch: 28480 , Loss: 0.0003175302699673921\n",
            "Epoch: 28481 , Loss: 0.000316990161081776\n",
            "Epoch: 28482 , Loss: 0.00031704711727797985\n",
            "Epoch: 28483 , Loss: 0.00031695637153461576\n",
            "Epoch: 28484 , Loss: 0.0003159038024023175\n",
            "Epoch: 28485 , Loss: 0.0003167306713294238\n",
            "Epoch: 28486 , Loss: 0.0003176481695845723\n",
            "Epoch: 28487 , Loss: 0.0003185206151101738\n",
            "Epoch: 28488 , Loss: 0.00031660671811550856\n",
            "Epoch: 28489 , Loss: 0.00031673989724367857\n",
            "Epoch: 28490 , Loss: 0.00031707881134934723\n",
            "Epoch: 28491 , Loss: 0.0003164499066770077\n",
            "Epoch: 28492 , Loss: 0.00031661693355999887\n",
            "Epoch: 28493 , Loss: 0.00031623675022274256\n",
            "Epoch: 28494 , Loss: 0.0003157848841510713\n",
            "Epoch: 28495 , Loss: 0.0003142656642012298\n",
            "Epoch: 28496 , Loss: 0.0003165231319144368\n",
            "Epoch: 28497 , Loss: 0.0003161886124871671\n",
            "Epoch: 28498 , Loss: 0.0003157320898026228\n",
            "Epoch: 28499 , Loss: 0.0003172191500198096\n",
            "Epoch: 28500 , Loss: 0.00031539853080175817\n",
            "=============================================\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "40 correctly classified among 100\n",
            "Accuracy as of 28500 epochs: 40.0\n",
            "=============================================\n",
            "Epoch: 28501 , Loss: 0.0003151076380163431\n",
            "Epoch: 28502 , Loss: 0.0003172185970470309\n",
            "Epoch: 28503 , Loss: 0.000316876161377877\n",
            "Epoch: 28504 , Loss: 0.00031601267983205616\n",
            "Epoch: 28505 , Loss: 0.0003146221279166639\n",
            "Epoch: 28506 , Loss: 0.0003150369448121637\n",
            "Epoch: 28507 , Loss: 0.0003147858951706439\n",
            "Epoch: 28508 , Loss: 0.00031427553039975464\n",
            "Epoch: 28509 , Loss: 0.00031588150886818767\n",
            "Epoch: 28510 , Loss: 0.00031517757452093065\n",
            "Epoch: 28511 , Loss: 0.0003145950031466782\n",
            "Epoch: 28512 , Loss: 0.0003142058558296412\n",
            "Epoch: 28513 , Loss: 0.00031517379102297127\n",
            "Epoch: 28514 , Loss: 0.00031574422609992325\n",
            "Epoch: 28515 , Loss: 0.0003137128078378737\n",
            "Epoch: 28516 , Loss: 0.000314462638925761\n",
            "Epoch: 28517 , Loss: 0.0003148133691865951\n",
            "Epoch: 28518 , Loss: 0.0003138218307867646\n",
            "Epoch: 28519 , Loss: 0.0003140723565593362\n",
            "Epoch: 28520 , Loss: 0.000314967124722898\n",
            "Epoch: 28521 , Loss: 0.0003139929613098502\n",
            "Epoch: 28522 , Loss: 0.00031353143276646733\n",
            "Epoch: 28523 , Loss: 0.00031399354338645935\n",
            "Epoch: 28524 , Loss: 0.00031212149769999087\n",
            "Epoch: 28525 , Loss: 0.0003131068078801036\n",
            "Epoch: 28526 , Loss: 0.0003166664391756058\n",
            "Epoch: 28527 , Loss: 0.00031374202808365226\n",
            "Epoch: 28528 , Loss: 0.0003143876092508435\n",
            "Epoch: 28529 , Loss: 0.00031186232808977365\n",
            "Epoch: 28530 , Loss: 0.0003130424302071333\n",
            "Epoch: 28531 , Loss: 0.00031613686587661505\n",
            "Epoch: 28532 , Loss: 0.0003147467796225101\n",
            "Epoch: 28533 , Loss: 0.0003154985897708684\n",
            "Epoch: 28534 , Loss: 0.00031234294874593616\n",
            "Epoch: 28535 , Loss: 0.0003120907349511981\n",
            "Epoch: 28536 , Loss: 0.00031273599597625434\n",
            "Epoch: 28537 , Loss: 0.00031115167075768113\n",
            "Epoch: 28538 , Loss: 0.00031271210173144937\n",
            "Epoch: 28539 , Loss: 0.00031324365409091115\n",
            "Epoch: 28540 , Loss: 0.00031104241497814655\n",
            "Epoch: 28541 , Loss: 0.00031349205528385937\n",
            "Epoch: 28542 , Loss: 0.0003148587129544467\n",
            "Epoch: 28543 , Loss: 0.00031399144791066647\n",
            "Epoch: 28544 , Loss: 0.00031269778264686465\n",
            "Epoch: 28545 , Loss: 0.0003142189234495163\n",
            "Epoch: 28546 , Loss: 0.00031236105132848024\n",
            "Epoch: 28547 , Loss: 0.00031355334795080125\n",
            "Epoch: 28548 , Loss: 0.00031330398633144796\n",
            "Epoch: 28549 , Loss: 0.0003123648639302701\n",
            "Epoch: 28550 , Loss: 0.0003141248016618192\n",
            "Epoch: 28551 , Loss: 0.00031357535044662654\n",
            "Epoch: 28552 , Loss: 0.0003119421598967165\n",
            "Epoch: 28553 , Loss: 0.0003120821784250438\n",
            "Epoch: 28554 , Loss: 0.0003124068898614496\n",
            "Epoch: 28555 , Loss: 0.00031325407326221466\n",
            "Epoch: 28556 , Loss: 0.0003109477402176708\n",
            "Epoch: 28557 , Loss: 0.0003118357853963971\n",
            "Epoch: 28558 , Loss: 0.00030995378619991243\n",
            "Epoch: 28559 , Loss: 0.0003122936759609729\n",
            "Epoch: 28560 , Loss: 0.0003115861618425697\n",
            "Epoch: 28561 , Loss: 0.0003121196641586721\n",
            "Epoch: 28562 , Loss: 0.00031115938327275217\n",
            "Epoch: 28563 , Loss: 0.0003114589489996433\n",
            "Epoch: 28564 , Loss: 0.000312059186398983\n",
            "Epoch: 28565 , Loss: 0.0003112717531621456\n",
            "Epoch: 28566 , Loss: 0.00031115656020119786\n",
            "Epoch: 28567 , Loss: 0.00031328987097367644\n",
            "Epoch: 28568 , Loss: 0.0003105791984125972\n",
            "Epoch: 28569 , Loss: 0.0003107743978034705\n",
            "Epoch: 28570 , Loss: 0.0003105346404481679\n",
            "Epoch: 28571 , Loss: 0.00031089861295185983\n",
            "Epoch: 28572 , Loss: 0.0003114152350462973\n",
            "Epoch: 28573 , Loss: 0.0003103960771113634\n",
            "Epoch: 28574 , Loss: 0.0003102945629507303\n",
            "Epoch: 28575 , Loss: 0.0003104691277258098\n",
            "Epoch: 28576 , Loss: 0.0003106187214143574\n",
            "Epoch: 28577 , Loss: 0.0003113239072263241\n",
            "Epoch: 28578 , Loss: 0.00031041743932291865\n",
            "Epoch: 28579 , Loss: 0.0003096581785939634\n",
            "Epoch: 28580 , Loss: 0.0003106383664999157\n",
            "Epoch: 28581 , Loss: 0.0003105013747699559\n",
            "Epoch: 28582 , Loss: 0.0003097633016295731\n",
            "Epoch: 28583 , Loss: 0.00030984648037701845\n",
            "Epoch: 28584 , Loss: 0.00031025672797113657\n",
            "Epoch: 28585 , Loss: 0.0003097676089964807\n",
            "Epoch: 28586 , Loss: 0.00030881204293109477\n",
            "Epoch: 28587 , Loss: 0.0003094978746958077\n",
            "Epoch: 28588 , Loss: 0.00030973803950473666\n",
            "Epoch: 28589 , Loss: 0.00031021671020425856\n",
            "Epoch: 28590 , Loss: 0.00030968119972385466\n",
            "Epoch: 28591 , Loss: 0.0003083504852838814\n",
            "Epoch: 28592 , Loss: 0.00030876422533765435\n",
            "Epoch: 28593 , Loss: 0.0003087814839091152\n",
            "Epoch: 28594 , Loss: 0.0003095807624049485\n",
            "Epoch: 28595 , Loss: 0.0003081071481574327\n",
            "Epoch: 28596 , Loss: 0.00030921970028430223\n",
            "Epoch: 28597 , Loss: 0.0003069850499741733\n",
            "Epoch: 28598 , Loss: 0.00031063472852110863\n",
            "Epoch: 28599 , Loss: 0.00031207464053295553\n",
            "Epoch: 28600 , Loss: 0.000308830727590248\n",
            "Epoch: 28601 , Loss: 0.00030659977346658707\n",
            "Epoch: 28602 , Loss: 0.0003088940866291523\n",
            "Epoch: 28603 , Loss: 0.00030892970971763134\n",
            "Epoch: 28604 , Loss: 0.0003086866927333176\n",
            "Epoch: 28605 , Loss: 0.000308881513774395\n",
            "Epoch: 28606 , Loss: 0.0003075391869060695\n",
            "Epoch: 28607 , Loss: 0.00030759823857806623\n",
            "Epoch: 28608 , Loss: 0.0003103093768004328\n",
            "Epoch: 28609 , Loss: 0.0003076335706282407\n",
            "Epoch: 28610 , Loss: 0.00030927264015190303\n",
            "Epoch: 28611 , Loss: 0.0003074194537475705\n",
            "Epoch: 28612 , Loss: 0.0003085628559347242\n",
            "Epoch: 28613 , Loss: 0.0003085532516706735\n",
            "Epoch: 28614 , Loss: 0.00030741136288270354\n",
            "Epoch: 28615 , Loss: 0.0003080981259699911\n",
            "Epoch: 28616 , Loss: 0.0003076367429457605\n",
            "Epoch: 28617 , Loss: 0.000306944886688143\n",
            "Epoch: 28618 , Loss: 0.0003067035577259958\n",
            "Epoch: 28619 , Loss: 0.00030735431937500834\n",
            "Epoch: 28620 , Loss: 0.00030747929122298956\n",
            "Epoch: 28621 , Loss: 0.0003068078076466918\n",
            "Epoch: 28622 , Loss: 0.0003071585379075259\n",
            "Epoch: 28623 , Loss: 0.0003075741697102785\n",
            "Epoch: 28624 , Loss: 0.0003096548898611218\n",
            "Epoch: 28625 , Loss: 0.0003075672429986298\n",
            "Epoch: 28626 , Loss: 0.0003071660757996142\n",
            "Epoch: 28627 , Loss: 0.0003084781055804342\n",
            "Epoch: 28628 , Loss: 0.0003077448345720768\n",
            "Epoch: 28629 , Loss: 0.0003063440090045333\n",
            "Epoch: 28630 , Loss: 0.00030732507002539933\n",
            "Epoch: 28631 , Loss: 0.0003077845904044807\n",
            "Epoch: 28632 , Loss: 0.0003075913991779089\n",
            "Epoch: 28633 , Loss: 0.0003065291966777295\n",
            "Epoch: 28634 , Loss: 0.00030766846612095833\n",
            "Epoch: 28635 , Loss: 0.0003055408305954188\n",
            "Epoch: 28636 , Loss: 0.0003061305033043027\n",
            "Epoch: 28637 , Loss: 0.0003065583296120167\n",
            "Epoch: 28638 , Loss: 0.0003040736191906035\n",
            "Epoch: 28639 , Loss: 0.0003038285067304969\n",
            "Epoch: 28640 , Loss: 0.0003084216732531786\n",
            "Epoch: 28641 , Loss: 0.00030507752671837807\n",
            "Epoch: 28642 , Loss: 0.00030696680187247694\n",
            "Epoch: 28643 , Loss: 0.0003058885922655463\n",
            "Epoch: 28644 , Loss: 0.00030558841535821557\n",
            "Epoch: 28645 , Loss: 0.00030464748851954937\n",
            "Epoch: 28646 , Loss: 0.0003088173398282379\n",
            "Epoch: 28647 , Loss: 0.0003033910470549017\n",
            "Epoch: 28648 , Loss: 0.0003059241862501949\n",
            "Epoch: 28649 , Loss: 0.0003069635131396353\n",
            "Epoch: 28650 , Loss: 0.0003044942859560251\n",
            "Epoch: 28651 , Loss: 0.0003049081424251199\n",
            "Epoch: 28652 , Loss: 0.0003070341481361538\n",
            "Epoch: 28653 , Loss: 0.00030325260013341904\n",
            "Epoch: 28654 , Loss: 0.00030420502298511565\n",
            "Epoch: 28655 , Loss: 0.00030585023341700435\n",
            "Epoch: 28656 , Loss: 0.0003044620680157095\n",
            "Epoch: 28657 , Loss: 0.0003044405020773411\n",
            "Epoch: 28658 , Loss: 0.0003053732216358185\n",
            "Epoch: 28659 , Loss: 0.0003047296777367592\n",
            "Epoch: 28660 , Loss: 0.0003060937160626054\n",
            "Epoch: 28661 , Loss: 0.00030518072890117764\n",
            "Epoch: 28662 , Loss: 0.0003058401634916663\n",
            "Epoch: 28663 , Loss: 0.00030264907400123775\n",
            "Epoch: 28664 , Loss: 0.00030439120018854737\n",
            "Epoch: 28665 , Loss: 0.0003048894286621362\n",
            "Epoch: 28666 , Loss: 0.00030380510725080967\n",
            "Epoch: 28667 , Loss: 0.00030577919096685946\n",
            "Epoch: 28668 , Loss: 0.00030506576877087355\n",
            "Epoch: 28669 , Loss: 0.00030354096088558435\n",
            "Epoch: 28670 , Loss: 0.0003057856811210513\n",
            "Epoch: 28671 , Loss: 0.0003066570789087564\n",
            "Epoch: 28672 , Loss: 0.0003012767410837114\n",
            "Epoch: 28673 , Loss: 0.0003037948627024889\n",
            "Epoch: 28674 , Loss: 0.0003038885479327291\n",
            "Epoch: 28675 , Loss: 0.00030317684286274016\n",
            "Epoch: 28676 , Loss: 0.00030328502180054784\n",
            "Epoch: 28677 , Loss: 0.00030258612241595984\n",
            "Epoch: 28678 , Loss: 0.0003044207696802914\n",
            "Epoch: 28679 , Loss: 0.0003063047188334167\n",
            "Epoch: 28680 , Loss: 0.0003034274559468031\n",
            "Epoch: 28681 , Loss: 0.0003030719526577741\n",
            "Epoch: 28682 , Loss: 0.00030732707818970084\n",
            "Epoch: 28683 , Loss: 0.0003039914299733937\n",
            "Epoch: 28684 , Loss: 0.00030453450744971633\n",
            "Epoch: 28685 , Loss: 0.00030314945615828037\n",
            "Epoch: 28686 , Loss: 0.0003022809396497905\n",
            "Epoch: 28687 , Loss: 0.0003043529577553272\n",
            "Epoch: 28688 , Loss: 0.0003031271044164896\n",
            "Epoch: 28689 , Loss: 0.00030175596475601196\n",
            "Epoch: 28690 , Loss: 0.00030290285940282047\n",
            "Epoch: 28691 , Loss: 0.000301775464322418\n",
            "Epoch: 28692 , Loss: 0.00030247049289755523\n",
            "Epoch: 28693 , Loss: 0.0003036601119674742\n",
            "Epoch: 28694 , Loss: 0.0003051628009416163\n",
            "Epoch: 28695 , Loss: 0.00030378124210983515\n",
            "Epoch: 28696 , Loss: 0.0003040286828763783\n",
            "Epoch: 28697 , Loss: 0.0003021335578523576\n",
            "Epoch: 28698 , Loss: 0.00030258012702688575\n",
            "Epoch: 28699 , Loss: 0.00030081707518547773\n",
            "Epoch: 28700 , Loss: 0.0003022847231477499\n",
            "Epoch: 28701 , Loss: 0.00030217203311622143\n",
            "Epoch: 28702 , Loss: 0.00030187470838427544\n",
            "Epoch: 28703 , Loss: 0.0003020981093868613\n",
            "Epoch: 28704 , Loss: 0.00030201624031178653\n",
            "Epoch: 28705 , Loss: 0.00029980516410432756\n",
            "Epoch: 28706 , Loss: 0.0003012410015799105\n",
            "Epoch: 28707 , Loss: 0.00030221452470868826\n",
            "Epoch: 28708 , Loss: 0.00030168594093993306\n",
            "Epoch: 28709 , Loss: 0.00030096256523393095\n",
            "Epoch: 28710 , Loss: 0.00029915827326476574\n",
            "Epoch: 28711 , Loss: 0.0003005965845659375\n",
            "Epoch: 28712 , Loss: 0.0003011993831023574\n",
            "Epoch: 28713 , Loss: 0.00030449035693891346\n",
            "Epoch: 28714 , Loss: 0.0002993313246406615\n",
            "Epoch: 28715 , Loss: 0.0002995316172018647\n",
            "Epoch: 28716 , Loss: 0.0003028960491064936\n",
            "Epoch: 28717 , Loss: 0.00030049984343349934\n",
            "Epoch: 28718 , Loss: 0.00030213192803785205\n",
            "Epoch: 28719 , Loss: 0.0003008501371368766\n",
            "Epoch: 28720 , Loss: 0.0003019834985025227\n",
            "Epoch: 28721 , Loss: 0.00030077563133090734\n",
            "Epoch: 28722 , Loss: 0.00029960303800180554\n",
            "Epoch: 28723 , Loss: 0.00030032050563022494\n",
            "Epoch: 28724 , Loss: 0.0003024896723218262\n",
            "Epoch: 28725 , Loss: 0.00030065866303630173\n",
            "Epoch: 28726 , Loss: 0.000299996929243207\n",
            "Epoch: 28727 , Loss: 0.0003009734209626913\n",
            "Epoch: 28728 , Loss: 0.0002984947059303522\n",
            "Epoch: 28729 , Loss: 0.0003005302860401571\n",
            "Epoch: 28730 , Loss: 0.00030095374677330256\n",
            "Epoch: 28731 , Loss: 0.0002994584501720965\n",
            "Epoch: 28732 , Loss: 0.00029950973112136126\n",
            "Epoch: 28733 , Loss: 0.00030143861658871174\n",
            "Epoch: 28734 , Loss: 0.00029877276392653584\n",
            "Epoch: 28735 , Loss: 0.0003004286845680326\n",
            "Epoch: 28736 , Loss: 0.00030163192423060536\n",
            "Epoch: 28737 , Loss: 0.0003000600263476372\n",
            "Epoch: 28738 , Loss: 0.00030150217935442924\n",
            "Epoch: 28739 , Loss: 0.000299224688205868\n",
            "Epoch: 28740 , Loss: 0.0003019648138433695\n",
            "Epoch: 28741 , Loss: 0.00029986834852024913\n",
            "Epoch: 28742 , Loss: 0.00029959084349684417\n",
            "Epoch: 28743 , Loss: 0.0002996070543304086\n",
            "Epoch: 28744 , Loss: 0.0002986804465763271\n",
            "Epoch: 28745 , Loss: 0.00029921552049927413\n",
            "Epoch: 28746 , Loss: 0.0003012853558175266\n",
            "Epoch: 28747 , Loss: 0.0003002806624863297\n",
            "Epoch: 28748 , Loss: 0.0002976750547531992\n",
            "Epoch: 28749 , Loss: 0.00030168957891874015\n",
            "Epoch: 28750 , Loss: 0.0002988722117152065\n",
            "=============================================\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "16 correctly classified among 100\n",
            "Accuracy as of 28750 epochs: 16.0\n",
            "=============================================\n",
            "Epoch: 28751 , Loss: 0.0002998501295223832\n",
            "Epoch: 28752 , Loss: 0.00029881272348575294\n",
            "Epoch: 28753 , Loss: 0.00029826274840161204\n",
            "Epoch: 28754 , Loss: 0.0002989827189594507\n",
            "Epoch: 28755 , Loss: 0.00029766096849925816\n",
            "Epoch: 28756 , Loss: 0.00029816306778229773\n",
            "Epoch: 28757 , Loss: 0.0002985482569783926\n",
            "Epoch: 28758 , Loss: 0.0002988260821439326\n",
            "Epoch: 28759 , Loss: 0.0003002149751409888\n",
            "Epoch: 28760 , Loss: 0.0002956251264549792\n",
            "Epoch: 28761 , Loss: 0.00029693497344851494\n",
            "Epoch: 28762 , Loss: 0.0002988576306961477\n",
            "Epoch: 28763 , Loss: 0.0003011488006450236\n",
            "Epoch: 28764 , Loss: 0.00029761134646832943\n",
            "Epoch: 28765 , Loss: 0.0002957785618491471\n",
            "Epoch: 28766 , Loss: 0.0002989120257552713\n",
            "Epoch: 28767 , Loss: 0.00029862395604141057\n",
            "Epoch: 28768 , Loss: 0.00029676142730750144\n",
            "Epoch: 28769 , Loss: 0.0002974047092720866\n",
            "Epoch: 28770 , Loss: 0.00029628563788719475\n",
            "Epoch: 28771 , Loss: 0.0003007097402587533\n",
            "Epoch: 28772 , Loss: 0.0002953246876131743\n",
            "Epoch: 28773 , Loss: 0.0002969511551782489\n",
            "Epoch: 28774 , Loss: 0.0002962278085760772\n",
            "Epoch: 28775 , Loss: 0.00029886880656704307\n",
            "Epoch: 28776 , Loss: 0.00029696355341002345\n",
            "Epoch: 28777 , Loss: 0.00029549223836511374\n",
            "Epoch: 28778 , Loss: 0.00029711006209254265\n",
            "Epoch: 28779 , Loss: 0.00029614896629936993\n",
            "Epoch: 28780 , Loss: 0.000298382859909907\n",
            "Epoch: 28781 , Loss: 0.00029697001446038485\n",
            "Epoch: 28782 , Loss: 0.0002972350048366934\n",
            "Epoch: 28783 , Loss: 0.00029696617275476456\n",
            "Epoch: 28784 , Loss: 0.0002969671622850001\n",
            "Epoch: 28785 , Loss: 0.00029663025634363294\n",
            "Epoch: 28786 , Loss: 0.0002971072099171579\n",
            "Epoch: 28787 , Loss: 0.00029671372612938285\n",
            "Epoch: 28788 , Loss: 0.0002986826584674418\n",
            "Epoch: 28789 , Loss: 0.0002955874369945377\n",
            "Epoch: 28790 , Loss: 0.0002963827573694289\n",
            "Epoch: 28791 , Loss: 0.0002957024553325027\n",
            "Epoch: 28792 , Loss: 0.0002970950154121965\n",
            "Epoch: 28793 , Loss: 0.0002975442912429571\n",
            "Epoch: 28794 , Loss: 0.0002973776718135923\n",
            "Epoch: 28795 , Loss: 0.0002963321458082646\n",
            "Epoch: 28796 , Loss: 0.0002958921540994197\n",
            "Epoch: 28797 , Loss: 0.0002961361897177994\n",
            "Epoch: 28798 , Loss: 0.00029496452771127224\n",
            "Epoch: 28799 , Loss: 0.0002967006294056773\n",
            "Epoch: 28800 , Loss: 0.0002961779828183353\n",
            "Epoch: 28801 , Loss: 0.0002944940351881087\n",
            "Epoch: 28802 , Loss: 0.0002978223783429712\n",
            "Epoch: 28803 , Loss: 0.0002961401187349111\n",
            "Epoch: 28804 , Loss: 0.0002945960150100291\n",
            "Epoch: 28805 , Loss: 0.00029748925589956343\n",
            "Epoch: 28806 , Loss: 0.00029544628341682255\n",
            "Epoch: 28807 , Loss: 0.0002938079705927521\n",
            "Epoch: 28808 , Loss: 0.00029534066561609507\n",
            "Epoch: 28809 , Loss: 0.00029506886494345963\n",
            "Epoch: 28810 , Loss: 0.00029596875538118184\n",
            "Epoch: 28811 , Loss: 0.00029644957976415753\n",
            "Epoch: 28812 , Loss: 0.00029408821137622\n",
            "Epoch: 28813 , Loss: 0.0002948337933048606\n",
            "Epoch: 28814 , Loss: 0.00029455168987624347\n",
            "Epoch: 28815 , Loss: 0.00029401754727587104\n",
            "Epoch: 28816 , Loss: 0.0002953027142211795\n",
            "Epoch: 28817 , Loss: 0.00029414379969239235\n",
            "Epoch: 28818 , Loss: 0.0002942381543107331\n",
            "Epoch: 28819 , Loss: 0.00029471743619069457\n",
            "Epoch: 28820 , Loss: 0.0002963708247989416\n",
            "Epoch: 28821 , Loss: 0.0002937269164249301\n",
            "Epoch: 28822 , Loss: 0.00029554899083450437\n",
            "Epoch: 28823 , Loss: 0.0002937147510237992\n",
            "Epoch: 28824 , Loss: 0.0002948689798358828\n",
            "Epoch: 28825 , Loss: 0.00029416338657028973\n",
            "Epoch: 28826 , Loss: 0.0002943701983895153\n",
            "Epoch: 28827 , Loss: 0.0002942488354165107\n",
            "Epoch: 28828 , Loss: 0.0002937084645964205\n",
            "Epoch: 28829 , Loss: 0.0002947961329482496\n",
            "Epoch: 28830 , Loss: 0.00029471758170984685\n",
            "Epoch: 28831 , Loss: 0.0002946489257737994\n",
            "Epoch: 28832 , Loss: 0.00029435084434226155\n",
            "Epoch: 28833 , Loss: 0.0002915431105066091\n",
            "Epoch: 28834 , Loss: 0.00029572469065897167\n",
            "Epoch: 28835 , Loss: 0.0002943203435279429\n",
            "Epoch: 28836 , Loss: 0.000291304022539407\n",
            "Epoch: 28837 , Loss: 0.00029548013117164373\n",
            "Epoch: 28838 , Loss: 0.0002918109530583024\n",
            "Epoch: 28839 , Loss: 0.0002911374031100422\n",
            "Epoch: 28840 , Loss: 0.00028796709375455976\n",
            "Epoch: 28841 , Loss: 0.00029847773839719594\n",
            "Epoch: 28842 , Loss: 0.0002953852235805243\n",
            "Epoch: 28843 , Loss: 0.0002986520994454622\n",
            "Epoch: 28844 , Loss: 0.0002946564054582268\n",
            "Epoch: 28845 , Loss: 0.0002917060046456754\n",
            "Epoch: 28846 , Loss: 0.0002977354743052274\n",
            "Epoch: 28847 , Loss: 0.00029924928094260395\n",
            "Epoch: 28848 , Loss: 0.00029684719629585743\n",
            "Epoch: 28849 , Loss: 0.00029269896913319826\n",
            "Epoch: 28850 , Loss: 0.0002950943890027702\n",
            "Epoch: 28851 , Loss: 0.0002950729976873845\n",
            "Epoch: 28852 , Loss: 0.0002920866245403886\n",
            "Epoch: 28853 , Loss: 0.00029190885834395885\n",
            "Epoch: 28854 , Loss: 0.0002913748612627387\n",
            "Epoch: 28855 , Loss: 0.00029247411293908954\n",
            "Epoch: 28856 , Loss: 0.0002923198917414993\n",
            "Epoch: 28857 , Loss: 0.0002915901714004576\n",
            "Epoch: 28858 , Loss: 0.0002917480014730245\n",
            "Epoch: 28859 , Loss: 0.0002921511186286807\n",
            "Epoch: 28860 , Loss: 0.00029563962016254663\n",
            "Epoch: 28861 , Loss: 0.00029005197575315833\n",
            "Epoch: 28862 , Loss: 0.0002941866987384856\n",
            "Epoch: 28863 , Loss: 0.0002920092665590346\n",
            "Epoch: 28864 , Loss: 0.0002920746919699013\n",
            "Epoch: 28865 , Loss: 0.00029618831467814744\n",
            "Epoch: 28866 , Loss: 0.00028951605781912804\n",
            "Epoch: 28867 , Loss: 0.00028319258126430213\n",
            "Epoch: 28868 , Loss: 0.0003123840724583715\n",
            "Epoch: 28869 , Loss: 0.0002868732553906739\n",
            "Epoch: 28870 , Loss: 0.00028984149685129523\n",
            "Epoch: 28871 , Loss: 0.0002909936592914164\n",
            "Epoch: 28872 , Loss: 0.000290927680907771\n",
            "Epoch: 28873 , Loss: 0.0002909111208282411\n",
            "Epoch: 28874 , Loss: 0.00029125798027962446\n",
            "Epoch: 28875 , Loss: 0.0002903238346334547\n",
            "Epoch: 28876 , Loss: 0.00028760204440914094\n",
            "Epoch: 28877 , Loss: 0.00029104124405421317\n",
            "Epoch: 28878 , Loss: 0.0002930821501649916\n",
            "Epoch: 28879 , Loss: 0.0002924029831774533\n",
            "Epoch: 28880 , Loss: 0.00028977968031540513\n",
            "Epoch: 28881 , Loss: 0.0002925919252447784\n",
            "Epoch: 28882 , Loss: 0.00029150035697966814\n",
            "Epoch: 28883 , Loss: 0.0002911882766056806\n",
            "Epoch: 28884 , Loss: 0.0002921961422543973\n",
            "Epoch: 28885 , Loss: 0.0002890363393817097\n",
            "Epoch: 28886 , Loss: 0.0002916509984061122\n",
            "Epoch: 28887 , Loss: 0.00029130245093256235\n",
            "Epoch: 28888 , Loss: 0.00029054039623588324\n",
            "Epoch: 28889 , Loss: 0.00028963160002604127\n",
            "Epoch: 28890 , Loss: 0.00029014391475357115\n",
            "Epoch: 28891 , Loss: 0.0002907930174842477\n",
            "Epoch: 28892 , Loss: 0.0002896794758271426\n",
            "Epoch: 28893 , Loss: 0.0002902320120483637\n",
            "Epoch: 28894 , Loss: 0.0002900536637753248\n",
            "Epoch: 28895 , Loss: 0.00029123391141183674\n",
            "Epoch: 28896 , Loss: 0.00029022706439718604\n",
            "Epoch: 28897 , Loss: 0.000291484990157187\n",
            "Epoch: 28898 , Loss: 0.00028997616027481854\n",
            "Epoch: 28899 , Loss: 0.00028969370760023594\n",
            "Epoch: 28900 , Loss: 0.0002902313135564327\n",
            "Epoch: 28901 , Loss: 0.0002894433564506471\n",
            "Epoch: 28902 , Loss: 0.0002896412624977529\n",
            "Epoch: 28903 , Loss: 0.0002893311611842364\n",
            "Epoch: 28904 , Loss: 0.00028999854112043977\n",
            "Epoch: 28905 , Loss: 0.00028946337988600135\n",
            "Epoch: 28906 , Loss: 0.00028932359418831766\n",
            "Epoch: 28907 , Loss: 0.00028915557777509093\n",
            "Epoch: 28908 , Loss: 0.0002890405594371259\n",
            "Epoch: 28909 , Loss: 0.00028913727146573365\n",
            "Epoch: 28910 , Loss: 0.00028898517484776676\n",
            "Epoch: 28911 , Loss: 0.00028904940700158477\n",
            "Epoch: 28912 , Loss: 0.00028637712239287794\n",
            "Epoch: 28913 , Loss: 0.00028891785768792033\n",
            "Epoch: 28914 , Loss: 0.0002893675409723073\n",
            "Epoch: 28915 , Loss: 0.0002885347348637879\n",
            "Epoch: 28916 , Loss: 0.00028867271612398326\n",
            "Epoch: 28917 , Loss: 0.00028660576208494604\n",
            "Epoch: 28918 , Loss: 0.0002892039774451405\n",
            "Epoch: 28919 , Loss: 0.0002871073374990374\n",
            "Epoch: 28920 , Loss: 0.0002879227395169437\n",
            "Epoch: 28921 , Loss: 0.0002879427629522979\n",
            "Epoch: 28922 , Loss: 0.0002880645915865898\n",
            "Epoch: 28923 , Loss: 0.00028674837085418403\n",
            "Epoch: 28924 , Loss: 0.00028971515712328255\n",
            "Epoch: 28925 , Loss: 0.0002884166606236249\n",
            "Epoch: 28926 , Loss: 0.00028934417059645057\n",
            "Epoch: 28927 , Loss: 0.000287906063022092\n",
            "Epoch: 28928 , Loss: 0.0002872169716283679\n",
            "Epoch: 28929 , Loss: 0.0002882106346078217\n",
            "Epoch: 28930 , Loss: 0.00028657499933615327\n",
            "Epoch: 28931 , Loss: 0.00028685119468718767\n",
            "Epoch: 28932 , Loss: 0.0002889156749006361\n",
            "Epoch: 28933 , Loss: 0.00028448979719541967\n",
            "Epoch: 28934 , Loss: 0.00028630890301428735\n",
            "Epoch: 28935 , Loss: 0.0002907077723648399\n",
            "Epoch: 28936 , Loss: 0.0002880598185583949\n",
            "Epoch: 28937 , Loss: 0.0002876887156162411\n",
            "Epoch: 28938 , Loss: 0.000287536415271461\n",
            "Epoch: 28939 , Loss: 0.00028506547096185386\n",
            "Epoch: 28940 , Loss: 0.0002865974965970963\n",
            "Epoch: 28941 , Loss: 0.00028906596708111465\n",
            "Epoch: 28942 , Loss: 0.0002864496782422066\n",
            "Epoch: 28943 , Loss: 0.00028619394288398325\n",
            "Epoch: 28944 , Loss: 0.00028701021801680326\n",
            "Epoch: 28945 , Loss: 0.0002837400825228542\n",
            "Epoch: 28946 , Loss: 0.0002860993845388293\n",
            "Epoch: 28947 , Loss: 0.00028566474793478847\n",
            "Epoch: 28948 , Loss: 0.00028718498651869595\n",
            "Epoch: 28949 , Loss: 0.0002872223558370024\n",
            "Epoch: 28950 , Loss: 0.00028509824187494814\n",
            "Epoch: 28951 , Loss: 0.00028559265774674714\n",
            "Epoch: 28952 , Loss: 0.00028579746140167117\n",
            "Epoch: 28953 , Loss: 0.00028291239868849516\n",
            "Epoch: 28954 , Loss: 0.0002870193566195667\n",
            "Epoch: 28955 , Loss: 0.0002868332958314568\n",
            "Epoch: 28956 , Loss: 0.00028553299489431083\n",
            "Epoch: 28957 , Loss: 0.00028601085068657994\n",
            "Epoch: 28958 , Loss: 0.00028441171161830425\n",
            "Epoch: 28959 , Loss: 0.0002867307630367577\n",
            "Epoch: 28960 , Loss: 0.0002862602414097637\n",
            "Epoch: 28961 , Loss: 0.0002847530704457313\n",
            "Epoch: 28962 , Loss: 0.00028572665178216994\n",
            "Epoch: 28963 , Loss: 0.00028126806137152016\n",
            "Epoch: 28964 , Loss: 0.00028332130750641227\n",
            "Epoch: 28965 , Loss: 0.00028558389749377966\n",
            "Epoch: 28966 , Loss: 0.00028545083478093147\n",
            "Epoch: 28967 , Loss: 0.0002839250082615763\n",
            "Epoch: 28968 , Loss: 0.00028348149498924613\n",
            "Epoch: 28969 , Loss: 0.00028567167464643717\n",
            "Epoch: 28970 , Loss: 0.0002866072172764689\n",
            "Epoch: 28971 , Loss: 0.00028961015050299466\n",
            "Epoch: 28972 , Loss: 0.0002851073513738811\n",
            "Epoch: 28973 , Loss: 0.0002868288429453969\n",
            "Epoch: 28974 , Loss: 0.00028517545433714986\n",
            "Epoch: 28975 , Loss: 0.0002847861615009606\n",
            "Epoch: 28976 , Loss: 0.00028421179740689695\n",
            "Epoch: 28977 , Loss: 0.0002839302469510585\n",
            "Epoch: 28978 , Loss: 0.0002888730959966779\n",
            "Epoch: 28979 , Loss: 0.0002825037809088826\n",
            "Epoch: 28980 , Loss: 0.00028097021277062595\n",
            "Epoch: 28981 , Loss: 0.00028365105390548706\n",
            "Epoch: 28982 , Loss: 0.0002848333679139614\n",
            "Epoch: 28983 , Loss: 0.0002828235155902803\n",
            "Epoch: 28984 , Loss: 0.00028360949363559484\n",
            "Epoch: 28985 , Loss: 0.0002835406921803951\n",
            "Epoch: 28986 , Loss: 0.0002850532182492316\n",
            "Epoch: 28987 , Loss: 0.00028913235291838646\n",
            "Epoch: 28988 , Loss: 0.0002824924304150045\n",
            "Epoch: 28989 , Loss: 0.0002843988477252424\n",
            "Epoch: 28990 , Loss: 0.00028719494002871215\n",
            "Epoch: 28991 , Loss: 0.00028401409508660436\n",
            "Epoch: 28992 , Loss: 0.0002866978757083416\n",
            "Epoch: 28993 , Loss: 0.00028583069797605276\n",
            "Epoch: 28994 , Loss: 0.0002853315672837198\n",
            "Epoch: 28995 , Loss: 0.0002832617028616369\n",
            "Epoch: 28996 , Loss: 0.00028315343661233783\n",
            "Epoch: 28997 , Loss: 0.0002831447636708617\n",
            "Epoch: 28998 , Loss: 0.0002820163790602237\n",
            "Epoch: 28999 , Loss: 0.0002832903410308063\n",
            "Epoch: 29000 , Loss: 0.00028414628468453884\n",
            "=============================================\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 49ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 46ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 44ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "12 correctly classified among 100\n",
            "Accuracy as of 29000 epochs: 12.0\n",
            "=============================================\n",
            "Epoch: 29001 , Loss: 0.0002830492448993027\n",
            "Epoch: 29002 , Loss: 0.0002833344624377787\n",
            "Epoch: 29003 , Loss: 0.000283001980278641\n",
            "Epoch: 29004 , Loss: 0.0002827462740242481\n",
            "Epoch: 29005 , Loss: 0.0002823221147991717\n",
            "Epoch: 29006 , Loss: 0.000282471562968567\n",
            "Epoch: 29007 , Loss: 0.0002828585566021502\n",
            "Epoch: 29008 , Loss: 0.00028211617609485984\n",
            "Epoch: 29009 , Loss: 0.0002810681762639433\n",
            "Epoch: 29010 , Loss: 0.0002821689413394779\n",
            "Epoch: 29011 , Loss: 0.0002818829962052405\n",
            "Epoch: 29012 , Loss: 0.0002830382436513901\n",
            "Epoch: 29013 , Loss: 0.00028337331605143845\n",
            "Epoch: 29014 , Loss: 0.0002827812568284571\n",
            "Epoch: 29015 , Loss: 0.0002811331069096923\n",
            "Epoch: 29016 , Loss: 0.0002813294413499534\n",
            "Epoch: 29017 , Loss: 0.0002817060158122331\n",
            "Epoch: 29018 , Loss: 0.00028139204368926585\n",
            "Epoch: 29019 , Loss: 0.0002821946400217712\n",
            "Epoch: 29020 , Loss: 0.000281319982605055\n",
            "Epoch: 29021 , Loss: 0.00028241967083886266\n",
            "Epoch: 29022 , Loss: 0.0002812970196828246\n",
            "Epoch: 29023 , Loss: 0.00028264711727388203\n",
            "Epoch: 29024 , Loss: 0.00028076409944333136\n",
            "Epoch: 29025 , Loss: 0.00028248215676285326\n",
            "Epoch: 29026 , Loss: 0.00028105120873078704\n",
            "Epoch: 29027 , Loss: 0.00028004124760627747\n",
            "Epoch: 29028 , Loss: 0.0002805365948006511\n",
            "Epoch: 29029 , Loss: 0.00028069395921193063\n",
            "Epoch: 29030 , Loss: 0.00028046779334545135\n",
            "Epoch: 29031 , Loss: 0.0002800906077027321\n",
            "Epoch: 29032 , Loss: 0.00028020632453262806\n",
            "Epoch: 29033 , Loss: 0.0002797595807351172\n",
            "Epoch: 29034 , Loss: 0.00028062646742910147\n",
            "Epoch: 29035 , Loss: 0.00027975975535809994\n",
            "Epoch: 29036 , Loss: 0.00028080568881705403\n",
            "Epoch: 29037 , Loss: 0.0002799749781843275\n",
            "Epoch: 29038 , Loss: 0.00028228454175405204\n",
            "Epoch: 29039 , Loss: 0.00028151710284873843\n",
            "Epoch: 29040 , Loss: 0.0002778840425889939\n",
            "Epoch: 29041 , Loss: 0.00027818232774734497\n",
            "Epoch: 29042 , Loss: 0.00028177103376947343\n",
            "Epoch: 29043 , Loss: 0.00027985410997644067\n",
            "Epoch: 29044 , Loss: 0.0002804233517963439\n",
            "Epoch: 29045 , Loss: 0.00027991936076432467\n",
            "Epoch: 29046 , Loss: 0.00028154553729109466\n",
            "Epoch: 29047 , Loss: 0.00027865651645697653\n",
            "Epoch: 29048 , Loss: 0.0002806395059451461\n",
            "Epoch: 29049 , Loss: 0.0002830453449860215\n",
            "Epoch: 29050 , Loss: 0.000281248678220436\n",
            "Epoch: 29051 , Loss: 0.00027585611678659916\n",
            "Epoch: 29052 , Loss: 0.00027629590476863086\n",
            "Epoch: 29053 , Loss: 0.0002814678300637752\n",
            "Epoch: 29054 , Loss: 0.00027896242681890726\n",
            "Epoch: 29055 , Loss: 0.00028056654264219105\n",
            "Epoch: 29056 , Loss: 0.000279189640423283\n",
            "Epoch: 29057 , Loss: 0.00027957939892075956\n",
            "Epoch: 29058 , Loss: 0.000279144907835871\n",
            "Epoch: 29059 , Loss: 0.00027858553221449256\n",
            "Epoch: 29060 , Loss: 0.0002780874783638865\n",
            "Epoch: 29061 , Loss: 0.0002780507202260196\n",
            "Epoch: 29062 , Loss: 0.00027770601445809007\n",
            "Epoch: 29063 , Loss: 0.00027916204999201\n",
            "Epoch: 29064 , Loss: 0.00027882447466254234\n",
            "Epoch: 29065 , Loss: 0.0002796018961817026\n",
            "Epoch: 29066 , Loss: 0.00027713360032066703\n",
            "Epoch: 29067 , Loss: 0.0002778582856990397\n",
            "Epoch: 29068 , Loss: 0.0002788939164020121\n",
            "Epoch: 29069 , Loss: 0.0002791411825455725\n",
            "Epoch: 29070 , Loss: 0.00027763881371356547\n",
            "Epoch: 29071 , Loss: 0.0002788975543808192\n",
            "Epoch: 29072 , Loss: 0.0002774959721136838\n",
            "Epoch: 29073 , Loss: 0.00027983254403807223\n",
            "Epoch: 29074 , Loss: 0.0002784080570563674\n",
            "Epoch: 29075 , Loss: 0.00027901213616132736\n",
            "Epoch: 29076 , Loss: 0.0002783701929729432\n",
            "Epoch: 29077 , Loss: 0.00027848724857904017\n",
            "Epoch: 29078 , Loss: 0.00027712355949915946\n",
            "Epoch: 29079 , Loss: 0.0002790435100905597\n",
            "Epoch: 29080 , Loss: 0.0002789274149108678\n",
            "Epoch: 29081 , Loss: 0.00027785709244199097\n",
            "Epoch: 29082 , Loss: 0.00027727411361411214\n",
            "Epoch: 29083 , Loss: 0.00027782918186858296\n",
            "Epoch: 29084 , Loss: 0.00027759038493968546\n",
            "Epoch: 29085 , Loss: 0.0002769033599179238\n",
            "Epoch: 29086 , Loss: 0.00027769082225859165\n",
            "Epoch: 29087 , Loss: 0.00027810249594040215\n",
            "Epoch: 29088 , Loss: 0.0002735854941420257\n",
            "Epoch: 29089 , Loss: 0.00027669998235069215\n",
            "Epoch: 29090 , Loss: 0.0002785417309496552\n",
            "Epoch: 29091 , Loss: 0.0002763374359346926\n",
            "Epoch: 29092 , Loss: 0.0002770465798676014\n",
            "Epoch: 29093 , Loss: 0.0002753896114882082\n",
            "Epoch: 29094 , Loss: 0.0002765173267107457\n",
            "Epoch: 29095 , Loss: 0.00028043275233358145\n",
            "Epoch: 29096 , Loss: 0.00027619494358077645\n",
            "Epoch: 29097 , Loss: 0.000273966375971213\n",
            "Epoch: 29098 , Loss: 0.0002760545175988227\n",
            "Epoch: 29099 , Loss: 0.00027854839572682977\n",
            "Epoch: 29100 , Loss: 0.00027634375146590173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A8AEirb0Tvy9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}